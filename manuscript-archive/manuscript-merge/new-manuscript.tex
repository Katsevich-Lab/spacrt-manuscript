\documentclass[12pt]{article}
\setlength{\headheight}{35pt} 

%----------------------------------------------------------
% Packages
%----------------------------------------------------------
\usepackage[vmargin=1.18in,hmargin=1.1in]{geometry}				% page layout
\usepackage{amssymb,amsfonts,amsthm,mathtools,bm,bbm,mathrsfs} 	% math fonts/symbols
\usepackage[dvipsnames]{xcolor} 								% for \textcolor
\usepackage[colorlinks, linkcolor = blue, citecolor = blue, urlcolor = blue]{hyperref} 								% for hyperlinks 
\usepackage{graphicx} 											% for \includegraphics
\usepackage{caption} 											% for figure and table captions
\usepackage{verbatim}
\usepackage[misc]{ifsym} 										% for \Letter
\usepackage[ruled, linesnumbered]{algorithm2e} 					% for algorithm box
\usepackage{layouts} % to get information about document layout
\usepackage[uniquename=false, 
			uniquelist = false, 
			url = false, 
			doi = false, 
			isbn = false, 
			natbib = true, 
			backend = bibtex, 
			style = authoryear, 
			maxbibnames = 5]{biblatex} 							% for citations and bibliography
% \RequirePackage{amsthm,amsmath,amsfonts,amssymb,mathtools,bm,bbm,mathrsfs}  % typesetting                                    % author-year citations
% \RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}          % coloring bibliography citations and linked URLs                                        
% \RequirePackage{verbatim}
% \RequirePackage[ruled, linesnumbered]{algorithm2e} 					        % for algorithm box
\usepackage{booktabs} % to make book table
\usepackage{multirow} % to create table with multirow
\usepackage[noend]{algpseudocode}
\usepackage{subcaption}
\usepackage{multirow}

\geometry{a4paper,scale=0.75}

\newenvironment{breakablealgorithm}
{% \begin{breakablealgorithm}
	\begin{center}
		\refstepcounter{algorithm}% New algorithm
		\hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
		\renewcommand{\caption}[2][\relax]{% Make a new \caption
			{\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
			\ifx\relax##1\relax % #1 is \relax
			\addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
			\else % #1 is not \relax
			\addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
			\fi
			\kern2pt\hrule\kern2pt
		}
	}{% \end{breakablealgorithm}
		\kern2pt\hrule\relax% \@fs@post for \@fs@ruled
	\end{center}
}
\makeatother


\newcommand{\zn}[1]{\textcolor{purple}{[ZN: #1]}}
\newcommand{\ek}[1]{\textcolor{red}{[EK: #1]}}
\newcommand{\jrc}[1]{\textcolor{blue}{[JRC: #1]}}

\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newcommand{\indep}{\perp \!\!\! \perp}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\X{\bm{X}}
\def\mP{\mathbb{P}}
\def\A{\bm{A}}
\def\P{\mathbb{P}}
\def\iid{\mathrm{iid}}
\def\H{\mathcal{H}}
\def\Hk{\mathcal{H}_k}
\def\tht{\bm{\theta}}
\def\gama{\bm{\gamma}}
\def\Del{\bm{\Delta}}
\def\sgn{\mathrm{sgn}}
\def\P{\mathbb{P}}


%----------------------------------------------------------
% Generic macros
%----------------------------------------------------------
\newcommand{\E}{\mathbb E}								% expectation
\newcommand{\V}{\mathrm{Var}}							% variance
\renewcommand{\P}{\mathbb{P}}							% probability
\newcommand{\Q}{\mathbb{Q}}								% quantile
\newcommand{\R}{\mathbb{R}}								% reals
\newcommand{\Z}{\mathbb{Z}}								% integers
\newcommand{\N}{\mathbb{N}}								% naturals
\newcommand{\indicator}{\mathbbm 1}						% indicator
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}		% norm
\newcommand{\independent}{{\perp \! \! \! \perp}}		% independent
\newcommand{\iidsim}{\stackrel{\mathrm{i.i.d.}}{\sim}} 	% i.i.d. distributed
\newcommand{\indsim}{\stackrel{\mathrm{ind}}{\sim}}		% independently distributed
\newcommand{\expit}{\mathrm{expit}}                 	% link function for logistic model
\newcommand{\convp}{\overset{\mathbb{P}}{\rightarrow}}             % convergence in probability
\newcommand{\convd}{\overset d \rightarrow}             % convergence in distribution
\newcommand{\convas}{\overset {a.s.} \rightarrow}       % convergence almost surely
\newcommand{\argmin}[1]{\underset{#1}{\arg \min}}       % arg min
\newcommand{\argmax}[1]{\underset{#1}{\arg \max}}       % arg max

%----------------------------------------------------------
% Paper-specific macros
%----------------------------------------------------------
\newcommand{\prx}{\bm X}								% population random X
\newcommand{\srx}{X}									% sample random X
\newcommand{\prz}{\bm Z}								% population random Z
\newcommand{\srz}{Z}									% sample random Z 
\newcommand{\prxk}{{{\widetilde{\bm X}}}}      			% population random resampled X
\newcommand{\seta}{s^{2}(\eta_0)}
\newcommand{\srxk}{\widetilde X}						% sample random resampled X
\newcommand{\pry}{{\bm Y}}								% population random Y
\newcommand{\pru}{{\bm U}}								% population random U
\newcommand{\sry}{Y}									% sample random Y 
\newcommand{\peps}{\bm \epsilon}						% population epsilon
\newcommand{\seps}{\epsilon}							% sample epsilon
\newcommand{\smu}{\mu}									% sample mu
\newcommand{\pmu}{\bm \mu}								% population mu
\newcommand{\law}{\mathcal L}							% law of (X,Y,Z)
\newcommand{\nulllaws}{\mathscr L^0}					% collection of null distributions
\newcommand{\regclass}{\mathscr R}					    % collection of distributions with regularity
\newcommand{\lawhat}{\widehat{\mathcal L}}				% estimated law of (X,Y,Z)
\newcommand{\CRT}{\textnormal{CRT}}             		% CRT
\newcommand{\dCRT}{\textnormal{dCRT}} 					% dCRT
\newcommand{\odCRT}{\textnormal{odCRT}} 					% dCRT
\newcommand{\GCM}{\textnormal{GCM}}						% GCM
\newcommand{\oGCM}{\textnormal{oGCM}}						% GCM
\newcommand{\dCRThat}{\widehat{\textnormal{dCRT}}}		% dCRT-hat
\newcommand{\MXtwohat}{\widehat{\textnormal{MX(2)}}}		% dCRT-hat
\newcommand{\ndCRThat}{\textnormal{ndCRT}}	% normalized dCRT-hat
\newcommand{\CRThat}{\widehat{\textnormal{CRT}}}		% CRT-hat
\renewcommand{\H}{\mathcal H}		 					% Hilbert subspace
\newcommand{\MXtwo}{\textnormal{MX(2)}}                 % MX(2)
\newcommand{\convdp}{\overset {d,p} \longrightarrow}    % conditional convergence in distribution
\newcommand{\convpp}{\overset {p,p} \longrightarrow}    % conditional convergence in probability
\newcommand{\spacrt}{\textnormal{spaCRT}}               % spaCRT
\newcommand{\aux}{\textnormal{aux}}               % auxiliary test
\newcommand{\asy}{\textnormal{asy}}              % asymptotic test
%----------------------------------------------------------
% Other macros
%----------------------------------------------------------
\let\oldnl\nl% Store \nl in \oldnl
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}} % Remove line number for one line of alg

\addbibresource{spacrt.bib}
% \bibliography{spacrt.bib}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{The conditional saddlepoint approximation for fast and accurate large-scale hypothesis testing}

\begin{document}

\author{Ziang Niu, Jyotishka Ray Choudhury, Eugene Katsevich}
\maketitle


\begin{abstract}
	Conditional independence (CI) testing is a fundamental problem in statistics. Modern data often exhibit large scale and high sparsity. CI testing methods using asymptotic normal approximations can induce inflated Type-I error rates when dealing with sparse data. Resampling-based CI testing procedures, while statistically more accurate, are computationally expensive, making them impractical for large-scale problems. To resolve this dilemma, we rely on the saddlepoint approximation (SPA) technique and establish a new SPA for the conditional distribution of a resampled test statistic that can be expressed as a sample average. The new result generalizes the existing SPA results in multiple facets and provide the first satisfactory justification of SPA for sign-flipping testing procedure proposed 70 years ago \citep{Daniels1955}. Leveraging the new theoretical result, we introduce the saddlepoint approximation-based conditional randomization test (spaCRT), a novel conditional independence test that effectively balances statistical accuracy and computational efficiency. The validity of $\spacrt$ is assessed with modern regression techniques applied such as lasso and kernel ridge regression. Through extensive simulations and real data analysis, we demonstrate that the spaCRT controls Type-I error and maintains high power, outperforming other asymptotic and resampling-based tests. Our method is particularly well-suited for data with extreme sparsity and large scale, such as single-cell CRISPR screen analyses and GWASs involving rare diseases.
\end{abstract}

  \section{Introduction} \label{sec:introduction}
  
  Given the joint distribution $(\prx, \pry, \prz) \sim \law_n$ (potentially depending on sample size $n$), the statistical task of conditional independence testing is to test the null hypothesis 
  \begin{equation}\label{eq:conditional-independent-null}
  H_0: \prx \indep \pry \mid \prz. 
  \end{equation}
  In the language of causal inference \citep{imbens2015causal}, $\prx$ is often interpreted as a treatment variable, $\pry$ as an outcome variable, and $\prz$ as a set of covariates. The null hypothesis $H_0$ states that the treatment has no effect on the outcome, given the covariates. Suppose the independent and identically distributed observations $(\srx_{in}, \sry_{in}, \srz_{in}) \iidsim \law_n$ are collected for subject $i = 1, \dots, n$. We denote these observations collectively as $\srx \in \R^n$, $\sry \in \R^n$, $\srz \in \R^{n \times d}$.
  
  \subsection{Challenges: data sparsity and large scale} \label{sec:statistical-computational-challenges}
  
  A particular challenge in modern applications involving tests of the CI null \eqref{eq:conditional-independent-null} is the high sparsity of the dataâ€”that is, an excessive number of zeros in the observations. This issue is especially prevalent in genetic and genomic data analysis. Moreover, sparsity is often accompanied by large scale, which further hinders the use of sophisticated but computationally intensive statistical methods.
  
  \small{
  A concrete example arises in population genetics. Genome-wide association studies (GWASs) investigate the relationship between genetic variants ($\prx$) and human diseases ($\pry$) \citep{visscher201710}, while accounting for potential confounding variables ($\prz$), such as population structure and experimental covariates like age and sex. Whole-genome sequencing can generate genotype data across thousands or even tens of thousands of loci for large population cohorts. When studying rare diseases and rare genetic variants, the observed data $\srx$ and $\sry$ often contain an excessive number of zeros. These two sources of sparsity have been widely recognized as major challenges in GWASs \citep{auer2015rare,dey2017fast,zhao2020uk,turro2020whole}.
  
  Another example comes from single-cell CRISPR screens \citep{Dixit2016,Adamson2016,Jaitin2016,Datlinger2017}. One of the primary biological objectives in these experiments is to use a large number of \textit{CRISPR perturbations} ($\prx$) and gene expression measurements ($\pry$) at the single-cell level to determine, for each perturbationâ€“gene pair, whether cells carrying the perturbation exhibit different expression levels compared to cells without the perturbation. The analysis is influenced by technical covariates ($\prz$), such as library size and batch effects. The number of hypotheses can reach hundreds of thousands of perturbationâ€“gene pairs, with over 200,000 cells in a single experiment \citep{Gasperini2019a}. Excessive sparsity in the treatment indicators $\srx$ is a common challenge due to the pooled nature of large-scale perturbation libraries. In addition, the single-cell resolution of gene expression data often leads to sparse, count-based observations in $\sry$.
  
  }
  
  Applying classical asymptotic inference methods in these sparse settings is often problematic, despite their computational appeal. Empirical evidence from both GWASs \citep{dey2017fast} and single-cell CRISPR screens \citep{Barry2024} has shown inflated Type-I error rates when using tests based on the central limit theorem. A natural alternative is to employ more well-calibrated, resampling-based testing procedures. However, the large scaleâ€”both in sample size and in the number of hypothesesâ€”makes the computational cost of such methods prohibitively high. As a result, direct applications of resampling techniques, such as the bootstrap or permutation tests, are often infeasible. This presents a dilemma for practitioners: how can one achieve accurate statistical inference while maintaining computational tractability in large-scale, high-sparsity scenarios?
  
  \subsection{Relevant literature}
  
  The tension between statistical accuracy and computational efficiency is not unique to CI testing; it is a common theme across many statistical applications. A substantial body of literature has been developed to address this challenge. We categorize the relevant works into two groups: resampling-based procedures and resampling-free procedures.
  
  Resampling-based procedures have been accelerated through adaptive resampling schemes, which dynamically adjust the number of resamples based on the data \citep{Besag1991,Gandy2009,Gandy2014,Gandy2016,Gandy2017a,Fischer2024a,Fischer2024}. These methods are broadly applicable to arbitrary resampling schemes and test statistics, though they typically require some resampling overhead. On the more applied side, to mitigate the computational burden of resampling, \citet{Ge2012,Winkler2016} proposed heuristic approaches that fit parametric curves to the resampled distributions, allowing a relatively small number of resamples to be used for learning the curve. \citet{Katsevich2020c} proposed the use of the $\dCRT$ \citep{Liu2022a}â€”an accelerated variant of the resampling-based conditional randomization test (CRT; \cite{CetL16})â€”in the context of single-cell CRISPR screens, and applied a similar parametric curve-fitting technique to the resampling distribution of $\dCRT$.
  
  On the other hand, a large class of methods addressing this dilemma relies on finer asymptotic expansion techniques. For instance, the classical Edgeworth expansion has been used to improve upon the naÃ¯ve normal approximation by better capturing the null distribution of test statistics \citep{hall2013bootstrap,bentkus1997edgeworth,Bickel1974}. However, its accuracy can deteriorate in the tail regions of the distribution \citep{hall2013bootstrap}, which poses a challenge in settings where small $p$-values are critical. The saddlepoint approximation (SPA; \citealp{Daniels1954,Lugannani1980}) is another classical technique that provides highly accurate approximations of both densities and tail probabilities. A key advantage of SPA is its relative error guarantee \citep{Butler2007,Kolassa2006}, which is especially important in large-scale settings where small $p$-values are common due to high multiplicity. SPA has also been proposed as an approximation for the resampling distributions of classical resampling-based procedures, such as permutation tests \citep{Robinson1982} and the bootstrap \citep{Hinkley1988}, although the theoretical justification for these applications remains incomplete.
  
  Despite the extensive literature discussed above, the challenge of reconciling statistical accuracy with computational efficiency remains unresolved in CI testing.
  
  \subsection{Our contributions}
  
  To fill this gap in CI testing, we propose a new approach that reconciles statistical accuracy with computational efficiency by leveraging the saddlepoint approximation (SPA) for resampling-based procedures. However, SPA has not been rigorously justified for any resampling-based methods, nor has it been applied in the context of CI testing. To address these challenges, we make two central contributions:
  
  \begin{enumerate}
	\item We first establish theoretical justification for SPA in the context of resampling-based hypothesis testing, where conditioning must be accounted for. This justifies the SPA for classical resampling-based procedures, like the sign-flipping test, 70 years after these approximations were first proposed. It also loosens the assumptions of SPA, not requiring continuity or lattice assumptions, establishing a new result even for the classical (unconditional) SPA.
	\item To extend this useful approximation to CI testing, we apply the SPA to the dCRT to arrive at the \textit{saddlepoint approximation-based conditional randomization test} (spaCRT), the first SPA for a CI testing procedure. We provide theoretical justification for the spaCRT in general, and in a variety of specific modern settings encompassing high-dimensional and nonparametric machine learning regressions. We provide the R package spacrt to implement the spaCRT, which is available on GitHub.
  \end{enumerate}
  
  Through simulation studies inspired by single-cell CRISPR screen and GWAS applications, we demonstrate that $\spacrt$ provides fast and accurate inference. We further apply $\spacrt$ to a real single-cell CRISPR screen dataset, achieving an approximately 250-fold speedup. The results, previewed in Figure~\ref{fig:dCRT_GCM_binomial_poisson}, highlight that $\spacrt$ offers substantially improved Type I error control compared to the normal approximation-based $\GCM$ test, while being significantly faster than $\dCRT$ and nearly as fast as $\GCM$. 
  \begin{figure*}[h!]
	  \centering
	  \includegraphics[width=0.6\textwidth]{figures-and-tables/motivating_example.pdf} 
	  \caption{\small{Comparing the Type-I error control and computation times the proposed $\spacrt$ with other methods on the \citet{Gasperini2019a} data. Left: QQ-plot of the $p$-values under the null hypothesis. Right: Mean computation times per hypothesis, in seconds.}}
	  \label{fig:dCRT_GCM_binomial_poisson} 
  \end{figure*}
  
  
  \subsection{Notation}
  
  
  Define $\sgn(x)$ as the sign of $x$, i.e. $\sgn(x)=1$ if $x>0$, $-1$ if $x<0$ and $0$ otherwise. For an infinitely differentiable function $f:\mathcal{X}\subset\mathbb{R}\mapsto\mathbb{R}$, define $f^{(r)}$ to be its $r$-th derivative. Define $f',f''$ as the first and second derivative of $f$ respectively. Denote $[n]$ for any $n\in\mathbb{N}_+$ as $\{1,\ldots,n\}$. Define $\expit(x)\equiv 1/ (1+\exp(-x))$. Define $\E_{\law_n}[\cdot\mid\mathcal{F}_n],\E_{\lawhat_n}[\cdot\mid\mathcal{F}_n]$ as the conditional expectations under law $\law_n$ and its estimate $\lawhat_n$ respectively. Similarly, we define $\V_{\law_n}[\cdot\mid\mathcal{F}_n]$ and $\V_{\lawhat_n}[\cdot\mid\mathcal{F}_n]$ as the conditional variance counterpart. We use the following standard notations regarding the asymptotic properties of $X_n$:
  \begin{align*}
	&X_n = O_{\P}(1) &&\text{ if for each } \delta > 0 \text{ there is an } M > 0 \text{ s.t. } \limsup_{n \rightarrow \infty}\P[|X_n| > M] < \delta; \\
	&X_n = \Omega_{\P}(1) &&\text{ if for each } \delta > 0 \text{ there is an } \eta > 0 \text{ s.t. } \limsup_{n \rightarrow \infty}\P[|X_n| < \eta] < \delta;\\
	&X_n = o_{\P}(1) &&\text{ if } \P[|X_n| > \eta] \rightarrow 0 \text{ for all } \eta > 0.
  \end{align*}
  We use $a_n\sim b_n$ if $0<\liminf_{n\rightarrow\infty}|a_n/b_n|\leq\limsup_{n\rightarrow\infty}|a_n/b_n|<\infty$ and $a_n=o(b_n)$ if $\limsup_{n\rightarrow\infty}|a_n/b_n|\rightarrow0$. 
  
  \section{The conditional saddlepoint approximation}\label{sec:conditional-spa}
  
  
  Let $\{W_{in}\}_{1 \leq i \leq n, n \geq 1}$ be a triangular array of random variables on a probability space $(\Omega, \mathcal F, \P)$ and let $\mathcal{F}_n \subseteq \mathcal F$ be a sequence of $\sigma$-algebras so that $\E[W_{in}|\mathcal{F}_n] = 0$ for each $(i, n)$ and $\{W_{in}\}_{1 \leq i \leq n}$ are independent conditionally on $\mathcal{F}_n$ for each $n$. For a sequence of cutoff values $w_n \in \mathcal F_n$, we will consider SPAs for the conditional tail probability:
  \begin{equation} \label{eq:tail-probability}
  p_n \equiv \P\left[\left.\frac{1}{n}\sum_{i = 1}^n W_{in} \geq w_n\ \right| \ \mathcal F_n\right].
  \end{equation}
  Conditional probability~\eqref{eq:tail-probability} arises naturally in $p$-value computation in the context of resampling-based hypothesis testing procedure. Consider a hypothesis test based on the statistic $\smash{T_n \equiv \frac{1}{n}\sum_{i = 1}^n X_{in}}$ and the resampling distribution $\smash{\widetilde T_n \equiv \frac{1}{n}\sum_{i = 1}^n \widetilde X_{in}}$ for some resampling mechanism generating $\smash{\widetilde X_{in} \mid \sigma(X_{1n}, \dots, X_{nn})}$. Setting $\smash{W_{in} \equiv \widetilde X_{in}}$, $w_n \equiv T_n$, and $\mathcal F_n \equiv \sigma(X_{1n}, \dots, X_{nn})$, we find that the $p$-value of this test,
  \begin{equation} \label{eq:resampling-p-value}
  p_n \equiv \P\left[\left.\frac{1}{n}\sum_{i = 1}^n \widetilde X_{in} \geq \frac{1}{n}\sum_{i = 1}^n X_{in}\ \right|\ X_{1n}, \dots, X_{nn}\right],
  \end{equation}
  matches the definition~\eqref{eq:tail-probability}. Thus an accurate analytic approximation to the conditional tail probability~\eqref{eq:tail-probability} is tempting to circumvent the necessity of repeated resampling. In this section, we will state our main results on the approximation accuracy of the saddlepoint approximation under two sets of tail assumptions on $W_{in}$ (Section~\ref{sec:main-results-spa}). We then discuss how the new results relate to the existing results in the literature (Section~\ref{sec:connection-spa-literatures}).
  
  \subsection{A new SPA for approximating conditional distribution} \label{sec:main-results-spa}
  
  Now, we impose assumptions on the triangular array $\{W_{in}\}_{1 \leq i \leq n, n \geq 1}$ in terms of their tail behaviors. In particular, we extend the definitions of sub-exponential and compactly supported random variables to the conditional setting and consider the \textit{conditionally sub-exponential} (CSE) and \textit{conditionally compactly supported} (CCS) conditions. We assume throughout that Assumption~\ref{assu:cse} or Assumption~\ref{assu:ccs} holds.
  \begin{assumption}[CSE condition]\label{assu:cse}
  There exist $\theta_n \in \mathcal F_n$ and $\beta > 0$ such that $\theta_n=O_{\P}(1)$ and almost surely,
  \begin{align*}
	0\leq \theta_n < \infty\quad\text{and}\quad\P[|W_{in}|\geq t\mid \mathcal{F}_n]\leq \theta_n\exp(-\beta t)\text{ for all $i,n$ and $t>0$}.
  \end{align*}
  \end{assumption}
  \begin{assumption}[CCS condition]\label{assu:ccs}
  There exist $\nu_{in} \in \mathcal F_n$ such that $\frac{1}{n}\sum_{i=1}^n \nu_{in}^4=O_{\P}(1)$ and almost surely,
  \begin{align*}
	0\leq \nu_{in}<\infty\quad\text{and}\quad W_{in}\in[-\nu_{in},\nu_{in}]\text{ for all $i$ and $n$}.
  \end{align*}
  \end{assumption}
  
  The saddlepoint approximation is usually based on the cumulant-generating functions (CGFs) of the summands. For our purposes, we use conditional CGFs:
  \begin{equation}\label{eq:CCGF_def}
  K_{in}(s) \equiv \log \E[\exp(sW_{in})|\mathcal F_n] \quad \text{and} \quad K_n(s) \equiv \frac{1}{n}\sum_{i = 1}^n K_{in}(s).
  \end{equation}
  The first step of the SPA is to find the solution $\hat s_n$ to the \textit{saddlepoint equation}:
  \begin{equation}\label{eq:saddlepoint-equation}
  K_n'(s) = w_n.
  \end{equation}
  We denote the solution to saddlepoint equation \eqref{eq:saddlepoint-equation} as $\hat s_n$ and later Theorem~\ref{thm:unified_unnormalized_moment_conditions} guarantees the existence and uniqueness of $\hat s_n$. With the solution $\hat s_n$ in hand, we define the saddlepoint approximation as follows. If we define
  \small
  \begin{align}\label{eq:lam_n_r_n_def}
	\lambda_n \equiv \hat s_n\sqrt{nK_n''(\hat s_n)}; \quad r_n \equiv
	\begin{cases}
	  \sgn(\hat s_n) \sqrt{2n( \hat s_n w_n - K_n(\hat s_n))} & \text{if } \hat s_n w_n - K_n(\hat s_n)\geq 0;\\
	  \mathrm{sgn}(\hat s_n) & \text{otherwise},
	\end{cases}
	\end{align}
  \normalsize
  then the saddlepoint approximation is given by
  \begin{equation}
  \widehat{\P}_{\text{LR}}\left[\left.\frac{1}{n}\sum_{i = 1}^n W_{in} \geq w_n\ \right|\ \mathcal{F}_n\right] \equiv 1-\Phi(r_n)+\phi(r_n)\left\{\frac{1}{\lambda_n}-\frac{1}{r_n}\right\}. \label{eq:lugannani-rice}
  \end{equation}
  When $w_n = 0$, we have $\hat s_n = \lambda_n = r_n = 0$. In this case, we take by convention that $1/0 - 1/0 \equiv 0$ in equation~\eqref{eq:conclusion_saddlepoint_approximation}, so that $\widehat{\P}_{\text{LR}} \equiv 1/2$. We will also use the convention $0/0=1$. The approximation $\widehat{\P}_{\text{LR}}$~\eqref{eq:lugannani-rice} is a direct generalization of the classical Lugannani-Rice formula \citep{Lugannani1980} to the conditional setting.
  
  \begin{theorem}\label{thm:unified_unnormalized_moment_conditions}
	  Let $W_{in}$ be a triangular array of random variables that are mean-zero and independent for each $n$, conditionally on $\mathcal F_n$. Suppose either Assumption \ref{assu:cse} or Assumption \ref{assu:ccs} holds, and that
	  \begin{align}\label{eq:lower_bound_conditional_variance}
		  \frac{1}{n}\sum_{i=1}^n \E[W_{in}^2 \mid \mathcal{F}_n]=\Omega_{\P}(1).
	  \end{align}
	  Let $w_n \in \mathcal F_n$ be a sequence with $w_n \overset{\P} \rightarrow 0$. Then, the saddlepoint equation~\eqref{eq:saddlepoint-equation} has a unique and finite solution $\hat s_n \in [-\varepsilon/2, \varepsilon/2]$ with probability approaching 1 as $n \rightarrow \infty$. The explicit form of $\hat s_n$ can be found in Appendix~\ref{sec:additional-details-spa}. Furthermore, the saddlepoint approximation $\widehat{\P}_{\textnormal{LR}}$ to the conditional tail probability~\eqref{eq:tail-probability} defined by equations \eqref{eq:lam_n_r_n_def} and~\eqref{eq:lugannani-rice} has vanishing relative error:
	  \begin{align}\label{eq:conclusion_saddlepoint_approximation}
		\P\left[\left.\frac{1}{n}\sum_{i = 1}^n W_{in} \geq w_n\ \right|\ \mathcal{F}_n\right] = \widehat{\P}_{\textnormal{LR}}\left[\left.\frac{1}{n}\sum_{i = 1}^n W_{in} \geq w_n\ \right|\ \mathcal{F}_n\right](1+o_{\P}(1)).
	  \end{align}
  \end{theorem}
  We make several remarks on the resuls of Theorem~\ref{thm:unified_unnormalized_moment_conditions}.
  \begin{remark}[Generality and transparency of assumptions]\label{rmk:generality-transparency}
	While most of the existing literature is fragmented based on whether the summands $W_{in}$ are smooth or lattice, Theorem~\ref{thm:unified_unnormalized_moment_conditions} (and later Corollary~\ref{cor:unconditional_LR_formula}) unify these two cases by not making any such assumptions. The price we pay for this generality is that our guarantee~\eqref{eq:conclusion_saddlepoint_approximation} does not come with a rate, as compared to most existing results. Nevertheless, we find it useful to have a single result encompassing a broad range of settings. Our assumptions are not only general but also transparent. Most existing results require complicated and difficult-to-verify conditions, often stated in terms of transforms of $W_{in}$. By contrast, consider for example the Assuption~\ref{assu:cse} (sub-exponential summands) and condition~\eqref{eq:lower_bound_conditional_variance} (non-degenerate variance). These are standard and easy to understand and verify.
  \end{remark}
  
  \begin{remark}[Justification of application to resampling-based tests]
	The result~\eqref{eq:conclusion_saddlepoint_approximation} lays a solid mathematical foundation for applying the SPA to a range of resampling-based hypothesis tests with independently resampled summands, including the sign-flipping test, bootstrap-based tests, and the conditional randomization test (CRT). Even though our main focus in this paper is on CRT, result~\eqref{eq:conclusion_saddlepoint_approximation} provides \textbf{the first rigorous justification of an SPA for the sign-flipping test} of the kind originally proposed by \citet{Daniels1955}. We summarize such result in Theorem~\ref{thm:example} in Appendix~\ref{sec:sign-flipping}. 
  \end{remark}
	
  \begin{remark}[Technical challenges]
	The proof of Theorem \ref{thm:unified_unnormalized_moment_conditions} presents several technical challenges, requiring us to significantly extend existing proof techniques. One of the most significant challenges is the conditioning, which adds an extra layer of randomness to the problem. For example, the saddlepoint equation~\eqref{eq:saddlepoint-equation} is a random equation, with random solution $\hat s_n$, which we must show exists with probability approaching 1. Furthermore, the cutoff $w_n$ is random, and in particular we must handle cases depending on the realization of the sign of this cutoff. A crucial step in our proof (which follows the general structure of that of \cite{Robinson1982}) is to use the Berry-Esseen inequality, but the extra conditioning requires us to prove a new conditional Berry-Esseen theorem. Another challenge is that we allow $w_n$ to decay to zero at an arbitrary rate, which requires a delicate analysis of the convergence of the SPA formula appearing in the RHS of the result \eqref{eq:conclusion_saddlepoint_approximation}. 
  \end{remark}
  
  \begin{remark}[Assumptions on the threshold $w_n$]
	The assumption $w_n=o_\P(1)$ in Theorem \ref{thm:unified_unnormalized_moment_conditions} appears to be restrictive at the first glance. However, this assumption allows us to guarantee the existence of a solution the saddlepoint equation beyond the case of compact support \citep{Daniels1954}. On the other hand, the rate of convergence of $w_n$ towards zero can be arbitrary, accommodating for two statistically meaningful regimes: (a) Moderate deviation regime: $w_n=O_{\P}(n^{-\alpha}),\alpha\in (0,1/2)$ and (b) CLT regime: $w_n=O_{\P}(1/\sqrt{n})$. We discuss this assumption further in Remark~\ref{rmk:cutoff-regime} after Theorem \ref{thm:example}.
  \end{remark}
  
  \subsection{Connection to existing unconditional results}\label{sec:connection-spa-literatures}
  
  It turns out our results are closely connected to several existing results in the literature. First, Theorem~\ref{thm:unified_unnormalized_moment_conditions} reduces to the following variant of the classical \citet{Lugannani1980} result by setting $\mathcal{F}_n \equiv \{\varnothing,\Omega\}$:
  
  \begin{corollary}\label{cor:unconditional_LR_formula}
	  Let $W_{in}$ be a triangular array of random variables that are mean-zero and independent for each $n$. Suppose that each $W_{in}$ is sub-exponential with constants $\theta, \beta > 0$, i.e. 
	  \begin{equation} \label{eq:subexponential}
	  \P[|W_{in}| \geq t] \leq \theta \exp(-\beta t) \quad \text{for all } t > 0.
	  \end{equation} 
	  Furthermore, suppose that
	  \begin{align}  \label{eq:nondegeneracy-unconditional}
	  \smash{\liminf_{n \rightarrow \infty}\ \frac{1}{n}\sum_{i = 1}^n \E[W_{in}^2] > 0.}
	  \end{align}
	  Given a sequence of cutoffs $w_n \rightarrow 0$ as $n \rightarrow \infty$, the saddlepoint equation~\eqref{eq:saddlepoint-equation} (based on the unconditional CGF $K_n$) has a unique solution $\hat s_n$ on $[-\varepsilon/2, \varepsilon/2]$ for all sufficiently large $n$. Furthermore, the unconditional saddlepoint approximation $\widehat{\P}_{\textnormal{LR}}$ (based on the unconditional CGF $K_n$) has vanishing relative error
	  \begin{align} \label{eq:conclusion_unconditional_saddlepoint_approximation}
	  \P\left[\frac{1}{n}\sum_{i = 1}^n W_{in} \geq w_n\right] = \widehat{\P}_{\textnormal{LR}}\left[\frac{1}{n}\sum_{i = 1}^n W_{in} \geq w_n\right](1+o(1)).
	  \end{align}
  \end{corollary}
  Compared to the classical results, the significance of Theorem~\ref{thm:unified_unnormalized_moment_conditions} and Corollary~\ref{cor:unconditional_LR_formula} is the generality to underlying distributions and transparency on the assumptions as discussed in Remark~\ref{rmk:generality-transparency}. Aside from the Lugannani-Rice formula, another tail probability estimate is proposed in \cite{Robinson1982}. In fact, we present an extension of Theorem~\ref{thm:unified_unnormalized_moment_conditions} in Proposition \ref{prop:equivalence_spa_formula} that employs a conditional variant of Robinson's formula:
  \begin{equation}
	\widehat{\P}_\text{R}\left[\left.\frac{1}{n}\sum_{i = 1}^n W_{in} \geq w_n\ \right|\ \mathcal{F}_n\right] \equiv \exp\left(\frac{\lambda_n^2-r_n^2}{2}\right)(1-\Phi(\lambda_n)).
	\label{eq:robinson-formula}
  \end{equation}
  \begin{proposition}\label{prop:equivalence_spa_formula}
	Under the assumptions of Theorem \ref{thm:unified_unnormalized_moment_conditions},   
	\begin{align}\label{eq:alternative_spa_formula}
	  \P\left[\left.\frac{1}{n}\sum_{i = 1}^n W_{in} \geq w_n\ \right|\ \mathcal{F}_n\right]=\widehat{\P}_\text{R}\left[\left.\frac{1}{n}\sum_{i = 1}^n W_{in} \geq w_n\ \right|\ \mathcal{F}_n\right](1+o_\P(1)).
	  \end{align}
  \end{proposition}
  \noindent In other words, $\widehat{\P}_\text{LR}$~\eqref{eq:lugannani-rice} is equivalent to $\widehat{\P}_\text{R}$~\eqref{eq:robinson-formula} with relative error $o_\P(1)$, linking Robinson's formula to that of Lugannani and Rice. A similar result was proved in the unconditional case by \citet{Kolassa2007}.
  
  \section{$\spacrt$: A resampling-free approximation to $\dCRT$} \label{sec:spacrt}
  
  \subsection{Background: $\dCRT$ and $\GCM$ test}\label{sec:background}
  
  The dCRT procedure, as proposed by \citet{Liu2022a}, is designed under the \textit{model-X assumption} that $\law_n(\prx \mid \prz)$ is known \citep{CetL16}. However, this procedure is usually deployed in practice by learning this conditional distribution in-sample. In a prior work, we established the statistical properties of the dCRT with $\law_n(\prx \mid \prz)$ estimated in sample \citep{Niu2022a}. In this paper, we will refer to the latter procedure as the $\dCRT$, allowing a minor abuse of terminology. Furthermore, we consider the special but still fairly general case when 
  \begin{equation}
  \law_n(\prx \mid \prz) = f(\prx \mid \theta_{n,x}(\prz)), 
  \end{equation}
  where $f(x|\theta)$ is an exponential family with natural parameter $\theta$, natural parameter space $\R$, and log-partition function $A$:
  \begin{align}\label{eq:NEF}
  f(x|\theta)=\exp(\theta x -A(\theta))h(x).
  \end{align}
  This is not a restrictive assumption, since we allow the function $\theta_{n,x}(\prz)$ to be arbitrary. Given this setup, consider estimating the functions $\theta_{n,x}(\prz)$ and $\mu_{n,y}(\prz) \equiv \E_{\law_n}[\pry \mid \prz]$ by $\widehat{\theta}_{n,x}(\prz)$ and $\widehat \mu_{n,y}(\prz)$, respectively. The learning procedures for these quantities can be arbitrary. The choices include but are not limited to parametric, non-parametric and high-dimensional regression methods. Setting $\widehat \mu_{n,x}(\prz) \equiv A'(\widehat{\theta}_{n,x}(\prz))$, we arrive at the test statistic 
  \begin{align}\label{eq:dCRThat}
	  T_n^{\dCRT}(X,Y,Z)
	=\frac{1}{n}\sum_{i=1}^{n}(\srx_{in}-\widehat\mu_{n,x}(\srz_{in}))(\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in})).
  \end{align}
  The $\dCRT$ is obtained by comparing $T_n^{\dCRT}(\srx, \sry, \srz)$ to a null distribution obtained by resampling $\srx_{in} \mid \srz_{in}$ based on the estimated distribution $f(\cdot \mid \widehat \theta_{n,x}(Z_{in})).$ This procedure is summarized in Algorithm \ref{alg:dcrt-hat}. 

  \small
  \begin{center}
	  \begin{minipage}{\linewidth}
		  \begin{algorithm}[H]
			  \nonl  \textbf{Input:}  Data $(\srx,\sry,\srz)$, number of randomizations $M$. \\
			  Learn $\widehat{\theta}_{n,x}(\cdot)$ and $\widehat \mu_{n,x}(\cdot)$ based on $(\srx, \srz)$; learn $\widehat{\mu}_{n,y}(\cdot)$ based on $(\sry, \srz)$\;
			  Compute $T_n^{\dCRT}(\srx, \sry, \srz)$ as in \eqref{eq:dCRThat}\;
			  \For{$m = 1, 2, \dots, M$}{
				  Sample $\srxk^{(m)}|\srx, \sry, \srz \sim \prod_{i = 1}^n f(\cdot\mid\widehat{\theta}_{n,x}(\srz_{in}))$ and compute 
				  \begin{equation}
					  T_n^{\dCRT}(\srxk^{(m)}, \srx, \sry, \srz) \equiv \frac{1}{n}\sum_{i = 1}^n (\srxk^{(m)}_{in} - \widehat \mu_{n,x}(\srz_{in}))(\sry_{in} - \widehat \mu_{n,y}(\srz_{in})); \label{eq:resampled-dcrt-def}
				  \end{equation}
			  }
			  \nonl \textbf{Output:} $\dCRT$ $p$-value $\frac{1}{M+1} (1+ \sum_{m=1}^M\indicator\{T_n^{\dCRT}(\srxk^{(m)}, \srx, \sry, \srz) \geq T_n^{\dCRT}(\srx, \sry, \srz)\}).$
			  \caption{\bf $\dCRT$ procedure with exponential family for $\law_n(\prx \mid \prz)$}
			  \label{alg:dcrt-hat}
		  \end{algorithm}
	  \end{minipage}
  \end{center}
  \normalsize

  Unlike dCRT procedure, \textit{generalized covariance measure} (GCM) test uses the asymptotic distribution of the same test statistic \eqref{eq:dCRThat} to find the $p$-value. Therefore, the computation of the $p$-value is much faster than that of $\dCRT$ because it does not require repeated resampling. However, the resampling nature of $\dCRT$ (Algorithm \ref{alg:dcrt-hat}), though computationally intensive, can lead to improved finite-sample performance. Recall Figure~\ref{fig:dCRT_GCM_binomial_poisson}. The key insight is that the GCM test, which relies on asymptotic normality, can suffer from slow convergence in the presence of excessive sparsity in the data. In contrast, $\dCRT$ leverages resampling, and the resulting distribution of the resampled test statistic can more accurately approximate the sampling distribution of the observed statistic. Such intuition can be mathematically justfied in an idealized setup and we show how $\GCM$ can suffer from the slow convergence rate of Type-I error while $\dCRT$ preserves the Type-I error control in Theorem~\ref{thm:illustrative} in Appendix~\ref{sec:finite_sample}. 
  
  We refer reader to Appendix~\ref{sec:additional-details-background} for more details on the comparison between $\GCM$ and $\dCRT$ and their common features such as double robustness (Appendix~\ref{sec:double-robustness}).
  
  \subsection{The $\spacrt$}
  
  The resampling procedure in Algorithm \ref{alg:dcrt-hat} is slow and can be infeasible for large datasets. In this section, we propose a new test, the \textit{saddlepoint approximation-based conditional randomization test} ($\spacrt$), which is completely resampling-free and has similar finite-sample performance as $\dCRT$. If we consider the limit of the dCRT $p$-value as the number of resamples $M$ grows indefinitely, we obtain
  \begin{align}\label{eq:dCRT-p-value-limit}
  p_{\dCRT} \equiv \P\left[T_n^{\dCRT}(\widetilde{X},X,Y,Z)\geq T_n^{\dCRT}(X,Y,Z) \mid X,Y,Z\right].
  \end{align}
  We approximate this conditional tail probability via the SPA. Note that the resampled test statistic defined in \eqref{eq:resampled-dcrt-def} is the mean of  conditionally independent random variables:
  \begin{align*}
	T_n^{\dCRT}(\srxk^{(m)}, \srx, \sry, \srz) \equiv \frac{1}{n}\sum_{i=1}^n W_{in},\ W_{in}\equiv a_{in}(\widetilde X_{in}-\widehat{\mu}_{n,x}(Z_{in})),\ a_{in}\equiv Y_{in}-\widehat\mu_{n,y}(Z_{in}).
  \end{align*}
  Indeed, $W_{in}$ are independent, but not identically distributed, conditionally on the $\sigma$-algebra $\mathcal{F}_n \equiv \sigma(\srx,\sry,\srz)$. Fitting the formulation to \eqref{eq:tail-probability}, we will use $K_{in}(s|\mathcal{F}_n)\equiv K_{in}(s)$ and $K_{n}(s|\mathcal{F}_n)\equiv K_{n}(s)$ as defined in \eqref{eq:CCGF_def}. The closed-form expressions for the $K_n(s|\mathcal{F}_n)$ and its derivatives under natural exponential family \eqref{eq:NEF} are available in Appendix~\ref{sec:proof_general_results}.
  \small
  \begin{center}
	  \begin{minipage}{\linewidth}
		  \begin{algorithm}[H]
			  \nonl  \textbf{Input:}  Data $(\srx,\sry,\srz)$. \\
			  
			  Learn $\widehat \theta_{n,x}(\cdot)$ and $\widehat \mu_{n,x}(\cdot)$ based on $(\srx, \srz)$, $\widehat{\mu}_{n,y}(\cdot)$ based on $(\sry, \srz)$\;
			  
			  Compute $T_n^{\dCRT}(\srx, \sry, \srz)$ as in \eqref{eq:dCRThat}\;
  
			  Find $\hat s_n$ that solves the saddlepoint equation
			  \begin{align}\label{eq:saddle_equation_nef}
				  K'_n(s \mid \mathcal F_n)= T_n^{\dCRT}(\srx,\sry,\srz);
			  \end{align}\\
  
			  Compute $\lambda_n = \sqrt{n} \hat s_n \sqrt{K_n''(\hat s_n\mid \mathcal{F}_n)}$ and 
			  \begin{align*}
				  r_n = 
				  \begin{cases}
					  \sgn(\hat s_n) \sqrt{2n( \hat s_n T_n^{\dCRT} - K_n(\hat s_n\mid\mathcal{F}_n))}, & \text{if } \hat s_n T_n^{\dCRT} - K_n(\hat s_n\mid\mathcal{F}_n)\geq 0;\\
					  \mathrm{sgn}(\hat s_n) & \text{otherwise}.
				  \end{cases}
			  \end{align*}
			  \nonl \textbf{Output:} $\spacrt$ $p$-value
		\begin{equation} \label{eq:p_spacrt_def}
		\smash{p_{\spacrt} \equiv 1-\Phi(r_n)+\phi(r_n)\left\{\frac{1}{\lambda_n}-\frac{1}{r_n}\right\}.}
		\end{equation}
			  \caption{\bf $\spacrt$ procedure}
			  \label{alg:spacrt}
		  \end{algorithm}
	  \end{minipage}
  \end{center}
  \normalsize
  Then we can apply the result in Theorem~\ref{thm:unified_unnormalized_moment_conditions} to obtain a closed-form approximation towards $\dCRT$ $p$-value \eqref{eq:dCRT-p-value-limit}. To this end, we present the $\spacrt$ procedure (Algorithm~\ref{alg:spacrt}). The $\spacrt$ procedure is attractive because it is completely resampling-free. It requires the following one-time computations: fitting the estimates $\widehat \theta_{n,x}$ and $\widehat \mu_{n,y}$, calculating the test statistic $T_n^\dCRT$, and finding the solution to the saddlepoint equation~\eqref{eq:saddle_equation_nef}. The latter is a one-dimensional root-finding problem and can be solved efficiently using standard numerical optimization algorithms. 
  
  To make the $\spacrt$ procedure more concrete, we provide an example in the case that $\prx$ is binary, a setting that matches our motivating application.
  
  \begin{example}[Bernoulli sampling]
	Suppose $\prx\mid\prz\sim \mathrm{Ber}(\mu_{n,x}(\prz))$, and $\theta_{n,x}(\prz) = \text{logit}(\mu_{n,x}(\prz))$. Then, we have $A(\theta) = \log(1 + \exp(\theta))$. After some manipulation, the saddlepoint equation reduces to 
	  \begin{align*}
		  \frac{1}{n}\sum_{i=1}^n (\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in}))(\srx_{in}-\text{expit}(\widehat \theta_{n,x}(\srz_{in})+s(\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in})))=0.
	  \end{align*}
	  Defining $\widetilde \mu_{n,x}(Z_{in}) \equiv \text{expit}(\widehat \theta_{n,x}(\srz_{in})+\hat s_n(\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in})))$ for convenience, $\lambda_n$ and $r_n$ can be computed as $\lambda_n=\hat s_n \sqrt{\sum_{i=1}^n (\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in}))^2\widetilde \mu_{n,x}(Z_{in})(1-\widetilde \mu_{n,x}(Z_{in}))}$ and
	  \begin{align*}
		  r_n=\mathrm{sgn}(\hat s_n)\sqrt{2\sum_{i=1}^n \left(X_{in} \log \frac{\widetilde \mu_{n,x}(Z_{in})}{\widehat \mu_{n,x}(Z_{in})} + (1 - X_{in})\log \frac{1 - \widetilde \mu_{n,x}(Z_{in})}{1 - \widehat \mu_{n,x}(Z_{in})}\right)},
	  \end{align*}
	or simply $\text{sgn}(\hat s_n)$ if the quantity under the square root is negative. Putting these pieces together, the $\spacrt$ $p$-value can be computed as in equation~\eqref{eq:p_spacrt_def}.	
  \end{example}
  
  \section{Theoretical results of the $\spacrt$}\label{sec:general_results}
  
  $\spacrt$ does not require any resampling and thus has a significant advantage over $\dCRT$ in terms of computation. This advantage does not come with a sacrifice of statistical accuracy. In this section, we establish the theoretical properties of the $\spacrt$.
  
  \subsection{General theory: approximation and Type-I errors}
  
  We first state general results concerning the approximation accuracy of the $\spacrt$ $p$-value and then prove the asymptotic Type-I error control of $\spacrt$.
  
  \begin{theorem}[Approximation accuracy]\label{thm:validity_spacrt}
	Suppose there exists $S>0$ such that \textbf{one} of the following conditions holds:
	\begin{align}
	  \sup_{i}|\widehat{\theta}_{n,x}(\srz_{in})|,\ \sup_{i}|\widehat{\mu}_{n,y}(\srz_{in})| = O_{\P}(1),\  \P[\sry_{in}\in [-S,S]]=1\text{ for any }i,n\label{eq:cse_assumption}\tag{CSE};\\
	  \frac{1}{n}\sum_{i=1}^n (\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in}))^4=O_{\P}(1),\ \P\left[\srxk_{in}\in [-S,S]\right]=1\text{ for any }i,n\label{eq:ccs_assumption}\tag{CCS}.
	\end{align}
	  Suppose the following conditions hold:
	  \begin{align}
		  |\widehat \theta_{n,x}(\srz_{in})|<\infty,|\widehat \mu_{n,y}(\srz_{in})|<\infty\text{ for any $i,n$ almost surely};\label{eq:upper_bound_theta_a} \\
		  \frac{1}{n}\sum_{i=1}^n (Y_{in}-\widehat{\mu}_{n,y}(Z_{in}))^2 A''(\widehat \theta_{n,x}(\srz_{in}))=\Omega_{\P}(1);\label{eq:lower_bound_spacrt}\\
		  T_n^{\dCRT}(\srx,\sry,\srz)\convp 0\label{eq:x_n_convergence_spacrt}.
	  \end{align}
	  Then, the saddlepoint equation~\eqref{eq:saddle_equation_nef} has a unique and finite solution $\hat s_n \in [-1/16, 1/16]$ with probability approaching 1 as $n \rightarrow \infty$. Furthermore, $\P\left[p_{\spacrt}>0\right]\rightarrow1 \text{ as }n\rightarrow\infty$ and the spaCRT $p$-value $p_{\spacrt}$ approximates the dCRT $p$-value $p_{\dCRT}$ with vanishing relative error:
	  \begin{align}\label{eq:approximation_accuracy_spacrt}
		  p_{\dCRT} = p_{\spacrt}\cdot (1+o_\P(1))
	  \end{align}
  \end{theorem}
  
  \noindent To better understand the assumptions in Theorem \ref{thm:validity_spacrt}, we provide the following remarks.
  
  \begin{remark}[Comment on condition \eqref{eq:x_n_convergence_spacrt}]\label{rmk:cutoff-regime}
	The role of the condition \eqref{eq:x_n_convergence_spacrt} is to guarantee the existence of the solution to the saddlepoint equation. This assumption allows the test statistic $T_n^{\dCRT}(\srx,\sry,\srz)$ to converge to zero in probability, \textbf{at any rate}. In particular, under the null hypothesis and contiguous local alternatives, this condition holds.
  \end{remark}
  
  
  \begin{remark}[Relative error guarantee]
	  The relative error guarantee in conclusion \eqref{eq:approximation_accuracy_spacrt} is a strong result and is a direct consequence of result~\eqref{eq:conclusion_saddlepoint_approximation}. It means not only the difference of $p$-values is close to $0$ with probability approaching 1, but also the ratio of $p$-values is close to 1 with probability approaching 1. This is a particularly desirable property for approximating small $p$-values.
  \end{remark}
  
  Defining the level-$\alpha$ tests associated with the dCRT and spaCRT $p$-values:
  \begin{align*}
  \phi^{\dCRT}_{n,\alpha} \equiv \indicator(p_{\dCRT} \leq \alpha) \quad \text{and} \quad	\phi^{\spacrt}_{n,\alpha}\equiv \indicator\left(p_{\spacrt} \leq \alpha\right),
  \end{align*}
  the following theorem states that the asymptotic Type-I error control of the spaCRT follows from that of the dCRT.
  
  \begin{corollary}[Asymptotic validity of $\spacrt$]\label{cor:asymptotic_validity_spacrt}
	Suppose the assumptions of Theorem \ref{thm:validity_spacrt} hold. Fix $\alpha\in (0,1)$. If $\lim_{n\rightarrow\infty}\P_{H_0}[p_{\dCRT}\leq \alpha]\leq \alpha$, then we have 
	\begin{align*}
	  \lim_{n\rightarrow\infty}\P_{H_0}[p_{\spacrt}\leq \alpha]\leq \alpha.
	\end{align*}
  \end{corollary}
  Corollary \ref{cor:asymptotic_validity_spacrt} states the asymptotic validity of $\spacrt$ given the asymptotic validity of $\dCRT$. $\dCRT$ is proposed originally assuming the exact knowledge of $\law_n(\prx\mid\prz)$, or the so-called Model-X assumption. Under the model-X assumptions, the $p$-value produced by $\dCRT$ procedure is exact, i.e., $\P_{H_0}[p_{\dCRT}\leq \alpha]\leq \alpha$. Therefore, $p$-value obtained from $\spacrt$ procedure is asymptotically valid under the assumptions of Theorem \ref{thm:validity_spacrt}. For asymptotic validity of $\dCRT$ with general in-sample fit $\lawhat_n(\prx\mid\prz)$, we refer reader to the detailed discussion in \citet{Niu2022a}.
  
  \subsection{Case studies with modern regression techniques}\label{sec:case_study}
  
  We will dedicate this section to special cases to show approximation accuracy of $\spacrt$ and its validity under null hypothesis. In particular, we investigate the performance of $\spacrt$ when $\pry|\prz$ is estimated using modern regression techniques. We will consider low-dimensional and high-dimensional generalized linear regressions in the main text and we postpone the discussion of the nonparametric regression with kernel ridge regression to Appendix~\ref{sec:theory_nonparametric_RF_classification}. We need two assumptions to state the results.
  \begin{assumption}\label{assu:non_degeneracy_variance}
  $0<\inf_n\E[(\srx_{in}-\E[\srx_{in}\mid \srz_{in}])^2(\sry_{in}-\E[\sry_{in}\mid \srz_{in}])^{2}]$.
  \end{assumption}
  
  \begin{assumption}\label{assu:compact_support_Z}
	Support of $\prz\in\mathbb{R}^d$ is compact, i.e., $\|\prz\|_{\infty}\leq C_Z$ for $C_Z\in(0,\infty)$.
  \end{assumption}
  
  \noindent Throughout this section, we will consider the following logistic model for $\prx|\prz$:
  \begin{align*}
	\prx|\prz\sim \mathrm{Ber}(\expit(\prz^\top \theta_n)).
  \end{align*}
  The choice of logistic model is mainly for the ease of the theoretical analysis. In fact, the $\spacrt$ can be easily integrated with diverse fitting methods beyond just logistic regression under binary valued $\prx$, for example hidden Markov models (in Section~\ref{sec:GWAS}) and nonparametric random forest classification (in Appendix \ref{sec:simulation_nonparametric_RF_classification}).
  
  
  \paragraph{Low-dimensional generalized linear regression.}
  
  Suppose we are under the classical low-dimensional setup so that we write $(\srz_{in},\srx_{in},\sry_{in})=(\srz_{i},\srx_{i},\sry_{i})$ and $\theta_n=\theta$. To echo Figure \ref{fig:dCRT_GCM_binomial_poisson}, we first consider the conditional distribution $\pry\mid\prz$ to be $\mathcal{P}_y(A'_y(\prz^\top\beta))$ where $\mathcal{P}_y$ is a distribution in the natural exponential family \eqref{eq:NEF} and $A_y$ is the log-parition functions for $\pry|\prz$. Now we state our result.
  
  \begin{theorem}\label{thm:low_dim_glm_spacrt}
	Consider $\prx\indep\pry|\prz$. Suppose assumptions \ref{assu:non_degeneracy_variance}-\ref{assu:compact_support_Z} hold. Then if the maximum likelihood estimates (MLEs) $\widehat{\beta},\widehat{\theta}$ satisfy
	\begin{align}\label{eq:GLM_theta_consistency}
	  \|\widehat{\beta}-\beta\|_1=O_\P(1/\sqrt{n})\quad\text{and}\quad\|\widehat{\theta}-\theta\|_1=O_\P(1/\sqrt{n}),
	\end{align}
	then the conclusion in Theorem \ref{thm:validity_spacrt} holds and we have $\phi_{n,\alpha}^\mathrm{\spacrt}$ is asymptotically valid, i.e. $\lim_{n\rightarrow\infty}\E[\phi_{n,\alpha}^{\spacrt}]=\alpha$.
  \end{theorem}
  We want to emphasize the $\sqrt{n}$ rate condition in \eqref{eq:GLM_theta_consistency} is classical for MLEs under mild regularity conditions.
  
  
  \paragraph{High-dimensional regression.}
   
  We establish the validity of the $\spacrt$ under the following generalized linear models for $\pry|\prz$ $\pry|\prz\sim \mathcal{P}_y(A'_{y}(\prz^\top \beta_{n}))$ and this setup is similar to the low-dimension case in the previous part while here we allow dimension of $\prz$ (as well as $\beta_n$) to grow with sample size $n$. We will demonstrate how $\spacrt$ can be used in the presence of high-dimensional parameters. In particular, we consider the estimators $\widehat{\beta}_n,\widehat{\theta}_n$ for $\beta_n,\theta_n$ are obtained from the lasso estimators \citep{tibshirani1996regression}. The definitions of these estimators are standard and can be found in Appendix~\ref{sec:proof_high_dim_spacrt_equivalence}. Recovering the true parameters $\beta_n,\theta_n$ is challenging (sometimes even unidentifiable) in the high-dimensional setting unless certain structure assumptions on these parameters are imposed. We show that the $\spacrt$ is valid under the sparse signal setup. We start by stating the following assumption on the covariate distribution $\prz$.
  
  \begin{assumption}[Design assumption]\label{assu:design_assumption}
	Suppose the distribution of $\prz$ satisfies
	\begin{align}
	  \text{minimum eigenvalue of }\E[\prz\prz^\top]\text{ is boundeed away from }0;\label{eq:boundedness_eigenvalue}\\
	  \E[\langle \bm \eta, \prz \rangle^4]\leq \kappa <\infty,\  \forall \eta\in\mathbb{R}^{d}, \|\bm\eta\|_2=1.\label{eq:fourth_moment_XZ}
	\end{align} 
  \end{assumption}
  
  \begin{theorem}\label{thm:high_dim_glm_spacrt}
	Consider $\prx\indep\pry|\prz$. Suppose assumptions \ref{assu:non_degeneracy_variance}-\ref{assu:design_assumption} hold and $\exists\delta\in(0,1)$,
	\begin{align}
	  \max\left\{1,s_{\theta_n},s_{\beta_n}\right\} \sqrt{\log(d)/n}\sim n^{-\delta}\text{ where }(s_{\beta_n},s_{\theta_n})\equiv (\|\beta_n\|_0,\|\theta_n\|_0);\label{eq:sparsity_condition}\\
	  \sup_n\|\theta_n\|_1<\infty,\ \sup_n\|\beta_n\|_1<\infty.\label{eq:boundedness_coefficients}
	\end{align}
	Then if we choose $\lambda_n=C_{\lambda} \sqrt{\log(d)/n}$ and $\nu_n=C_{\nu}\sqrt{\log(d)/n}$ for some universal constants $C_\lambda,C_\nu$, the conclusion in Theorem \ref{thm:validity_spacrt} hold. If additionaly, we have 
	\begin{align}\label{eq:product_sparsity_condition}
	  s_{\theta_n}\cdot s_{\beta_n}\cdot \frac{\log(d)}{n^{1/2}}=o(1)
	\end{align}
	then $\phi_{n,\alpha}^\mathrm{\spacrt}$ is asymptotically valid, i.e. $\lim_{n\rightarrow\infty}\E[\phi_{n,\alpha}^{\spacrt}]=\alpha$ for any $\alpha\in(0,1)$. 
  \end{theorem}
  
  Let us comment on the assumptions required. Assumption \ref{assu:design_assumption} imposes the structures on the covariate distribution which are commonly required in the high-dimensional regression. In fact, conditions \eqref{eq:boundedness_eigenvalue}-\eqref{eq:fourth_moment_XZ} are also required in Example 9.17 and Theorem 9.36 in \citet{wainwright2019high} when proving the \textit{restricted strong convexity}, a fundamental property involved when proving consistency results of high-dimensional regression coefficients (see for example Corollary 9.26 in \citet{wainwright2019high}). Condition \eqref{eq:sparsity_condition} regulates the rate growth between $s_{\theta_n},s_{\beta_n},d$ and $n$. The boundedness condition \eqref{eq:boundedness_coefficients} is a relatively mild condition required by showing the almost sure convergence of $\widehat{\mu}_{n,y}(\cdot), \widehat{\theta}_{n,x}(\cdot)$ when verifying condition \eqref{eq:upper_bound_theta_a}.
  
  
  \section{Application to data with excessive sparsity}\label{sec:simulation} 
  
  As discussed in Section \ref{sec:introduction}, the $\spacrt$ method can be very useful when it comes to analysis with very sparse data, i.e. excessive zeros in outcome data $Y$ and/or in treatment $X$. In the following sections, we evaluate the finite-sample performances of $\spacrt$ in two simulation setups inspired by single-cell CRISPR screens analysis and GWAS with rare disease and rare genetic variant, respectively.
  
  \subsection{Single-cell CRISPR screens analysis}\label{sec:CRISPR-screens}
  
  \paragraph{Simulation setup:}
  
  To mimic the application of single-cell CRISPR screens, we model $\prx\mid\prz$ as a logistic regression model and $\pry\mid\prz$ as a negative binomial regression model \citep{Barry2024,Katsevich2020c,Gasperini2019a}. The latter modeling choice is quite common not just in single-cell CRISPR screen analysis but in single-cell RNA-seq analysis more broadly \citep{Huang2018, Townes2019, Svensson2020}. We consider the following data-generating model:
  \small
  \begin{align}\label{eq:simulation-model-CRISPR-screens}
	\bm Z\sim N(0,1); \	\bm X \mid \bm Z\sim \textnormal{Ber}(\expit(\gamma_0+\bm Z));\ \bm Y \mid \bm X,\bm Z\sim \textnormal{NB}(\exp(\beta_0+\rho\bm X+\bm Z),r),
  \end{align}
  \normalsize
  where $r > 0$ is the \textit{size parameter} controlling the overdispersion of the negative binomial distribution. Here, $\prx$, $\pry$, and $\prz$ represent the indicator of perturbation presence, gene expression, and a single covariate with a confounding effect, respectively. The parameters $\gamma_0$ and $\beta_0$ control the proportion of cells with perturbations and the mean expression of the gene, respectively, and therefore control the sparsity level of $\srx$ and $\sry$. The source for the sparsity from real data can be found in Appendix~\ref{sec:source_sparsity_CRISPR}. In this setup, we consider testing for association between a single CRISPR perturbation and a single gene. The parameter $\rho$ controls the strength of the signal, i.e., the dependence of $\pry$ on $\prx$ conditional on $\prz$. Therefore, $\rho = 0$ and $\rho \neq 0$ corresponds to the null and alternative hypotheses, respectively. 
  
  
  \paragraph{Methodologies compared:}
  
  
  We compare the four tests: $\spacrt,\dCRT,\GCM$ test and score test. The detailed discussion of the methods can be found in Appendix~\ref{sec:simulation_methods_CRISPR_screens}. Note we use $M=10,000$ resamples for $\dCRT$ for the accuracy of $p$-value.
  
  
  
  \paragraph{Simulation results}
  
  Here, we present a representative selection of simulation results (Figure~\ref{fig:simulation-summary}). These results correspond to $r = 0.05$ and $\beta_0 = -5$, and all tests are applied at nominal level $\alpha = 0.01$. Additional results are provided in Appendix~\ref{sec:additional_simulation_results_CRISPR}. We find from Figure~\ref{fig:simulation-summary}a, which displays $p$-value distributions under the null hypothesis, that the $\spacrt$ and $\dCRT$ tests have similar $p$-value distributions, both of which are close to uniform. Meanwhile, the GCM test behaves too liberally for left-sided tests and too conservatively for right-sided tests, while the score test behaves too conservatively for left-sided tests and too liberally for right-sided tests. These trends are reflected in the Type-I error rates and powers in Figure~\ref{fig:simulation-summary}b,c. We remark that the $\spacrt$ and $\dCRT$ tests control Type-I error for all settings of $\gamma_0$, though both tests tend to become conservative as $\srx$ becomes sparser. Furthermore, the $\spacrt$ and $\dCRT$ are the most powerful tests among those that have Type-I error control for every parameter setting.
  \begin{figure*}[!ht]
	  \centering
	  \includegraphics[width=1.0\textwidth]{figures-and-tables/simulation/NB-regression/summary/simulation-summary.pdf}
	  \caption{\small{Summary of CRISPR screens simulation results for size parameter $r = 0.05$ and the significance level is set to $0.01$. (a) QQ-plots of the $p$-values obtained under the null hypothesis for $(\gamma_0, \beta_0) = (-3, -5)$. (b) Type-I error rates for $\beta_0 = -5$ as a function of the sparsity of $\srx$ ($\gamma_0$). (c) Power for $(\gamma_0, \beta_0) = (-3, -5)$ as a function of the signal strength ($\rho$). (d) The boxplot shows the time consumed across different simulation parameters while each point is the averaged time consumption over $10,000$ replications.}}
	  \label{fig:simulation-summary}
  \end{figure*}
  
  Next, we remark on how the methods' performance is impacted by the problem parameters $\gamma_0$ and $\beta_0$. As either $\srx$ or $\sry$ becomes less sparse (i.e., as $\gamma_0$ or $\beta_0$ increase), the $p$-value distributions, Type-I error rates, and powers for the GCM and score tests improve. This is to be expected, as the test statistics converge more quickly towards the standard normal distribution when the quantities being averaged are less sparse. We defer the disucssion on size parameter $r$ to Appendix \ref{sec:additional_simulation_results_CRISPR} where we will show $\dCRT$ and $\spacrt$ are more robust to the values of $r$ than the GCM and score tests.
  
  
  Moreover, Figure~\ref{fig:simulation-summary}d displays the computing times for the different methods in the settings considered in Figure~\ref{fig:simulation-summary}a,b,c. We see that the spaCRT and GCM test are roughly tied for fastest, the score test is roughly half an order of magnitude slower than these two, while the dCRT is more than an order of magnitude slower. 
  
  Even though the CRISPR screens analysis is often concerned about multiple genes and perturbations, our simulation setup already captures the relevant statistical phenomena on the single perturbation-gene pair. To complete our narrative, we treat the $p$-values obtained from the simulation setup as arising under a multiple testing regime, and report the number of rejections after applying multiplicity corrections in Appendix~\ref{sec:additional_simulation_results_CRISPR}. These results are consistent with the simulations presented in Figure~\ref{fig:simulation-CRISPR-screens-high-multiplicity-disp-0.05}. 
  
  \subsection{GWASs with rare diseases and genetic variants}\label{sec:GWAS}
  
  \paragraph{Simulation setup:}
  
  The goal of identifying genetic variaions associated with phenotypes in GWAS can be formulated as a conditional independence testing problem \citep{sesia2019gene}:
  \begin{align}\label{eq:variable_selection}
	H_0^j:\prx_j\indep \pry\mid \prx_{-j}\quad j=1,\ldots,d
  \end{align}
  where $\prx=(\prx_1,\ldots,\prx_d)^\top\in\{0,1\}^d$ is typically referring to the presence or absence of a specific allele at a given locus and $\pry$ is the phenotype of interest. Often time, a large scale loci are considered simultaneously. In other words, this is a high-dimensional setup and we consider $\prz\equiv \prx_{-j}$ as covariate. We consider a binary outcome $\pry$, which may be viewed as indicator of the presence of a disease:
  \begin{align*}
	  \bm Y \mid \bm X\sim \textnormal{Ber}(\expit(\gamma_0 + \prx^\top \beta)).
  \end{align*}
  Then under this model, the variable selection problem \eqref{eq:variable_selection} is now euqivalent to testing $\beta_j=0$ or not \citep{CetL16}. Similarly, $\gamma_0$ here controls the sparsity of outcome $Y$ and we consider $\{-3,-2\}$ for \textit{high} and \textit{low} sparsity setting, mimicking the relative frequency of phenotype occurrence. More explanation on the source of sparsity can be found in Appendix~\ref{sec:simulation_methods_GWAS}.
  
  We consider the genetic variable $\prx \in\{0,1\}^{d}$ is generated from a \textit{hidden Markov model} (HMM). HMMs have been broadly adopted to describe haplotypes, the sequence of alleles at a series of markers along one chromosome, and offer a good phenomenological description of the dependence between the explanatory variables in GWAS \citep{scheet2006fast,marchini2007new,browning2007rapid}. A brief introduction of the HMM can be found in Appendix \ref{sec:HMM}. We consider two sets of parameters generating HMMs with one setup mimicking the rare genetic variation scenario (high sparsity in $X$) and the other setup mimicking the common genetic variation scenario (low sparsity in $X$). 
  
  We will consider $d=500$ and sample size $n=2000$ in the simulation and leave the other details on parameters used in the simulation to Appendix \ref{sec:simulation_methods_CRISPR_screens}. 
  
  \paragraph{Methodologies compared:}
  
  We consider the four tests: $\spacrt,\dCRT,\GCM$ test and the Knockoff procedure \citep{barber2015controlling}. Methods $\spacrt,\dCRT$ and $\GCM$ use the same fitting methods and only differ on their calibration approach for the $p$-values. The method details can be found in Appendix~\ref{sec:additional_details_GWAS}. We apply a both-sided test for all four methods. For $\spacrt,\dCRT$ and $\GCM$, the $p$-values are corrected for multiple testing using the Benjamini-Hochberg procedure \citep{benjamini1995controlling}. The FDR is controlled at level $0.1$.
  
  \paragraph{Simulation results}
  \begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\textwidth]{figures-and-tables/simulation/HMM-variable-selection/HMM_simulation_skew_lambda.1se.pdf}
	\caption{\small{Summary of numerical simulation results GWAS with high sparsity in data $X$. All the results are obtained with the regularization parameter $\lambda=\texttt{lambda.1se}$. (a) FDR for $\gamma_0=-3$ (high sparsity) and $\gamma_0=-2$ (low sparsity). (b) Power for the same set of $\gamma_0$. (c) Time consumed by different methods across $50$ replications and all the varying parameters.}}
	\label{fig:simulation-summary-GWAS-skew-lambda-1se}
  \end{figure}
  We present a representative selection of simulation results (Figure~\ref{fig:simulation-summary-GWAS-skew-lambda-1se}) with high sparsity in $X$ and the other results can be found in Appendix \ref{sec:simulation_results_GWAS}. Note that the Knockoff procedure does not provide $p$-values but just a rejection set. Thus controlling family-wise error rate (FWER) is more challenging for the Knockoff procedure due to the lack of $p$-values. Therefore we only present the FDR plot (Figure~\ref{fig:simulation-summary-GWAS-skew-lambda-1se}a) and power plot (Figure~\ref{fig:simulation-summary-GWAS-skew-lambda-1se}b) in this simulation setup. We find from Figure~\ref{fig:simulation-summary-GWAS-skew-lambda-1se}a that the $\spacrt$ and $\dCRT$ tests have similar FDR, both of which are close to the nominal level. Meanwhile, the $\GCM$ test behaves too liberally due to excessive sparsity in $X$ and $Y$ although the degree of inflation is more severe when $\gamma_0=-3$. The Knockoff method can also control FDR while being conservative when signal is weak. Figure~\ref{fig:simulation-summary-GWAS-skew-lambda-1se}b shows $\GCM,\dCRT$ and $\spacrt$ have similar power whereas Knockoff procedure tends to be less powerful. We believe the comparably low power in Knockoff may be due to sensitivity of Knockoff procedure to the distribution of $\prx$, which can heavily affect the construction of decorrleated knockoff variable $\tilde \prx$ to observed variable $\prx$. Therefore we are reluctant to claim that Knockoff procedure is generally less powerful but postpone the more comprehensive investigation to future work. 
  
  Figure~\ref{fig:simulation-summary-GWAS-skew-lambda-1se}c shows the time consumption for different methods. Among all methods, Knockoff is the computationally most efficient method and this hinges on the fact that only one high-dimensional regression fit of $\pry|\prx,\tilde{\prx}$ is required, where here $\tilde{\prx}$ is the constructed knockoff variable. On the other hand, the other three methods involve $d$ regressions for $\pry|\prx_{-j},j\in[d]$ if no further acceleration is applied, not to mention the excessive the resampling required in $\dCRT$. With this being said, we employ a \textit{tower trick} to boost the computation of $\widehat \E[\pry|\prx_{-j}]$ for $\dCRT,\GCM$ and $\spacrt$. The implementation details can be found in Appendix \ref{sec:tower-trick}. The acceleration can push the computation time of $\spacrt$ and $\GCM$ to be within a factor of 2 to 3 of the Knockoff procedure while being much faster than the $\dCRT$. Note we use \texttt{R} package \texttt{SNPknock} to generate knockoff variable and the construction procedure is implemented in \texttt{Rcpp} for speed. Currently, $\spacrt$ is implemented mostly in \texttt{R} so we expect further speedup when employing more efficient implementation. 
  
  \section{Real data analysis} \label{sec:real_data}
  
  In this section, we compare the performance of the $\spacrt$ to those of alternative methods on the analysis of the \citet{Gasperini2019a} single-cell CRISPR screen dataset. We refer reader to Appendix~\ref{sec:additional-details-real-data} for detailed description of the dataset.
  
  \subsection{Analyses conducted}
  
  \paragraph{Hypotheses tested.} In order to assess the Type-I error and power of the methods compared, we will use CRISPR perturbations intended as negative (under null) and positive controls (under alternative), respectively. In particular, for Type-I error analysis, we test for association between each of the 51 negative control perturbations and each of 3,000 randomly sampled genes, for a total of 51 $\times$ 3,000 = 153,000 tests. We subsample the genes to reduce the computational burden of the analysis. To assess the power of each method, we test for association between each of the 754 positive control perturbations targeting genes and the gene they target, for a total of 754 tests. 
  
  \paragraph{Methods compared.} We compare essentially the same methods as in the numerical simulations (recall Section~\ref{sec:CRISPR-screens}). The only difference is that we replace the dCRT with a faster variant implemented in the R package \verb|sceptre| \citep{Barry2024,Katsevich2020c}, in order to make the analysis computationally feasible. The \verb|sceptre| implementation of the dCRT fits a parametric curve to the resampling distribution of the test statistic based on a smaller number of resamples (a heuristic acceleration that is not theoretically justified). Furthermore, it is implemented in C++ for speed, unlike the other methods we consider, which are implemented in R. We apply left- and right-sided variants of each test on the negative control perturbation-gene pairs. For the positive control pairs, we apply only left-sided tests, since we are testing for a perturbation-induced decrease in gene expression.
  
  \subsection{Results}
  
  \paragraph{Type-I error.} Figure~\ref{fig:summary-real-data}a displays QQ plots of the negative control $p$-values obtained from all four methods. The two tests relying on asymptotic normality, the GCM and score tests, exhibit severe $p$-value inflation for left- and right-sided tests, respectively. This finding is consistent with our simulation results (Figure~\ref{fig:simulation-summary}). On the other hand, the $\spacrt$ and \verb|sceptre| tests control Type-I error well for both left- and right-sided tests. We also report the number of false discoveries on the negative control pairs in Appendix~\ref{sec:additional_table_realdata}; the message from these results are consistent with that of the single testing results.
  \begin{figure*}[!ht]
	  \centering
	  \includegraphics[width=1.0\textwidth]{figures-and-tables/combined_summary.pdf}
	  \caption{\small{Calibration and power performance in the \citet{Gasperini2019a} data. (a): Left- and right-sided $p$-values for negative control perturbation-gene pairs on the Gasperini data. (b): Left-sided $p$-values computed on the 754 positive control perturbation-gene pairs in the Gasperini data.}}
	  \label{fig:summary-real-data}
  \end{figure*}
  
  Next, we investigate the impact of the problem sparsity on calibration. Following \citep{Barry2024}, we measure sparsity in terms of the \textit{effective sample size} $\sum_{i = 1}^n \indicator(X_{i} Y_{i} > 0)$, which measures the number of cells with a given perturbation and nonzero expression of a given gene. Table~\ref{tab:sparsity_level_ess} displays the distribution of effective sample sizes across the negative control perturbation-gene pairs tested, showing that the effective samples sizes are vastly smaller than the number of cells, $n =$ 207,324. Furthermore, Figure~\ref{fig:qqplot_lowess} stratifies the QQ plots for each method by effective sample size, focusing on those pairs with effective sample size of at most 100. As expected based on our simulation study, we find more severe miscalibration for pairs with lower effective sample sizes, especially for the GCM and score tests, and to a lesser extent for \verb|sceptre| and the $\spacrt$. We carried out a similar analysis, stratifying the pairs based on the estimated size parameter (Figure~\ref{fig:qqplot_dispersion}). In line with our simulation results, we find that the GCM and score tests exhibit more miscalibration for smaller size parameters. 
  
  \begin{table*}[h!]
	\centering
	\begin{tabular}[t]{lccccc}
	\toprule
	  & Min. & 1st Qu. & Median & 3rd Qu. & Max.\\
	\midrule
	Effective sample size & 0   &   53   &  204  &   504  & 2044 \\
	\bottomrule
	\end{tabular}
	\caption{Effective sample size in subsampled dataset.}
	\label{tab:sparsity_level_ess}
  \end{table*}
  
  \paragraph{Power.}
  
  Next, we compare the power of the four methods based on their left-sided $p$-values on the 754 positive control perturbation-gene pairs (Figure~\ref{fig:summary-real-data}b). The signal is quite strong in these positive control pairs, as evidenced by small $p$-values for all four methods. We remark that the $\spacrt$ overcomes the discreteness in the $p$-values returned by resampling-based methods such as the dCRT, delivering very small $p$-values in the presence of strong signals. Given the scale of the $p$-values, we refrain from making definitive conclusions about the relative power of the methods, but remark only that the $\spacrt$ appears at least as powerful as the alternative methods considered.
  
  
  \paragraph{Computation.}
  
  The excellent statistical properties of the $\spacrt$ do not come at the cost of computational efficiency. Since the \verb|sceptre| software is highly optimized, while the other methods are not, we benchmark our dCRT implementation (used in the numerical simulations) instead of \verb|sceptre|'s for computational efficiency. We use $M = $ 100,000 resamples for the dCRT, given the high multiplicity of the problem (recall Section~\ref{sec:statistical-computational-challenges}). To assess running time, we paired two randomly sampled genes with each of the 51 non-targeting perturbations, for a total of 102 perturbation-gene pairs. We find that the $\spacrt$ is roughly as fast as the GCM test, about five times faster than the score test, and about 250 times faster than the dCRT.
  
  \input{figures-and-tables/gasperini-computing-times.tex}
  
  \section{Discussion} \label{sec:discussion}
  
  In this paper, we prove a new SPA for approximating resampling distribution and then introduce the $\spacrt$, a new conditional independence test enjoying several desirable properties building on the new theoretical results. $\spacrt$ is completely resampling-free, applicable to general regression setups and enjoys excellent Type-I error control and power in both numerical simulations and real data analysis. The $\spacrt$ is particularly well-suited to data with low signal-to-noise ratio and large scale, such as single-cell CRISPR screen data analysis.
  
  Despite the attractive properties of the $\spacrt$, the current work still has several limitations. First, there remains a gap between our theoretical guarantees and the practical performance of the $\spacrt$ and $\dCRT$. We have shown that both of these tests have Type-I error control asymptotically. Except for Theorem \ref{thm:illustrative} where we establish the theoretical gain of $\odCRT$ compared to $\oGCM$, we have not theoretically justified why these tests perform so well in finite samples compared with asymptotic tests like the GCM and score tests, when in-sample fitting for $\mu_{n,x}$ and $\mu_{n,y}$ are considered. Second, we have focused on approximating the $\dCRT$ in this paper, but the conditional randomization test framework is applicable to general test statistics. It would be interesting to see how the saddlepoint approximation can be extended to other test statistics.
  
  \section{Acknowledgments}
  
  We acknowledge the Wharton research computing team for their help with our use of the Wharton high-performance computing cluster for the numerical simulations and real data analyses in this paper. This work was partially support by NSF DMS-2113072 and NSF DMS-2310654.



\printbibliography

\clearpage

\appendix


\section{Probability theory preliminaries}

\subsection{Single probability space embedding}
To better state and understand the conditional convergence result, the following lemma helps to embed all the random variables into one big probability space.

\begin{lemma}[Embedding into a single probability space, Lemma 14 in \cite{Niu2022a}]\label{lem:embedding}
	Consider a sequence of probability spaces $\{(\P_n,\Omega_n,\mathcal{G}_n),n \geq 1\}$. For each $n$, let $\{W_{i,n}\}_{i \geq 1}$ be a collection of integrable random variables defined on $(\P_n,\Omega_n,\mathcal{G}_n)$ and let $\mathcal F_n \subseteq \mathcal G_n$ be a $\sigma$-algebra. Then there exists a single probability space $(\widetilde{\P}, \widetilde{\Omega}, \widetilde{\mathcal G})$, random variables $\{\widetilde W_{i,n}\}_{i,n \geq 1}$ on $(\widetilde{\P}, \widetilde{\Omega}, \widetilde{\mathcal G})$, and $\sigma$-fields $\widetilde{\mathcal F}_n \subseteq \widetilde{\mathcal G}$ for $n \geq 1$, such that for each $n$, the joint distribution of $(\{W_{i,n}\}_{i \geq 1}, \{\E[W_{i,n}\mid\mathcal F_n]\}_{i \geq 1})$ on $(\P_n,\Omega_n,\mathcal{G}_n)$ coincides with that of $(\{\widetilde W_{i,n}\}_{i \geq 1}, \{\E[\widetilde W_{i,n}\mid \widetilde{\mathcal F}_n]\}_{i \geq 1})$ on $(\widetilde{\P}, \widetilde{\Omega}, \widetilde{\mathcal G})$.
\end{lemma}
\noindent With the above Lemma, we are safe to state any almost sure statement which can be interpreted within one probability space. 

\subsection{Some facts about natural exponential family}

Consider the NEF with probability density function
\begin{align*}
  f(x|\theta)=h(x)\exp(\theta x-A(\theta)).
\end{align*}
Then there is one-to-one correspondence between the moments of the random variable from NEF and the derivative of the log-partition function $A(\theta)$. We summarise the relationship in the following Lemma.

\begin{lemma}[Chapter 1.2 in \cite{Efron2022}]\label{lem:moment_logpartition}
  Suppose $X\sim f(x|\theta)$ then the following identities hold:
  \begin{enumerate}
    \item $\E[X]=A'(\theta)$;
    \item $\E[X^2]-(\E[X])^2=A''(\theta)$;
    \item $\E[(X-\E[X])^3]=A^{(3)}(\theta)$;
    \item $\E[(X-\E[X])^4]-3\left(\E[(X-\E[X])^2]\right)^2=A^{(4)}(\theta)$.
  \end{enumerate}
\end{lemma}


\section{Additional details of Section~\ref{sec:conditional-spa}}

\subsection{Additional details of Section~\ref{sec:main-results-spa}}\label{sec:additional-details-spa}

Either of the Assumption~\ref{assu:cse} or Assumption~\ref{assu:ccs} in the previous section guarantees the existence of these CGFs and their derivatives in a neighborhood of the origin.
\begin{lemma}\label{lem:finite_cgf}
Suppose Assumption~\ref{assu:cse} or Assumption~\ref{assu:ccs} holds. Then, there exists a probability-one event $\mathcal A$ and an $\varepsilon > 0$ such that, on $\mathcal A$,
\begin{align}
K_{in}(s) < \infty\quad \text{for any } s\in (-\varepsilon,\varepsilon)\ \text{and}\ \text{for all}\ i \leq n,\ n \geq 1 \label{eq:finite_cgf}
\end{align}
and 
\begin{equation}\label{eq:finite_cgf_derivatives}
	|K_{in}^{(r)}(s)| < \infty\quad \text{for any } s\in (-\varepsilon,\varepsilon)\ \text{and}\ \text{for all}\ i \leq n,\ n \geq 1, \ r \geq 1, r\in\mathbb{N},
\end{equation}
where $K_{in}^{(r)}$ denotes the $r$-th derivative of $K_{in}$.
\end{lemma}

We now explicitly define the solution to saddlepoint equation~\eqref{eq:saddlepoint-equation} $\hat s_n$. In particular, we restrict our attention to solutions in the interval $[-\varepsilon/2, \varepsilon/2]$:
\begin{align*}
S_n\equiv \left\{s\in [-\varepsilon/2,\varepsilon/2]:K_n'(s)=w_n\right\}.
\end{align*}
It is possible, for specific realizations of $K'_n(s)$ and $w_n$, that the set $S_n$ is either empty or contains multiple elements. To make the saddlepoint approximation well-defined in these cases, we define $\hat s_n$ as follows:
\begin{equation}\label{eq:def_s_n}
\hat s_n \equiv 
\begin{cases}
\text{the single element of }S_n & \text{if } |S_n|=1; \\
\frac{\varepsilon}{2}\mathrm{sgn}(w_n) & \text{otherwise}.
\end{cases}
\end{equation}
Note that this definition ensures that $\hat s_n \in [-\varepsilon/2, \varepsilon/2]$.



\subsection{Application to sign-flipping test}\label{sec:sign-flipping}

In this section, we apply Theorem~\ref{thm:unified_unnormalized_moment_conditions} to derive and justify the validity of the Lugannani-Rice SPA for the sign-flipping test. Suppose
\begin{equation}
X_{in}=\mu_n+\varepsilon_{in}, \quad \varepsilon_{in} \indsim F_{in}, 
\label{eq:location-model-symmetric-errors}
\end{equation}
where the error distributions $F_{in}$ are symmetric, but potentially distinct and unknown. We are interested in testing 
\begin{equation}
H_{0n}:\mu_n=0 \quad \text{versus} \quad H_{1n}:\mu_n>0
\end{equation}
based on $T_n \equiv \frac{1}{n}\sum_{i=1}^n X_{in}$. Note that the SPA cannot directly be applied to approximate tail probabilities of $T_n$ because the error distributions $F_{in}$ are unknown. Instead, we can approximate the tail probability of $T_n$ by conditioning on the observed data and resampling the signs of the data. In particular, define the resamples
\begin{equation}
\widetilde X_{in} \equiv \pi_{in} X_{in}, \quad \pi_{in} \iidsim \text{Rad}(0.5),
\end{equation}
where $\text{Rad}(0.5)$ denotes the Rademacher distribution placing equal probability mass on $\pm 1$. Due to the assumed symmetry of the distributions $F_{in}$, flipping the signs of $X_{in}$ preserves their distributions under the null hypothesis, guaranteeing finite-sample validity of the resampling-based $p$-value from equation~\eqref{eq:resampling-p-value} \citep{Hemerik2018, Hemerik2019a}. To circumvent the computationally costly resampling inherent in the sign-flipping test, we can obtain an accurate approximation to the $p$-value by applying the SPA to the tail probabilities of the resampling distribution
\begin{equation}
\widetilde T_n \equiv \frac1n \sum_{i=1}^n \widetilde X_{in} \equiv \frac1n \sum_{i=1}^n \pi_{in} X_{in}.
\end{equation}
Such approximations have been proposed before \citep{Daniels1955, Robinson1982, Hinkley1988}, but have not been rigorously justified (see also Section~\ref{sec:connection-spa-literatures}). We will now apply Theorem~\ref{thm:unified_unnormalized_moment_conditions} to derive and justify the Lugananni-Rice SPA for the sign-flipping test. We derive the saddlepoint approximation $\widehat{\P}_\text{LR}$. Defining $\mathcal F_n \equiv \sigma(X_{1n}, \dots, X_{nn})$, we first calculate the conditional cumulant-generating functions
\begin{align*}
	K_{in}(s)\equiv \log\E\left[\exp(s\widetilde{X}_{in})|\mathcal{F}_n\right]=\log\left(\frac{\exp(sX_{in}) + \exp(-sX_{in})}{2}\right) = \log\cosh(sX_{in})
\end{align*}
and their first two derivatives
\begin{align}\label{eq:tilde_K_in_prime}
	K'_{in}(s)=X_{in}-\frac{2X_{in}}{1+\exp(2sX_{in})} \quad \text{and} \quad K''_{in}(s)=\frac{4X_{in}^2\exp(2sX_{in})}{(1+\exp(2sX_{in}))^2}.
\end{align}
Therefore, the saddlepoint equation \eqref{eq:saddlepoint-equation} reduces to
\begin{align}\label{eq:saddlepoint_equation_example_simplified}
\frac{1}{n}\sum_{i = 1}^n K'_{in}(s) = \frac{1}{n}\sum_{i = 1}^n X_i \quad \Longleftrightarrow \quad \sum_{i=1}^n \frac{X_{in}}{1+\exp(2sX_{in})}=0.
\end{align}
Given a solution $\hat s_n$ to the saddlepoint equation (whose existence and uniqueness is guaranteed by Theorem~\ref{thm:example} below), we can define the quantities $\lambda_n$ and $r_n$ from equation~\eqref{eq:lam_n_r_n_def}:
\begin{equation}
\lambda_n \equiv \hat s_n \sqrt{n K''_{n}(\hat s_n)} = \hat s_n\sqrt{\sum_{i=1}^n\frac{4X_{in}^2\exp(2\hat s_nX_{in})}{(1+\exp(2\hat s_nX_{in}))^2}}
\label{eq:lambda_n_sign_flipping}
\end{equation}
and
\begin{equation}
\begin{split}
r_n \equiv \sgn(\hat s_n) \sqrt{2n( \hat s_n w_n - K_n(\hat s_n))} = \sgn(\hat s_n) \sqrt{2\sum_{i=1}^n (\hat s_n X_{in} - \log \cosh(\hat s_n X_{in}))},
\label{eq:r_n_sign_flipping}
\end{split}
\end{equation}
where we set $r_n \equiv \sgn(\hat s_n)$ when the quantity under the square root is negative. With these definitions, the SPA for the tail probability of interest is
\begin{equation}
\widehat{\P}_{\text{LR}} \left[\left.\frac{1}{n}\sum_{i=1}^n \widetilde{X}_{in} \geq \frac{1}{n}\sum_{i=1}^n X_{in}\ \right|\ \mathcal{F}_n\right] \equiv 1-\Phi(r_n)+\phi(r_n)\left\{\frac{1}{\lambda_n}-\frac{1}{r_n}\right\}.
\label{eq:sign-flipping-spa}
\end{equation} 
The following theorem gives sufficient conditions for this saddlepoint approximation to have vanishing relative error.

\begin{theorem}\label{thm:example}
Suppose $X_{in}$ are drawn from the probability model~\eqref{eq:location-model-symmetric-errors}, such that
	\begin{align}
		\liminf_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^n \E[\varepsilon_{in}^2]>0;&\label{eq:lower_bound_second_moment} \\
		\text{there exists $\delta>0$ such that }\limsup_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^n\E[|\varepsilon_{in}|^{4+\delta}]<\infty;&\label{eq:upper_bound_four_delta_moment} \\
		\mu_n=o(1).&\label{eq:mu_n_convergence}
	\end{align}
	Then, the saddlepoint equation \eqref{eq:saddlepoint_equation_example_simplified} has a unique solution $\hat s_n\in [-1,1]$ with probability approaching 1 as $n\rightarrow \infty$. Furthermore, the tail probability approximation~\eqref{eq:sign-flipping-spa} obtained from equations~\eqref{eq:lambda_n_sign_flipping} and~\eqref{eq:r_n_sign_flipping} has vanishing relative error:
	\small
	\begin{align}\label{eq:conclusion_example}
	\P\left[\frac{1}{n}\sum_{i=1}^n \widetilde{X}_{in}\geq \frac{1}{n}\sum_{i=1}^n X_{in}\mid \mathcal{F}_n\right]=\widehat{\P}_{\textnormal{LR}}\left[\left.\frac{1}{n}\sum_{i=1}^n \widetilde{X}_{in} \geq \frac{1}{n}\sum_{i=1}^n X_{in}\ \right|\ \mathcal{F}_n\right](1+o_{\P}(1)).
	\end{align}
	\normalsize
\end{theorem}


\section{Additional details of Section~\ref{sec:background}}\label{sec:additional-details-background}

\subsection{Doubly robust properties of $\dCRT$ and $\GCM$ test}\label{sec:double-robustness}


$\dCRT$ is a resampling-based procedure that can be computationally challenging when the sample size is moderate to large. An alternative test procedure is instead of using the resampling to construct $p$-value, one can use the normal approximation to the test statistic $T_n^{\dCRT}(\srx, \sry, \srz)$ \eqref{eq:dCRThat} under null and construct a $p$-value based on the cutoff of the standard normal distribution after properly normalizing the standard deviation estimate. This is the so-called \textit{generalized covariance measure} (GCM) test \citep{Shah2018}. Building on the same test statistic (except for extra normalization), $\GCM$ also allows flexible modeling choices on estimators $\widehat{\mu}_{n,x}(\cdot)$ and $\widehat{\mu}_{n,y}(\cdot)$. In fact, it has been proved in \citet{Shah2018} that $\GCM$ enjoys the so-called double robustness property: as long as both estimators $\widehat\mu_{n,x}(\cdot)$ and $\widehat{\mu}_{n,y}(\cdot)$ are consistent and converge to the true conditional expectations at rate faster than $n^{-1/4}$, the validity of the test can be guaranteed \citep[Theorem 6]{Shah2018}. 

Proved in \citet{Niu2022a}, the $\dCRT$ is asymptotically equivalent to $\GCM$ under mild conditions. Thus $\dCRT$ inherits the doubly robust statistical property from $\GCM$. One important difference is $\dCRT$ relies on resampling to construct the $p$-value, while $\GCM$ relies on the asymptotic normal approximation. This difference can lead to advantage for $\GCM$ for analyzing data with large scale because of fast computation. 

\subsection{Illustrative results with oracle knowledge of $\mu_{n,x}$ and $\mu_{n,y}$}\label{sec:finite_sample}

To develop the intuition, we work with a simple but illustrative setup where we assume that the conditional expectations $\mu_{n,x}(\cdot)$ and $\mu_{n,y}(\cdot)$ are known and we will set $\widehat{\mu}_{n,x}(\cdot) = \mu_{n,x}(\cdot)$ and $\widehat{\mu}_{n,y}(\cdot) = \mu_{n,y}(\cdot)$ in the test statistics. We will focus on a Bernoulli model for $\prx|\prz$:
\begin{align}\label{eq:illustrative_bernoulli}
  \prx|\prz\sim \mathrm{Ber}(\mu_{n,x}(\prz)).
\end{align}
We define the \textit{oracle $\GCM$} ($\oGCM$) test by considering the test statistic 
\begin{align}\label{eq:oGCM}
  \phi_{n,\alpha}^{\oGCM}\equiv \indicator(T_n^{\oGCM}(\srx,\sry,\srz)>z_{1-\alpha})\quad\text{where}\quad T_{n}^{\oGCM}(\srx,\sry,\srz)\equiv\frac{1}{nS_n}\sum_{i=1}^{n}R_{in}^o,
\end{align}
$R_{in}^o\equiv(\srx_{in}-\mu_{n,x}(\srz_{in}))(\sry_{in}-\mu_{n,y}(\srz_{in}))$
and $S_n^2=\E[(R_{in}^o)^2]$. For $\dCRT$, we consider the modified $\dCRT$ with theoretical quantile, which we call oracle $\dCRT$ ($\odCRT$):
\begin{align*}
  \phi^{\odCRT}_{n,\alpha}\equiv \indicator\left(T_n^{\dCRT}\geq \Q_{1-\alpha}(\tilde T_{n}^{\dCRT}|\srx,\sry,\srz)\right)
\end{align*}
where $T_n^{\dCRT},\tilde T_{n}^{\dCRT}$ are defined in \eqref{eq:dCRThat} and \eqref{eq:resampled-dcrt-def} respectively, but with oracle $\mu_{n,x}(\cdot),\mu_{n,y}(\cdot)$ and resampling distribution $\widetilde{X}^{(m)}\sim \prod_{i=1}^n \mathrm{Ber}(\mu_{n,x}(\srz_{in}))$. The intuition for the Type-I error deviating from the specified significance lelve for $\GCM$ (and $\oGCM$) is the CLT, when excessive sparsity exists, can happen in an arbitrarily slow rate depending how sparse the data is. To formalize such intuition, we consider the following assumptions.
\begin{assumption}[Sparsity level in $\prx$]\label{assu:sparsity}
  Suppose $v_n$ is a sequence of positive constants and $cv_n\leq\inf_{z}|\mu_{n,x}(z)|\leq \sup_{z}|\mu_{n,x}(z)|\leq Cv_n$ for some universal constants $C>0$ and $c>0$. 
\end{assumption}
\begin{assumption}[Conditional moments of $\pry$]\label{assu:finite-y-on-z-variance}
	Suppose the following conditions hold: $\sup_z\E[\pry^4|\prz=z]<\infty$, $\inf_z\E[(\pry - \E[\pry|\prz])^3|\prz=z]>0$ and $\inf_z\mathrm{Var}[\pry|\prz=z]>0$.
\end{assumption}
\begin{assumption}[Cram\'er's condition]\label{assu:abs-cont}
	Suppose $S_{in}\equiv R_{in}^o/\sqrt{\E[(S_n)^2]}$ satisfies the Cram\'er's condition: $\limsup_{n\rightarrow\infty}\sup_{t\in\mathbb{R}}|\E[\exp(itS_{in})]|<1$.
  \end{assumption}
Parameter $v_n$ in Assumption \ref{assu:sparsity} characterizes the sparsity level of data, which will play an important role in Theorem \ref{thm:illustrative} to unveil the failure of $\oGCM$ to control Type-I error under small sample size.  Assumption \ref{assu:finite-y-on-z-variance} states the bounded moment condition for $\pry$ given $\prz$ as well as the non-degeneracy of the conditional variance and conditional third central moment. This is mainly required to prove the rate of convergece on Type-I error for $\oGCM$ test. Such assumption can be staisfied by examples including Poisson or negative binomial case with uniformly lower and upper bounded conditional mean (and fixed dispersion parameter for negative binomial case) in $\pry|\prz$. This corresponds to the setup in Figure~\ref{fig:dCRT_GCM_binomial_poisson}. Assumption \ref{assu:abs-cont} is used to guarantee the validity of \textit{Edgeworth expansion} on $S_{in}$. The assumption may seem to be contradictory with model setup \eqref{eq:illustrative_bernoulli} at the first glance because of potential sparsity in $X$. However, this is not the case. The key reason hinges on the convolution nature of random variable $S_{in}$ and as long as $\mu_{n,x}(\srz_{in})\cdot \mu_{n,y}(\srz_{in})$ are continuous random variables, the convolution of the product variable with discrete random variables can still satisfy the Cram\'er's condition. Now we state our illustrative results.

\begin{theorem}\label{thm:illustrative}
  Consider $\prx\indep\pry|\prz$. Suppose Assumption \ref{assu:sparsity}-\ref{assu:abs-cont} hold. Then we have
  \begin{enumerate}
    \item \textbf{Finite-sample validity of $\odCRT$:} $\E[\phi_{n,\alpha}^{\odCRT}]=\alpha$;
    \item \textbf{Convergence of Type-I error of $\oGCM$:} If $1/v_n=o(n)$, then there exists a sequence $r_n>0$ such that $r_n\sim 1/(nv_n)^{1/2}$ and
    \begin{align}\label{eq:illustrative-convergence}
      \left|\E[\phi_{n,\alpha}^{\oGCM}]-\alpha-r_n\right|=o(r_n).
    \end{align}
  \end{enumerate}
\end{theorem}
The argument for finite-sample validity is by the exchangeability of the resampled data and the original data under null hypothesis $H_0:\prx\indep\pry|\prz$. Now we discuss the implication of the results on $\oGCM$.
\begin{remark}[Implication for testing with $\oGCM$]
	Theorem \ref{thm:illustrative} unveils that sparsity in data can slow down the Type-I error converging to the specified significance level $\alpha$, which is a result of slow convergence rate of CDF of $\oGCM$ test statistic converging to the CDF of standard normal distribution. When $v_n$ is of order $n^{-s}$ for $s>0$, the convergence rate of Type-I error of $\oGCM$ is $n^{(1-s)/2}$. The closer $s$ is to $1$, the slower the convergence rate is. 
\end{remark}

Theorem \ref{thm:illustrative} considers the model-X assumption for $\odCRT$ and oracle knowledge of $\mu_{n,x},\mu_{n,y}$. Thus it only serves as a illustration and the results are not directly applicable to the general case where $\mu_{n,x}(\cdot)$ and $\mu_{n,y}(\cdot)$ are unknown. However, the theorem provides a high-level insight on the finite-sample performance of $\dCRT$ and $\GCM$ tests. 


\subsection{Proof of Theorem \ref{thm:illustrative}}


To prove Theorem~\ref{thm:illustrative}, we first need an auxiliary result. 
\begin{lemma}[Theorem 5.18 in \citep{Petrov1995}; Theorem 4.1 in \citep{kuchibhotla2023central}]\label{lem:asymptotic-expansion}
    Consider a sequence of independently and identically distributed random variables $W_{in}\in\mathbb{R}$. Suppose  $\E[W_{in}]=0,\E[W_{in}^2]=1$ and $\E[W_{in}^4]<\infty$ for any $n\in\mathbb{N}$. Then there exists a universal constant $C>0$ such that for all $x\in\mathbb{R}$,
	\begin{align}
		&\nonumber
		\left|\P\left[\frac{1}{\sqrt{n}}\sum_{i=1}^n W_{in}\leq x\right]-\P[Z\leq x]-\frac{(1-x^2)\exp(-x^2/2)\E[W_{in}^3]}{6\sqrt{2\pi n}}\right|\\
		&\label{eq:asymptotic-expansion-UB}
		\qquad\leq C\frac{\E[W_{in}^4]}{n}+C\left(\sup_{|t|\geq 1/ (12\E[|W_{in}|^3])}|\E[\exp(itW_{in})]|+\frac{1}{2n}\right)^n\frac{n^6}{1+|x|^4}.
	\end{align}
\end{lemma}

\begin{proof}[Proof of Theorem \ref{thm:illustrative}]
  This argument for the validity of $\odCRT$ is based on the exchangeability of the resampled data and the original data under null hypothesis $H_0:\prx\indep\pry|\prz$. Now we prove the convergence rate of $\oGCM$. In order to prove the result, we will use Lemma \ref{lem:asymptotic-expansion} to state an asymptotic expansion of CDF of $\oGCM$ test statistic \eqref{eq:oGCM}. We apply $W_{in}=S_{in}$ and $x=z_{1-\alpha}$ in Lemma \ref{lem:asymptotic-expansion} so that we get a bound as in \eqref{eq:illustrative-convergence} with explicit $r_n$ defined as 
  \begin{align*}
	r_n=\frac{(1-z_{1-\alpha}^2)\exp(-z_{1-\alpha}^2/2)\E[S_{in}^3]}{6\sqrt{2\pi n}}.
  \end{align*}
  We just need to show that \eqref{eq:asymptotic-expansion-UB} with $W_{in}=S_{in}$ and $x=z_{1-\alpha}$ is of smaller order of $r_n$. It is sufficient to show the following results:
  \begin{align*}
	\frac{\E[S_{in}^4]}{nr_n}=o(1)\quad\text{and}\quad \frac{1}{r_n}\left(\sup_{|t|\geq 1/ (12\E[|S_{in}|^3])}|\E[\exp(itS_{in})]|+\frac{1}{2n}\right)^n\frac{n^6}{1+|z_{1-\alpha}|^4}=o(1).
  \end{align*}
  To prove these statements, we first show the order of convergence of $r_n$. Then we can obtain by conditional independence $\prx\indep\pry|\prz$ and Assumption \ref{assu:finite-y-on-z-variance} that
  \begin{align*}
	r_n
	&
	\sim \frac{1}{n^{1/2}}\frac{\E[\mu_{n,x}(\srz_{in})(1-\mu_{n,x}(\srz_{in}))(1-2\mu_{n,x}(\srz_{in}))\E[(\sry_{in}-\E[\sry_{in}|\srz_{in}])^3|\srz_{in}]]}{(\E[\mu_{n,x}(\srz_{in})(1-\mu_{n,x}(\srz_{in}))\mathrm{Var}[\sry_{in}|\srz_{in}]])^{3/2}}\\
	&
	\sim \frac{1}{n^{1/2}v_n^{1/2}}.
  \end{align*} 
  The last equivalence holds by H\"older's inequality. Together with Assumption \ref{assu:abs-cont}, we know
  \begin{align*}
	\limsup_{n\rightarrow\infty}\sup_{|t|\geq 1/ (12\E[|S_{in}|^3])}|\E[\exp(itS_{in})]|\leq \limsup_{n\rightarrow\infty}\sup_{t\in\mathbb{R}}  |\E[\exp(itS_{in})]|\equiv c<1.
  \end{align*} 
  Thus we have for any $\gamma>0$,
  \begin{align*}
	\left(\sup_{|t|\geq 1/ (12\E[|S_{in}|^3])}|\E[\exp(itS_{in})]|+\frac{1}{2n}\right)^n\lesssim (c+1/2n)^n=o(n^{\gamma})
  \end{align*}
  so that we prove 
  \begin{align*}
	\frac{1}{r_n}\left(\sup_{|t|\geq 1/ (12\E[|S_{in}|^3])}|\E[\exp(itS_{in})]|+\frac{1}{2n}\right)^n\frac{n^6}{1+|z_{1-\alpha}|^4}=o(1).
  \end{align*}
  Then it remains to prove
  \begin{align}\label{eq:upper-bounded-expansion}
	\frac{\E[S_{in}^4]}{nr_n}=o(1). 
  \end{align}
  \paragraph{Proof of convergence \eqref{eq:upper-bounded-expansion}:}
  It suffices to show $\E[S_{in}^4]=o(n^{1/2}/v_n^{1/2})$ by the proved results $r_n\sim 1/(nv_n)^{1/2}$. To see this, by Assumption \ref{assu:finite-y-on-z-variance}, we can bound 
  \begin{align*}
	\E[S_{in}^4]
	&
	\leq \frac{\E[\mu_{n,x}(\srz_{in})(1-\mu_{n,x}(\srz_{in}))(1-3\mu_{n,x}(\srz_{in})+3\mu_{n,x}^2(\srz_{in}))]}{(\E[\mu_{n,x}(\srz_{in})(1-\mu_{n,x}(\srz_{in}))])^{2}}\\
	&
	\qquad
	\times \frac{\sup_z\E[(\sry_{in}-\E[\sry_{in}|\srz_{in}])^4|\srz_{in}=z]}{\inf_{z}\mathrm{Var}^2[\sry_{in}|\srz_{in}=z]}\\
	&
	\sim \frac{1}{v_n}.
  \end{align*}
  Then by the assumption that $1/v_n=o(n)$, we know $\E[S_{in}^4]=o(n^{1/2}/v_n^{1/2})$.
\end{proof}



\section{Some useful lemmas and proofs}\label{sec:useful_lemma}


\begin{lemma}[Lemma 3 in \cite{Niu2022a}]\label{lem:unified_asymptotic_equivalence}
	Consider two hypothesis tests based on the same test statistic $T_n(\srx, \sry, \srz)$ but different critical values:
	\begin{equation*}
		\phi_n^1(\srx, \sry, \srz) \equiv \indicator(T_n(\srx, \sry, \srz) > C_n(\srx, \sry, \srz)); \quad \phi_n^2(\srx, \sry, \srz) \equiv \indicator(T_n(\srx, \sry, \srz) > z_{1-\alpha}). 
	\end{equation*}
	If the critical value of the first converges in probability to that of the second:
	\begin{equation*}
		C_n(\srx, \sry, \srz) \convp z_{1-\alpha}
	\end{equation*}
	and the test statistic does not accumulate near the limiting critical value:
	\begin{equation}
		\lim_{\delta \rightarrow 0}\limsup_{n \rightarrow \infty}\ \P_{\law_n}[|T_n(\srx, \sry, \srz)-z_{1-\alpha}| \leq \delta] = 0,
		\label{eq:non-accumulation-app}
	\end{equation}
	then the two tests are asymptotically equivalent:
	\begin{equation*}
		\lim_{n \rightarrow \infty}\P_{\law_n}[\phi_n^{1}(\srx, \sry, \srz) = \phi_n^2(\srx, \sry, \srz)] = 1.
	\end{equation*}
\end{lemma}


\paragraph{\textbf{Regularity condition:}}
there exists $\delta>0$ such that for a sequence of laws $\law_n$ and its estimate $\lawhat_n$, the following assumptions hold:
\begin{align}
    (\widehat{S}_n^{\dCRT})^2\equiv\frac{1}{n}\sum_{i=1}^n\V_{\lawhat_n}[\srx_{in}\mid \srz_{in}](\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in}))^2=\Omega_\P(1) \label{eq:variance_lower_bound};\\
    \frac{1}{n^{1+\delta/2}} \sum_{i=1}^n |\sry_{in}-\widehat \mu_{n,y}(\srz_{in})|^{2+\delta}\E_{\lawhat_n}[|\srxk_{in}-\widehat\mu_{n,x}(\srz_{in})|^{2+\delta}\mid \srx,\srz] =o_\P(1)\label{eq:Lyapunov_condition_2};\\
    \V_{\lawhat_n}[\srx_{in}|\srz_{in}],(\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in}))^2,(\sry_{in}-\mu_{n,y}(\srz_{in}))^2<\infty\ \text{almost surely}\label{eq:non_degeneracy}.
\end{align}

\begin{lemma}[Theorem 9 in \cite{Niu2022a}]\label{lem:quantile_convergence_ptwise}
	Let $\law_n$ be a sequence of laws and $\lawhat_n$ be a sequence of estimates. Suppose  there exists a sequence of laws $\law_n$ satisfying all the assumptions in \textbf{Regularity condition}. Then, the quantile of 
  \begin{align}\label{eq:normalized_dcrt_stats}
    T_n^{\ndCRThat}(\srxk,\srx,\sry,\srz)\equiv \frac{T_n^{\dCRT}(\srxk,\srx,\sry,\srz)}{\widehat S_n^{\dCRT}}
  \end{align}
  converges to the quantile of the standard normal distribution pointwisely in probability, i.e., for any $p\in(0,1)$,
	\begin{align*}
		\Q_{p}\left[n^{1/2}T_n^{\ndCRThat}(\srxk,\srx,\sry,\srz)|\srx,\sry,\srz\right]\convp z_{p}.
	\end{align*}
\end{lemma}

\begin{lemma}[Corollary 6 in \cite{Niu2022a}]\label{lem:wlln}
  Let $X_{in}$ be a triangular array of random variables, such that $X_{in}$ are independent for each $n$. If for some $\delta>0$ we have 
  \begin{align}\label{eq:moment_condition_wlln}
    \frac{1}{n^{1+\delta}}\sum_{i=1}^n \E[|X_{in}|^{1+\delta}]\rightarrow 0,
  \end{align}
  then 
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n (X_{in}-\E[X_{in}])\overset{\P}{\rightarrow}0.
  \end{align*}
  The condition \eqref{eq:moment_condition_wlln} is satisfied when 
  \begin{align*}
    \sup_{1\leq i\leq n}\E[|X_{in}|^{1+\delta}]=o(n^\delta).
  \end{align*}
\end{lemma}

\begin{lemma}[Dominance of higher moment]\label{lem:moment_dominance}
  For any $1<p<q<\infty$, the following inequality is true almost surely:
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n \E[|X_{in}|^{p}|\mathcal{F}_n]\leq \left(\frac{1}{n}\sum_{i=1}^n \E[|X_{in}|^{q}|\mathcal{F}_n]\right)^{p/q}.
  \end{align*}
\end{lemma}


\begin{lemma}[Conditional H\"older inequality, \cite{Swanson2019}, Theorem 6.60]\label{lem:cond_holder}
	Let $W_1$ and $W_2$ be random variables and let $\mathcal F$ be a $\sigma$-algebra. If for some $q_1, q_2 \in (1,\infty)$ with $\frac{1}{q_1} + \frac{1}{q_2} = 1$ we have $\E[|W_1|^{q_1}], \E[|W_2|^{q_2}] < \infty$, then
	\begin{align*}
		\E[|W_1 W_2| \mid \mathcal F] \leq (\E[|W_1|^{q_1} \mid \mathcal F])^{1/q_1}(\E[|W_2|^{q_2} \mid \mathcal F])^{1/q_2} \quad \text{almost surely}.
	\end{align*}
\end{lemma}

\begin{lemma}[Conditional Jensen inequality, \cite{Davidson2003}, Theorem 10.18] \label{lem:conditional-jensen}
	Let $W$ be a random variable and let $\phi$ be a convex function, such that $W$ and $\phi(W)$ are integrable. For any $\sigma$-algebra $\mathcal F$, we have the inequality
	\begin{equation*}
		\phi(\E[W \mid \mathcal F]) \leq  \E[\phi(W) \mid \mathcal F] \quad \text{almost surely}.
	\end{equation*}
\end{lemma}

\begin{lemma}[Conditional Markov's inequality, \cite{Davidson2003}, Theorem 10.17]\label{lem:conditional_Markov}
  Let $W$ be a random variable and let $\mathcal{F}$ be a $\sigma$-algebra. If for some $q>0$, we have $\E[|W|^q]<\infty$, then for any $\varepsilon$ we have
	\begin{equation*}
		\P[|W|\geq \varepsilon|\mathcal{F}]\leq \frac{\E[|W]^q}{\varepsilon^q}\quad\text{almost surely}.
	\end{equation*}
\end{lemma}

\begin{lemma}[Conditional central limit theorem] \label{lem:lyapunov-clt} 
	Let $W_{in}$ be a triangular array of random variables, such that for each $n,W_{in}$ are independent conditionally on $\mathcal F_n$. Define
	\begin{equation}
		S_n^2 \equiv \sum_{i = 1}^n \V[W_{in} \mid \mathcal F_n],
	\end{equation} 
	and assume $\mathrm{Var}[W_{in}|\mathcal{F}_n]<\infty $ almost surely for all $i=1,\ldots,n$ and for all $n\in\mathbb{N}$. If for some $\delta > 0$ we have
	\begin{equation}
		\frac{1}{S_n^{2+\delta}} \sum_{i = 1}^n \E[|W_{in}-\E[W_{in}|\mathcal{F}_n]|^{2+\delta} \mid \mathcal{F}_n] \convp 0,
		\label{eq:conditional-lyapunov}
	\end{equation}
	then 
	\begin{equation}
		\frac{1}{S_n} \sum_{i = 1}^n (W_{in} - \E[W_{in} \mid \mathcal{F}_n]) \mid \mathcal F_n \convdp N(0,1).
	\end{equation}
\end{lemma}

\begin{lemma}\label{lem:dCRT_convergence}
  Consider the fixed dimension setup: $(\srz_{in},\srx_{in},\sry_{in})=(\srz_{i},\srx_{i},\sry_{i})$. Define $\sigma_{\dCRT}^2=\E[(\srx_i-\E[\srx_i|\srz_i])^2(\sry_i-\E[\sry_i|\srz_i])^2]$. Then if $\E[(\sry_i-\E[\sry_i|\srz_i])^2]<\infty$ and $\P[\srx_i,\widehat{\mu}_x(\srz_i)\in [-S,S]]=1$ for some $S>0$, then as long as the following conditions hold:
  \begin{align*}
    \prx\indep\pry|\prz;\\
    \sigma_{\dCRT}^2\in (0,\infty);\\
    \frac{1}{n}\sum_{i=1}^n (\mu_{x}(\srz_i)-\widehat\mu_{x}(\srz_i))^2=o_{\P}(1);\\
    \frac{1}{n}\sum_{i=1}^n (\mu_{x}(\srz_i)-\widehat\mu_{x}(\srz_i))^2=o_{\P}(1);\\
    \left(\frac{1}{n}\sum_{i=1}^n (\mu_{x}(\srz_i)-\widehat\mu_{x}(\srz_i))^2\right)\left(
    \frac{1}{n}\sum_{i=1}^n (\mu_{x}(\srz_i)-\widehat\mu_{x}(\srz_i))^2\right)=o_{\P}(1/n),
  \end{align*}
  then we have $\sqrt{n}T_n^{\dCRT}\convp N(0,\sigma_{\dCRT}^2)$.
\end{lemma}

\subsection{Proof of Lemma \ref{lem:moment_dominance}}

\begin{proof}[Proof of Lemma \ref{lem:moment_dominance}]
  By Lemma \ref{lem:cond_holder}, we have
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n \E[|X_{in}|^{p}|\mathcal{F}_n]
    &
    \leq 
    \frac{1}{n}\left(\sum_{i=1}^n \left(\E[|X_{in}|^{p}|\mathcal{F}_n]\right)^{q/p}\right)^{p/q}n^{1-p/q}\\
    &
    =\left(\frac{1}{n}\sum_{i=1}^n \left(\E[|X_{in}|^{p}|\mathcal{F}_n]\right)^{q/p}\right)^{p/q}.
  \end{align*}
  We use Jensen's inequality, Lemma \ref{lem:conditional-jensen}, to obtain
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n \E[|X_{in}|^{p}|\mathcal{F}_n]\leq \left(\frac{1}{n}\sum_{i=1}^n \E[|X_{in}|^{q}|\mathcal{F}_n]\right)^{p/q}.
  \end{align*}
\end{proof}

\subsection{Proof of Lemma \ref{lem:dCRT_convergence}}

We consider the following decomposition:
\begin{align*}
  T_n^{\dCRT}
  &
  =\frac{1}{n}\sum_{i=1}^n (\srx_i-\mu_x(\srz_i))(\sry_i-\mu_y(\srz_i)) + \frac{1}{n}\sum_{i=1}^n (\mu_{x}(\srz_i)-\widehat{\mu}_{x}(\srz_i))(\sry_i-\mu_{y}(\srz_i))\\
  &
  + \frac{1}{n}\sum_{i=1}^n (\srx_i-\mu_{x}(\srz_i))(\mu_{y}(\srz_i)-\widehat{\mu}_{y}(\srz_i))+\frac{1}{n}\sum_{i=1}^n (\mu_{x}(\srz_i)-\widehat{\mu}_{x}(\srz_i))(\mu_{y}(\srz_i)-\widehat{\mu}_{y}(\srz_i))\\
  &
  \equiv \frac{1}{n}\sum_{i=1}^n (\srx_i-\mu_x(\srz_i))(\sry_i-\mu_y(\srz_i))+\sum_{k=1}^3B_k.
\end{align*}
We will prove $\sqrt{n}B_k$ converges $0$ in probability for any $k=1,2,3$. For $B_1$ and $B_2$, we have 
\begin{align*}
  \P\left[\sqrt{n}B_1>\varepsilon|\srx,\srz\right]\leq \frac{1}{\varepsilon^2n}\sum_{i=1}^n (\mu_{x}(\srz_i)-\widehat{\mu}_{x}(\srz_i))^2\E[(\sry_i-\mu_{y}(\srz_i))^2|\srz]=o_{\P}(1)
\end{align*}
and 
\begin{align*}
  \P\left[\sqrt{n}B_2>\varepsilon|\srx,\srz\right]\leq \frac{1}{\varepsilon^2n}\sum_{i=1}^n (\mu_{y}(\srz_i)-\widehat{\mu}_{y}(\srz_i))^2\E[(\sry_i-\mu_{x}(\srz_i))^2|\srz]=o_{\P}(1).
\end{align*}
As for $B_3$, we know by Cauchy-Schwarz inequality that
\begin{align*}
  \sqrt{n}|B_3|\leq \sqrt{n}\sqrt{\frac{1}{n}\sum_{i=1}^n (\mu_{x}(\srz_i)-\widehat{\mu}_{x}(\srz_i))^2}\sqrt{\frac{1}{n}\sum_{i=1}^n (\mu_{y}(\srz_i)-\widehat{\mu}_{y}(\srz_i))^2}=o_{\P}(1).
\end{align*}
Therefore, we only need to prove the weak convergence of $\sum_{i=1}^n (\srx_i-\mu_x(\srz_i))(\sry_i-\mu_y(\srz_i))/\sqrt{n}$. This is true by usual CLT.


\section{Asymptotic equivalence and proofs of results in section \ref{sec:general_results}}\label{sec:proof_general_results}

\subsection{Statement of an asymptotic equivalence result}\label{sec:asymptotic_equivalence}

We first state a result on the asymptotic equivalence of $\spacrt$ and $\dCRT$. This is a generalization of Corollary \ref{cor:asymptotic_validity_spacrt}.
\begin{theorem}\label{thm:asymptotic_equivalence}
	Suppose the assumptions of Theorem \ref{thm:validity_spacrt} hold. Fix $\alpha\in (0,1)$. If the normalized test statistic $n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)/\widehat{S}_n^{\dCRT}$, where $\widehat{S}_n^{\dCRT}$ is defined in equation \eqref{eq:variance_lower_bound} (in Appendix), does not accumulate around the $1-\alpha$ quantile of standard normal distribution $z_{1-\alpha}$, i.e.,
  \begin{align}\label{eq:nonaccumulant_condition}
    \lim_{\delta\rightarrow0}\limsup_{n\rightarrow\infty}\P_{\law_n}\left[\left|\frac{n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)}{\widehat{S}_n^{\dCRT}}-z_{1-\alpha}\right|\leq \delta\right]=0,
  \end{align}
  then $\lim_{n\rightarrow\infty}\P_{\law_n}\left[\phi_{n,\alpha}^{\spacrt}=\phi_{n,\alpha}^{\dCRT}\right]=1$.
\end{theorem}
\noindent We will restate a stronger version of Theorem \ref{thm:asymptotic_equivalence}.
\begin{theorem}[Stronger version of Theorem \ref{thm:asymptotic_equivalence}]\label{thm:asymptotic_equivalence_stronger}
  Suppose the assumptions of Theorem \ref{thm:asymptotic_equivalence} hold. Then define the asymptotic test:
  \begin{align}\label{eq:asy_test}
    \phi_{n,\alpha}^{\asy}\equiv \indicator\left(\frac{n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)}{\widehat{S}_n^{\dCRT}}>z_{1-\alpha}\right).
  \end{align}
  Then we have
  \begin{align*}
    \lim_{n\rightarrow\infty}\P_{\law_n}\left[\phi_{n,\alpha}^{\spacrt}=\phi_{n,\alpha}^{\dCRT}=\phi_{n,\alpha}^{\asy}\right]=1.
  \end{align*}
  Consequently, if $n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)/\widehat{S}_n^{\dCRT}\convd N(0,1)$, we have $ \lim_{n\rightarrow\infty}\E[\phi_{n,\alpha}^{\spacrt}]=\alpha$.
\end{theorem}

\subsection{Proof of Theorem \ref{thm:validity_spacrt}}

We have conditional CGF 
\begin{equation}
K_{in}(s|\mathcal{F}_n) = A(\widehat \theta_{n,x}(\srz_{in})+a_{in}s)-A(\widehat \theta_{n,x}(\srz_{in}))-a_{in}sA'(\widehat \theta_{n,x}(\srz_{in})).
\end{equation}
Then we can compute the CCGF under model \eqref{eq:NEF} as
\begin{equation}
K_n(s \mid \mathcal F_n) = \frac{1}{n}\sum_{i = 1}^n \left\{A(\widehat \theta_{n,x}(\srz_{in})+a_{in}s)-A(\widehat \theta_{n,x}(\srz_{in}))-a_{in}sA'(\widehat \theta_{n,x}(\srz_{in}))\right\}.
\end{equation}
The first two derivatives of this quantity are
\begin{align}
  K_n'(s \mid \mathcal F_n) &= \frac{1}{n}\sum_{i = 1}^n a_{in}\left(A'(\widehat \theta_{n,x}(\srz_{in})+a_{in}s)-A'(\widehat \theta_{n,x}(\srz_{in}))\right), \label{eq:conditional-cgf-derivative} \\
  K_n''(s \mid \mathcal F_n) &= \frac{1}{n}\sum_{i = 1}^n a_{in}^2A''(\widehat \theta_{n,x}(\srz_{in})+a_{in}s). \label{eq:conditional-cgf-second-derivative}
\end{align}
We will apply Theorem~\ref{thm:unified_unnormalized_moment_conditions} and thus verify the conditions in the theorem. We first verify the variance condition \eqref{eq:lower_bound_conditional_variance}.

\paragraph{\textbf{Verification of \eqref{eq:lower_bound_conditional_variance}}:}
Compute $K_{in}''(s\mid \mathcal{F}_n)= a_{in}^2A''(\widehat \theta_{n,x}(\srz_{in})+a_{in}s)$. Then it suffices to guarantee 
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n \E[W_{in}^2|\mathcal{F}_n]=\frac{1}{n}\sum_{i=1}^n K_{in}''(0\mid \mathcal{F}_n)=\frac{1}{n}\sum_{i=1}^n a_{in}^2A''(\widehat \theta_{n,x}(\srz_{in}))=\Omega_\P(1).
\end{align*}

\noindent Next we verify assumption \ref{assu:cse} and condition \eqref{eq:ccs_assumption} respectively.

\paragraph{Verification of Assumption \ref{assu:cse} with condition \eqref{eq:cse_assumption} and \eqref{eq:upper_bound_theta_a}:} 

We denote the conditional upper tail probability and lower probability respectively as 
\begin{align*}
  L_{X,\mathcal{F}_n}(a)\equiv \P\left[X\leq a|\mathcal{F}_n\right],\ U_{X,\mathcal{F}_n}(a)\equiv \P\left[X\geq a|\mathcal{F}_n\right].
\end{align*}
By condition \eqref{eq:cse_assumption}, we can compute 
\begin{align*}
  &
  \P[W_{in}\geq t|\mathcal{F}_n]\\
  &
  =\P[a_{in}(\srxk_{in}-A'(\widehat \theta_{n,x}(\srz_{in})))\geq t|\mathcal{F}_n]\\
  &
  =\indicator(a_{in}>0)U_{\srxk_{in},\mathcal{F}_n}\left(\frac{t}{a_{in}}+A'(\widehat{\theta}_{n,x}(\srz_{in}))\right)+\indicator(a_{in}<0)L_{\srxk_{in},\mathcal{F}_n}\left(\frac{t}{a_{in}}+A'(\widehat{\theta}_{n,x}(\srz_{in}))\right)
\end{align*}
Then by the definition of natural exponential family, we can write 
\begin{align*}
  &
  \indicator(a_{in}>0)U_{\srxk_{in},\mathcal{F}_n}\left(\frac{t}{a_{in}}+A'(\widehat{\theta}_{n,x}(\srz_{in}))\right)\\
  &
  =\indicator(a_{in}>0)\int_{t/a_{in}+A'(\widehat{\theta}_{n,x}(\srz_{in}))}^{\infty}\exp(\widehat{\theta}_{n,x}(\srz_{in})x-A(\widehat{\theta}_{n,x}(\srz_{in})))h(x)dx\\
  &
  =\indicator(a_{in}>0)\int_{t/a_{in}+A'(\widehat{\theta}_{n,x}(\srz_{in}))}^{\infty}\exp(\widehat{\theta}_{n,x}(\srz_{in})x+a_{in}x -a_{in}x-A(\widehat{\theta}_{n,x}(\srz_{in})))h(x)dx\\
  &
  \leq \indicator(a_{in}>0)\int_{t/a_{in}+A'(\widehat{\theta}_{n,x}(\srz_{in}))}^{\infty}\exp((\widehat{\theta}_{n,x}(\srz_{in})+a_{in})x-A(\widehat{\theta}_{n,x}(\srz_{in})))h(x)\mathrm{d}x\\
  &
  \qquad\times \exp(-t-a_{in}A'(\widehat{\theta}_{n,x}(\srz_{in})))\\
  &
  \leq \indicator(a_{in}>0)\int_{t/a_{in}+A'(\widehat{\theta}_{n,x}(\srz_{in}))}^{\infty}\exp((\widehat{\theta}_{n,x}(\srz_{in})+a_{in})x-A(a_{in}+\widehat{\theta}_{n,x}(\srz_{in})))h(x)\mathrm{d}x\\
  &
  \qquad\times \exp(A(a_{in}+\widehat{\theta}_{n,x}(\srz_{in}))-A(\widehat{\theta}_{n,x}(\srz_{in})))\exp(-t-a_{in}A'(\widehat{\theta}_{n,x}(\srz_{in})))\\
  &
  \leq \indicator(a_{in}>0)\exp(A(\widehat{\theta}_{n,x}(\srz_{in})+a_{in})-A(\widehat{\theta}_{n,x}(\srz_{in}))-a_{in}A'(\widehat{\theta}_{n,x}(\srz_{in})))\exp(-t)\\
  &
  \leq \indicator(a_{in}>0)\exp(|A(\widehat{\theta}_{n,x}(\srz_{in})+a_{in})|+|A(\widehat{\theta}_{n,x}(\srz_{in}))|+|a_{in}||A'(\widehat{\theta}_{n,x}(\srz_{in}))|)\exp(-t)
\end{align*}
Similarly, we can derive the upper bound for the lower tail Probability: 
\begin{align*}
  &
  \indicator(a_{in}<0)L_{\srxk_{in},\mathcal{F}_n}\left(\frac{t}{a_{in}}+A'(\widehat{\theta}_{n,x}(\srz_{in}))\right)\\
  &
  =\indicator(a_{in}<0)\int_{-\infty}^{t/a_{in}+A'(\widehat{\theta}_{n,x}(\srz_{in}))}\exp(\widehat{\theta}_{n,x}(\srz_{in})x-A(\widehat{\theta}_{n,x}(\srz_{in})))h(x)dx\\
  &
  =\indicator(a_{in}<0)\int_{-\infty}^{t/a_{in}+A'(\widehat{\theta}_{n,x}(\srz_{in}))}\exp(\widehat{\theta}_{n,x}(\srz_{in})x+a_{in}x - a_{in}x-A(\widehat{\theta}_{n,x}(\srz_{in})))h(x)dx\\
  &
  \leq \indicator(a_{in}<0)\exp(A(\widehat{\theta}_{n,x}(\srz_{in})+a_{in})-A(\widehat{\theta}_{n,x}(\srz_{in}))-a_{in}A'(\widehat{\theta}_{n,x}(\srz_{in})))\exp(-t)\\
  &
  \leq \indicator(a_{in}<0)\exp(|A(\widehat{\theta}_{n,x}(\srz_{in})+a_{in})|+|A(\widehat{\theta}_{n,x}(\srz_{in}))|+|a_{in}||A'(\widehat{\theta}_{n,x}(\srz_{in}))|)\exp(-t).
\end{align*}
Then we have for any $t>0$,
\begin{align*}
  &
  \P[W_{in}\geq t|\mathcal{F}_n]\\
  &
  \leq \exp(|A(\widehat{\theta}_{n,x}(\srz_{in})+a_{in})|+|A(\widehat{\theta}_{n,x}(\srz_{in}))|+|a_{in}||A'(\widehat{\theta}_{n,x}(\srz_{in}))|)\exp(-t)\\
  &
  \leq \exp(\sup_{i}|A(\widehat{\theta}_{n,x}(\srz_{in})+a_{in})|+\sup_{i}|A(\widehat{\theta}_{n,x}(\srz_{in}))|+\sup_i|a_{in}||A'(\widehat{\theta}_{n,x}(\srz_{in}))|)\exp(-t).
\end{align*}
Choosing
\begin{align*}
  \theta_n=\exp\left(\sup_{i}|A(\widehat{\theta}_{n,x}(\srz_{in})+a_{in})|+\sup_{i}|A(\widehat{\theta}_{n,x}(\srz_{in}))|+\sup_i|a_{in}||A'(\widehat{\theta}_{n,x}(\srz_{in}))|\right)
\end{align*}
and $\beta = 1$, we need to verify 
\begin{align*}
  \theta_n=O_{\P}(1)\text{ and }\theta_n<\infty,\text{ almost surely}.
\end{align*}
Since by condition \eqref{eq:upper_bound_theta_a}, we know $\sup_i|a_{in}|\leq \sup_i |\sry_{in}|+\sup_{i}|\widehat{\mu}_{n,y}(\srz_{in})|<\infty $ almost surely and $|\widehat{\theta}_{n,x}(\srz_{in})|<\infty$ almost surely, we know $\theta_n<\infty$ almost surely. Now we prove $\theta_n=O_{\P}(1)$. By condition \eqref{eq:cse_assumption}, we know for any fixed $\delta>0$, there exists $M(\delta)>0$ such that 
\begin{align*}
  \P\left[\mathcal{S}\right]\geq 1-\delta,\text{ where }\mathcal{S}\equiv\left\{\sup_{i}|\widehat{\theta}_{n,x}(\srz_{in})|,\ \sup_{i}|a_{in}|\in [0,M(\delta)]\right\}.
\end{align*}
Then on the event $\mathcal{S}$, we know 
\begin{align*}
  \sup_{i}|A(\widehat{\theta}_{n,x}(\srz_{in})+a_{in})|\leq \sup_{x\in [-2M(\delta),2M(\delta)]}|A(x)|
\end{align*}
and 
\begin{align*}
  \sup_{i}|A'(\widehat{\theta}_{n,x}(\srz_{in}))|\leq \sup_{x\in [-M(\delta),M(\delta)]}|A'(x)|.
\end{align*}
Similarly, on the event $\mathcal{S}$, we have 
\begin{align*}
  \sup_{i}|a_{in}|\leq M(\delta),\ \sup_{i}|A(\widehat{\theta}_{n,x}(\srz_{in}))|\leq \sup_{x\in [-2M(\delta),2M(\delta)]}|A(x)|.
\end{align*}
Therefore we have 
\begin{align*}
  \P\left[\theta_n\leq \exp\left(2\sup_{x\in [-2M(\delta),2M(\delta)]}|A(x)|+M(\delta)\sup_{x\in [-M(\delta),M(\delta)]}|A'(x)|\right)\right]\geq\P[\mathcal{S}]\geq 1-\delta.
\end{align*}
Therefore we have $\theta_n=O_{\P}(1)$. Thus $\varepsilon=\beta/8=1/8$ according to Lemma \ref{lem:finite_cgf}.

\paragraph{Verification of Assumption \ref{assu:ccs} with condition \eqref{eq:upper_bound_theta_a} and \eqref{eq:ccs_assumption}:}

By condition \eqref{eq:ccs_assumption}, we know 
\begin{align*}
  \indicator(\widetilde{X}_{in}\in [-S,S])=1,\text{ almost surely}.
\end{align*}
This implies that for any $F\in\mathcal{F}_n$, we have 
\begin{align*}
  \int_F \indicator\left(\widetilde{X}_{in}\in [-S,S]\right)d\mathbb{P}=\int_F 1 \mathrm{d}\mathbb P.
\end{align*}
Thus we know 
\begin{align*}
  \P[\widetilde{X}_{in}\in [-S,S]|\mathcal{F}_n]=\E[\indicator(\widetilde{X}_{in}\in [-S,S])|\mathcal{F}_n]=1,\text{ almost surely}.
\end{align*}
Thus we have
\begin{align*}
  \P\left[a_{in}(\srxk_{in}-\widehat{\mu}_{n,x}(\srz_{in}))\in [-2|a_{in}|S,2|a_{in}|S]|\mathcal{F}_n\right]=1,\text{ almost surely}.
\end{align*}
Then again by condition \eqref{eq:upper_bound_theta_a}, we know 
\begin{align*}
  |a_{in}|\leq |\sry_{in}|+|\widehat{\mu}_{n,x}(\srz_{in})|<\infty,\text{ almost surely}.
\end{align*}
Moreover, by condition \eqref{eq:ccs_assumption}, we know 
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n 16S^4a_{in}^4=\frac{16S^4}{n}\sum_{i=1}^n (\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in}))^4=O_{\P}(1).
\end{align*}
Choosing $\nu_{in}=2|a_{in}|S$, we complete the proof for CCS distribution. Thus $\varepsilon=1/8$ according to Lemma \ref{lem:finite_cgf}.


  \subsection{Proof of Theorem \ref{thm:asymptotic_equivalence} and Theorem \ref{thm:asymptotic_equivalence_stronger}}

  \begin{proof}[Proof of Theorem \ref{thm:asymptotic_equivalence} and Theorem \ref{thm:asymptotic_equivalence_stronger}]
	Define the auxiliary test 
	  \begin{align*}
		  \phi^{\aux}_{n,\alpha}\equiv \indicator\left(\frac{n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)}{\widehat{S}_n^{\dCRT}}>\Q_{1-\alpha(1+M_n)}\left[n^{1/2}T_n^{\ndCRThat}(\widetilde{X},X,Y,Z)|X,Y,Z\right]\right)
	  \end{align*}
	where $T_n^{\ndCRThat}(\srxk,\srx,\sry,\srz)$ is defined in \eqref{eq:normalized_dcrt_stats} and $M_n$ is the sequence such that
	\begin{align*}
	  \P\left(T_n^{\dCRT}(\srxk,\srx,\sry,\srz)\geq T_n^{\dCRT}(\srx,\sry,\srz)|\mathcal{F}_n\right)=p_{\spacrt}(1+M_n),\text{ almost surely}.
	\end{align*}
	  Notice the tests $\phi_{n,\alpha}^{\spacrt}$ and $\phi_{n,\alpha}^{\aux}$ are equivalent as long as $M_n \in (-1, 1/\alpha - 1)$. Indeed, we have 
	\begin{align*}
	  \P[\phi_{n,\alpha}^{\spacrt}\neq \phi_{n,\alpha}^{\aux}]\leq \P[\widehat S_n^{\dCRT}=0]+\P[M_n \notin (-1, 1/\alpha - 1)]\rightarrow0
	\end{align*}
	due to assumption \eqref{eq:variance_lower_bound} and that $M_n=o_{\P}(1)$. Thus we only need to verify $\phi_{n,\alpha}^{\aux}$ and $\phi_{n,\alpha}^{\dCRT}$ are asymptotically equivalent. Recall the asymptotic test \eqref{eq:asy_test}. We will prove $\phi_{n,\alpha}^{\aux},\phi_{n,\alpha}^{\asy}$ are asymptotically equivalent and $\phi_{n,\alpha}^{\dCRT},\phi_{n,\alpha}^{\asy}$ are asymptotically equivalent, respectively.
  
	\paragraph{Proof of the equivalence of $\phi_{n,\alpha}^{\aux},\phi_{n,\alpha}^{\asy}$:}
	
	We apply Lemma \ref{lem:unified_asymptotic_equivalence} with the test statistic $T_n(\srx,\sry,\srz)$ to be
	\begin{align*}
		T_n(\srx,\sry,\srz)\equiv\frac{n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)}{\widehat{S}_n^{\dCRT}}
	\end{align*}
	and cutoff $C_n(\srx,\sry,\srz)$ to be 
	\begin{align*}
	  C_n(\srx,\sry,\srz)\equiv\Q_{1-\alpha(1+M_n)}\left[n^{1/2}T_n^{\ndCRThat}(\srxk, \srx, \sry,\srz)|\srx,\sry,\srz\right].
	\end{align*}
	The following lemma characterizes the convergence of $C_n(X,Y,Z)$.
	\begin{lemma}\label{lem:quantile_equivalence}
	  Suppose the sequence of laws $\law_n$ and its estimate $\lawhat_n$ satisfy all the assumptions in \textbf{Regularity condition}. Then for any given $\alpha\in (0,1)$, we have for any sequence $M_n\in\mathcal{F}_n$ satisfying $M_n=o_\P(1)$,
	  \begin{align*}
		\Q_{1-\alpha(1+M_n)}\left[n^{1/2}T_n^{\ndCRThat}(\srxk, \srx, \sry,\srz)|\srx,\sry,\srz\right]\convp z_{1-\alpha}
	  \end{align*}
	  where $T_n^{\ndCRThat}(\srxk,\srx,\sry,\srz)$ is defined as in \eqref{eq:normalized_dcrt_stats}.
	\end{lemma}
  \noindent To apply Lemma \ref{lem:quantile_equivalence} (as well as Lemma \ref{lem:quantile_convergence_ptwise}), we will first verify condition \eqref{eq:variance_lower_bound}-\eqref{eq:non_degeneracy} in \textbf{Regularity condition} are satisfied under the assumptions of Theorem \ref{thm:asymptotic_equivalence}.

  \paragraph{Verification of \eqref{eq:variance_lower_bound}:}

  This is true by assumption \eqref{eq:lower_bound_spacrt}.

  \paragraph{Verification of \eqref{eq:Lyapunov_condition_2}:}

  We verify the condition when $\delta=2$. When condition \eqref{eq:cse_assumption} holds, it suffices to prove 
  \begin{align*}
    \frac{1}{n^2}\sum_{i=1}^n \E_{\lawhat_n}[|\srxk_{in}-\widehat\mu_{n,x}(\srz_{in})|^4|\srx,\srz]=o_{\P}(1).
  \end{align*}
  By Lemma \ref{lem:moment_logpartition}, we know 
  \begin{align*}
    \E_{\lawhat_n}[|\srxk_{in}-\widehat\mu_{n,x}(\srz_{in})|^4|\srx,\srz]=A^{(4)}(\widehat{\theta}_{n,x}(\srz_{in}))+3(A''(\widehat{\theta}_{n,x}(\srz_{in})))^2. 
  \end{align*}
  Since by condition \eqref{eq:cse_assumption}, $\sup_{i}|\widehat{\theta}_{n,x}(\srz_{in})|=O_{\P}(1)$, we know there exists $\delta>0$ such that 
  \begin{align*}
    \P\left[\mathcal{L}\right]\geq 1-\delta,\text{ where }\mathcal{L}\equiv \left\{\sup_{i}|\widehat{\theta}_{n,x}(\srz_{in})|\leq M(\delta)\right\}.
  \end{align*}
  Then on the event $\mathcal{L}$, by the smoothness of function $A$, we have
  \begin{align*}
    \sup_i|A^{(4)}(\widehat{\theta}_{n,x}(\srz_{in}))|
    &
    \leq \sup_{x\in [-M(\delta),M(\delta)]}|A^{(4)}(x)|<\infty,\\ 
    \sup_i|A^{(2)}(\widehat{\theta}_{n,x}(\srz_{in}))|
    &
    \leq \sup_{x\in [-M(\delta),M(\delta)]}|A^{(2)}(x)|<\infty.
  \end{align*}
  Thus we know for any $\delta>0$,
  \begin{align*}
    \P\left[\sup_{i}|A^{(4)}(\widehat{\theta}_{n,x}(\srz_{in}))|\leq \sup_{x\in [-M(\delta),M(\delta)]}|A^{(4)}(x)|<\infty\right]
    &
    \geq\P[\mathcal{L}]\geq 1-\delta\\
    \P\left[\sup_{i}|A^{(2)}(\widehat{\theta}_{n,x}(\srz_{in}))|\leq \sup_{x\in [-M(\delta),M(\delta)]}|A^{(2)}(x)|<\infty\right]
    &
    \geq\P[\mathcal{L}]\geq 1-\delta
  \end{align*}
  This implies
  \begin{align*}
    \sup_i|A^{(4)}(\widehat{\theta}_{n,x}(\srz_{in}))|=O_{\P}(1),\ \sup_i |A^{(2)}(\widehat{\theta}_{n,x}(\srz_{in}))|=O_{\P}(1).
  \end{align*}
  Thus we have 
  \begin{align*}
    \frac{1}{n^2}\sum_{i=1}^n \E_{\lawhat_n}[|\srxk_{in}-\widehat\mu_{n,x}(\srz_{in})|^4|\srx,\srz]
    &
    \leq \frac{1}{n}\left(\sup_i|A^{(4)}(\widehat{\theta}_{n,x}(\srz_{in}))|+3\sup_i(A''(\widehat{\theta}_{n,x}(\srz_{in})))^2\right)\\
    &
    =o_{\P}(1).
  \end{align*}
  When condition \eqref{eq:ccs_assumption} holds, it suffices to prove 
  \begin{align*}
    \frac{1}{n^2}\sum_{i=1}^n (\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in}))^4=o_{\P}(1).
  \end{align*}
  This is true by the condition \eqref{eq:ccs_assumption}.

  \paragraph{Verification of \eqref{eq:non_degeneracy}:}

  $\V_{\lawhat_n}[\srx_{in}|\srz_{in}]=A''(\widehat{\theta}(\srz_{in}))<\infty$ and $(\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in}))^2<\infty$ almost surely can be guaranteed respectively by $|\widehat{\theta}(\srz_{in})|<\infty$ and $|a_{in}|<\infty$ almost surely in assumption \eqref{eq:upper_bound_theta_a}. As for $(\sry_{in}-\mu_{n,y}(\srz_{in}))^2<\infty$, it is true by the integrability of $\sry_{in}$.
  
  \noindent Therefore, applying Lemma \ref{lem:quantile_equivalence}, we have
	  \begin{align*}
		  \Q_{1-\alpha(1+M_n)}\left[n^{1/2}T_n^{\ndCRThat}(\srxk, \srx, \sry,\srz)|\srx,\sry,\srz\right]\convp z_{1-\alpha}.
	\end{align*}
	Moreover, the condition \eqref{eq:non-accumulation-app} holds for the chosen $T_n(\srx,\sry,\srz)$ by condition \eqref{eq:nonaccumulant_condition} so that this proves the asymptotic equivalence of $\phi_{n,\alpha}^{\aux}$ and $\phi_{n,\alpha}^{\asy}$. 
  
	\paragraph{Proof of the equivalence of $\phi_{n,\alpha}^{\aux},\phi_{n,\alpha}^{\dCRT}$:}
  
	In order to prove the asymptotic equivalence between $\phi_{n,\alpha}^{\dCRT}$ and $\phi_{n,\alpha}^{\asy}$, we apply Lemma \ref{lem:unified_asymptotic_equivalence} with the test statistic $T_n(\srx,\sry,\srz)$ to be
	\begin{align*}
	  T_n(\srx,\sry,\srz)\equiv\frac{n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)}{\widehat{S}_n^{\dCRT}}
	\end{align*}
	and cutoff $C_n(\srx,\sry,\srz)$ to be 
	\begin{align*}
		C_n(\srx,\sry,\srz)\equiv \Q_{1-\alpha}\left[n^{1/2}T_n^{\ndCRThat}(\srxk, \srx, \sry,\srz)|\srx,\sry,\srz\right].
	\end{align*}
	By Lemma \ref{lem:quantile_convergence_ptwise}, we have proved that under the assumptions in Theorem \ref{thm:asymptotic_equivalence}, 
	\begin{align*}
	  \Q_{1-\alpha}\left[n^{1/2}T_n^{\ndCRThat}(\srxk, \srx, \sry,\srz)|\srx,\sry,\srz\right]\convp z_{1-\alpha}.
	\end{align*}
	Similarly, the nonaccumulant assumption \eqref{eq:non-accumulation-app} has been satisfied by \eqref{eq:nonaccumulant_condition} so that we have proved the asymptotic equivalence between $\phi_{n,\alpha}^{\dCRT}$ and $\phi_{n,\alpha}^{\asy}$.
  \end{proof}


  \subsection{Proof of Corollary \ref{cor:asymptotic_validity_spacrt}}

  \begin{proof}[Proof of Corollary \ref{cor:asymptotic_validity_spacrt}]
    
    For any $\varepsilon>0$,
    \begin{align*}
      \P_{H_0}[p_{\spacrt}\leq \alpha]
      &
      =\P_{H_0}[p_{\spacrt}\leq 0]+\P_{H_0}[p_{\spacrt}\in (0,\alpha]]\\
      &
      =\P_{H_0}[p_{\spacrt}\leq 0]+\P_{H_0}[p_{\spacrt}\in (0,\alpha],p_{\dCRT}/p_{\spacrt}\leq 1+\varepsilon]\\
      &
      \qquad+\P_{H_0}[p_{\spacrt}\in (0,\alpha],p_{\dCRT}/p_{\spacrt}> 1+\varepsilon]\\
      &
      \leq \P_{H_0}[p_{\spacrt}\leq 0]+\P_{H_0}[p_{\dCRT}/p_{\spacrt}> 1+\varepsilon]\\
      &
      \qquad+\P_{H_0}[p_{\dCRT}\leq\alpha(1+\varepsilon)].
    \end{align*}
    By the asymptotic validity of dCRT, $\lim_{n\rightarrow\infty}\P_{H_0}[p_{\dCRT}\leq\alpha]\leq\alpha$, and conclusion \eqref{eq:approximation_accuracy_spacrt} in Theorem \ref{thm:validity_spacrt}, we have 
    \begin{align*}
      \lim_{n\rightarrow\infty}\P_{H_0}[p_{\spacrt}\leq\alpha]\leq \alpha(1+\varepsilon) + 0 + \lim_{n\rightarrow\infty}\P_{H_0}[p_{\spacrt}\leq 0].
    \end{align*}
    By the positivity result $\P_{H_0}[p_{\spacrt}\leq 0]\rightarrow 0$ in Theorem~\ref{thm:validity_spacrt}, we prove 
    \begin{align*}
      \lim_{n\rightarrow\infty}\P_{H_0}[p_{\spacrt}\leq\alpha]\leq \alpha(1+\varepsilon).
    \end{align*}
    Since $\varepsilon>0$ is arbitrary, we have $\lim_{n\rightarrow\infty}\P_{H_0}[p_{\spacrt}\leq \alpha]\leq \alpha$. Therefore, $\spacrt$ is asymptotic valid.

  \end{proof}


  \subsection{Proof of Lemma \ref{lem:quantile_equivalence}}
  
  \begin{proof}[Proof of Lemma \ref{lem:quantile_equivalence}]
    For any given $\varepsilon\in (0,\min\{1/\alpha-1,1\}),\eta>0$, there exists $N(\varepsilon,\eta)$ such that 
    \begin{align*}
      \P[|M_n|>\varepsilon]<\eta,\ \forall n\geq N(\varepsilon,\eta).
    \end{align*}
    We will use $T_n^{\ndCRThat}$ to denote $T_n^{\ndCRThat}(\srxk,\srx,\sry,\srz)$. Then consider the $1-\alpha(1-\varepsilon)$ and $1-\alpha(1+\varepsilon)$ quantiles, we have with probability at least $1-\eta$, for large enough $n$, the following is true:
    \begin{align*}
      \Q_{1-\alpha(1-\varepsilon)}\left[T_n^{\ndCRThat}|\srx,\sry,\srz\right]\geq \Q_{1-\alpha(1+M_n)}\left[T_n^{\ndCRThat}|\srx,\sry,\srz\right]\geq \Q_{1-\alpha(1+\varepsilon)}\left[T_n^{\ndCRThat}|\srx,\sry,\srz\right].
    \end{align*}
    Then with probability at least $1-\eta$, for sufficiently large $n$, we have
    \begin{align}\label{eq:quantile_upper_bound}
      |\Q_{1-\alpha(1+M_n)}\left[T_n^{\ndCRThat}|\srx,\sry,\srz\right] - z_{1-\alpha}|\leq A_n + B_n
    \end{align}
    where 
    \begin{align*}
      A_n\equiv \left|\Q_{1-\alpha(1-\varepsilon)}\left[T_n^{\ndCRThat}|\srx,\sry,\srz\right]- z_{1-\alpha}\right|,\ B_n\equiv \left|\Q_{1-\alpha(1+\varepsilon)}\left[T_n^{\ndCRThat}|\srx,\sry,\srz\right]- z_{1-\alpha}\right|.
    \end{align*}
    Applying Lemma \ref{lem:quantile_convergence_ptwise}, we have 
    \begin{align*}
      \Q_{1-\alpha(1-\varepsilon)}\left[T_n^{\ndCRThat}|\srx,\sry,\srz\right]\convp z_{1-\alpha(1-\varepsilon)},\ \Q_{1-\alpha(1+\varepsilon)}\left[T_n^{\ndCRThat}|\srx,\sry,\srz\right]\convp z_{1-\alpha(1+\varepsilon)}.
    \end{align*}
    Thus for the given $\varepsilon$ and sufficiently large $n$, we have with probability at least $1-\eta$,
    \begin{align*}
      A_n< \varepsilon+|z_{1-\alpha(1-\varepsilon)}-z_{1-\alpha}|,\ B_n< \varepsilon+|z_{1-\alpha(1+\varepsilon)}-z_{1-\alpha}|.
    \end{align*}
    By the continuity of the quantile function of standard normal distribution, we know there exists a universal constant $C_\alpha$ that only depends on $\alpha$ such that 
    \begin{align*}
      |z_{1-\alpha(1+\varepsilon)}-z_{1-\alpha}| < C_\alpha\varepsilon,\ |z_{1-\alpha(1-\varepsilon)}-z_{1-\alpha}|< C_\alpha\varepsilon.
    \end{align*}
    Then combining \eqref{eq:quantile_upper_bound}, we know with probability at least $1-2\eta$, for sufficiently large $n$, we have 
    \begin{align*}
      \left|\Q_{1-\alpha(1+M_n)}\left[T_n^{\ndCRThat}|\srx,\sry,\srz\right] - z_{1-\alpha}\right|\leq A_n+B_n< 2C_\alpha\varepsilon + 2\varepsilon.
    \end{align*}
    Then since $\eta,\varepsilon$ is arbitrary, we have
    \begin{align*}
		\Q_{1-\alpha(1+M_n)}\left[T_n^{\ndCRThat}(\srxk,\srx,\sry,\srz)|\srx,\sry,\srz\right]=\Q_{1-\alpha(1+M_n)}\left[T_n^{\ndCRThat}|\srx,\sry,\srz\right]\convp z_{1-\alpha}.
    \end{align*}
    Therefore we complete the proof.
  \end{proof}


  \section{A special case of binary sampling}


We will first present a grand result that states the validity of the $\spacrt$ procedure when $\prx$ is a binary random variable. 

\begin{lemma}[Bernoulli sampling]\label{lem:bernoulli_case}
	Suppose $\prx$ is a binary variable following the natural exponential family model \eqref{eq:NEF} and Assumption \ref{assu:non_degeneracy_variance} holds. Furthermore, suppose 
	\begin{align}\label{eq:Lyap-consistency}
		\frac{1}{n}\sum_{i=1}^n (\mu_{n,y}(\srz_{in})-\widehat{\mu}_{n,y}(\srz_{in}))^{4}=o_{\P}(1),\ \frac{1}{n}\sum_{i=1}^n (\theta(\srz_{in})-\widehat \theta_{n,x}(\srz_{in}))^{2}=o_{\P}(1),
	\end{align}
  and either of the following set of conditions hold:
  \begin{itemize}
    \item \textbf{Fourth moment condition}: 
    \begin{align}
      |\widehat{\mu}_{n,y}(\srz_{in})-\mu_{n,y}(\srz_{in})|\overset{a.s.}{\rightarrow}0,\ |\widehat \theta_{n,x}(\srz_{in})-\theta(\srz_{in})|\overset{a.s.}{\rightarrow}0,\ \forall i\in [n],n\in\mathbb{N}_+\label{eq:almost-sure-convergence};\\
      \P[|\theta(\srz_{in})|<\infty]=1,\ \sup_n\E_{\law_n}[\pry^4]<\infty\label{eq:bounded_moment_y}.
    \end{align}
    \item \textbf{Almost sure conditoin:}
    \begin{align}
      |\widehat \theta_{n,x}(\srz_{in})|<\infty,|\widehat \mu_{n,y}(\srz_{in})|<\infty\text{ for any $i,n$ almost surely};\label{eq:almost_sure_bound_NB_case}\\
      \sup_n\E_{\law_n}[\pry^4]<\infty\label{eq:bounded_moment_y_NB_case}.
    \end{align}
    \item \textbf{Bounded condition:}
    \begin{align}
      |\widehat \theta_{n,x}(\srz_{in})|<\infty,|\widehat \mu_{n,y}(\srz_{in})|<\infty\text{ for any $i,n$ almost surely};\label{eq:almost_sure_bound}\\
      \P[\pry\in[-S,S]]=1\text{ for some }S>0.\label{eq:compact_support_y}
    \end{align}
  \end{itemize}
	Then if condition \eqref{eq:x_n_convergence_spacrt} is true, the conclusion in Theorem \ref{thm:validity_spacrt} holds and 
  \begin{align}\label{eq:dCRT_variance_convergence}
    (\widehat{S}_n^{\dCRT})^2=\E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^2(\srx_{in}-\mu_{n,x}(\srz_{in}))^2]+o_{\P}(1).
  \end{align}
  Additionally, if the condition \eqref{eq:nonaccumulant_condition} is true, the conclusion in Theorem \ref{thm:asymptotic_equivalence} holds.
\end{lemma}
\noindent Lemma \ref{lem:bernoulli_case} will be used to prove Theorems \ref{thm:low_dim_glm_spacrt}-\ref{thm:nonparametric_ml_spacrt}.


\begin{remark}
  We can see all these assumptions are very mild conditions. In particular, Assumption \ref{assu:non_degeneracy_variance} is a standard non-degeneracy condition. The conditions \eqref{eq:Lyap-consistency} and \eqref{eq:almost-sure-convergence} only impose the consistency of estimators without the rate condition requirement. This thus gives much flexibility on estimators $\widehat{\mu}_{n,y}(\cdot)$ and $\widehat{\theta}_{n,x}(\cdot)$. Condition \eqref{eq:bounded_moment_y} is a standard regularity condition. These conditions, together with Assumption \ref{assu:non_degeneracy_variance}, are crucial to show the lower bound \eqref{eq:lower_bound_spacrt}.
\end{remark}

We now divide the proof of Lemma \ref{lem:bernoulli_case} into two parts, depending either the \textbf{fourth moment condition} or \textbf{bounded condition} is used.


\subsection{Proof of Lemma \ref{lem:bernoulli_case} with fourth moment condition}

Since $\prx$ is a binary variable and follws model \eqref{eq:NEF}, we know $\prx\mid\prz \sim \mathrm{Ber}(\expit(\theta(\prz)))$. We now verify the conditions in Theorem \ref{thm:validity_spacrt}.

	\paragraph{Verification of \eqref{eq:upper_bound_theta_a}:}
  Since $|\theta(\srz_{in})|<\infty$ almost surely by condition \eqref{eq:bounded_moment_y}, together with $|\widehat \theta_{n,x}(\srz_{in})-\theta(\srz_{in})|\overset{a.s.}{\rightarrow}0$ in assumption \eqref{eq:almost-sure-convergence}, we have
	\begin{align*}
		|\widehat \theta_{n,x}(\srz_{in})|\leq |\widehat \theta_{n,x}(\srz_{in})-\theta(\srz_{in})|+|\theta(\srz_{in})|<\infty,\ \text{almost surely}.
	\end{align*}
	We now show $\sup_n\E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^2]<\infty$. This implies 
	\begin{align}\label{eq:bounded_y}
		|\sry_{in}-\mu_{n,y}(\srz_{in})|<\infty,\forall i\in [n],n\in\mathbb{N}_+\text{ almost surely}.
	\end{align}
	This is true by using Jensen's inequality (Lemma \ref{lem:conditional-jensen}) and Cauchy-Schwarz inequality:
	\begin{align*}
		\sup_n\E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^2]
		&
		\leq 2\sup_n(\E[\sry_{in}^2]+\E[\mu_{n,y}(\srz_{in})^2])\\
		&
		\leq 2\sup_n(\E[\sry_{in}^2]+\E[\sry_{in}^2])=4\sup_n\E[\sry_{in}^2]\leq 4\sqrt{\sup_n\E[\sry_{in}^4]}<\infty.
	\end{align*}
	Then by $|\widehat{\mu}_{n,y}(\srz_{in})-\mu_{n,y}(\srz_{in})|\overset{a.s.}{\rightarrow}0$ in assumption \eqref{eq:almost-sure-convergence}, we have 
	\begin{align*}
		|a_{in}|=|\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in})|\leq |\sry_{in}-\mu_{n,y}(\srz_{in})|+ |\widehat{\mu}_{n,y}(\srz_{in})-\mu_{n,y}(\srz_{in})|<\infty
	\end{align*}
	almost surely. 

	\paragraph{Verification of \eqref{eq:lower_bound_spacrt}:}

	We can write 
	\begin{align*}
		\frac{1}{n}\sum_{i=1}a_{in}^2A''(\widehat \theta_{n,x}(\srz_{in}))
		&
		=\frac{1}{n}\sum_{i=1}^n a_{in}^2A''(\theta(\srz_{in}))+\frac{1}{n}\sum_{i=1}^n a_{in}(A''(\widehat \theta_{n,x}(\srz_{in}))-A''(\theta(\srz_{in})))\\
		&
		\equiv T_1+T_2.
	\end{align*}
	We will first prove $T_2=o_{\P}(1)$. To see this, we notice that $A''(x)=\exp(x)/ (1+\exp(x))^2$ and it can be easily checked that $A''(x)$ is a lipschitz function with Lipschitz constant $1$. Thus we have
	\begin{align*}
		|T_2|\leq \frac{1}{n}\sum_{i=1}^n a_{in}^2|\widehat \theta_{n,x}(\srz_{in})-\theta(\srz_{in})|\leq \sqrt{\frac{1}{n}\sum_{i=1}^n a_{in}^4}\sqrt{\frac{1}{n}\sum_{i=1}^n |\widehat \theta_{n,x}(\srz_{in})-\theta(\srz_{in})|^2}.
	\end{align*}
	Thus it suffces to show 
	\begin{align}
		\frac{1}{n}\sum_{i=1}^n a_{in}^4=O_{\P}(1),\ \frac{1}{n}\sum_{i=1}^n |\widehat \theta_{n,x}(\srz_{in})-\theta(\srz_{in})|^2=o_{\P}(1).\label{eq:fourth_moment_a}
	\end{align}
	By assumption \eqref{eq:Lyap-consistency}, it suffcies to show $\frac{1}{n}\sum_{i=1}^n a_{in}^4=O_{\P}(1)$. In fact, we have
	\begin{align*}
		\frac{1}{n}\sum_{i=1}^n a_{in}^4\leq \frac{16}{n}\sum_{i=1}^n (\sry_{in}-\mu_{n,y}(\srz_{in}))^4+\frac{16}{n}\sum_{i=1}^n (\widehat{\mu}_{n,y}(\srz_{in})-\mu_{n,y}(\srz_{in}))^4.
	\end{align*}
	By the convergence of $\widehat{\mu}_{n,y}(\srz_{in})$ in assumption \eqref{eq:Lyap-consistency}, it suffices to show $\sup_n\E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^4]<\infty$. This is guaranteed by assumption \eqref{eq:bounded_moment_y} and Jensen's inequality (Lemma \ref{lem:conditional-jensen}):
	\begin{align*}
		\sup_n\E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^4]
		&
		\leq 16\sup_n(\E[\sry_{in}^4]+\E[\E[\sry_{in}\mid\srz_{in}]^4])\\
		&
		\leq  16\sup_n(\E[\sry_{in}^4]+\E[\sry_{in}^4])=32\sup_n\E[\sry_{in}^4]<\infty.
	\end{align*}
	Therefore, we have proved $T_2=o_{\P}(1)$ and now we prove $T_1=\Omega_{\P}(1)$. We first decompose 
	\begin{align*}
		T_1=\frac{1}{n}\sum_{i=1}^n (\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in}))^2A''(\theta(\srz_{in}))\equiv\frac{1}{n}\sum_{i=1}^n (\sry_{in}-\mu_{n,y}(\srz_{in}))^2A''(\theta(\srz_{in}))+T_3
	\end{align*}
	where 
	\begin{align*}
		T_3\equiv  \frac{1}{n}\sum_{i=1}^n \left\{(\sry_{in}-\widehat \mu_{n,y}(\srz_{in}))^2-(\sry_{in}- \mu_{n,y}(\srz_{in}))^2\right\}A''(\theta(\srz_{in})).
	\end{align*}
	Then by the boundedness of $A''$, we have 
	\begin{align*}
		|T_3|
		&
		\leq \frac{1}{n}\sum_{i=1}^n|\widehat{\mu}_{n,y}(\srz_{in})-\mu_{n,y}(\srz_{in})||2\sry_{in}-\mu_{n,y}(\srz_{in})-\widehat\mu_{n,y}(\srz_{in})|\\
		&
		\leq \sqrt{\frac{1}{n}\sum_{i=1}^n (\widehat{\mu}_{n,y}(\srz_{in})-\mu_{n,y}(\srz_{in}))^2}\sqrt{\frac{2}{n}\sum_{i=1}^n (\sry_{in}-\mu_{n,y}(\srz_{in}))^2+\frac{2}{n}\sum_{i=1}^n a_{in}^2}.
	\end{align*}
	We have shown in above that 
	\begin{align*}
		\frac{1}{n}\sum_{i=1}^n (\sry_{in}-\mu_{n,y}(\srz_{in}))^2=O_{\P}(1),\ \frac{1}{n}\sum_{i=1}^n a_{in}^2=O_{\P}(1).
	\end{align*}
	Then by assumption \eqref{eq:Lyap-consistency}, we have 
	\begin{align*}
		\frac{1}{n}\sum_{i=1}^n (\widehat{\mu}_{n,y}(\srz_{in})-\mu_{n,y}(\srz_{in}))^2\leq \sqrt{\frac{1}{n}\sum_{i=1}^n (\widehat{\mu}_{n,y}(\srz_{in})-\mu_{n,y}(\srz_{in}))^4}=o_{\P}(1).
	\end{align*}
	Thus we have proved $T_3=o_{\P}(1)$. The final step is to prove 
	\begin{align*}
		\frac{1}{n}\sum_{i=1}^n (\sry_{in}-\mu_{n,y}(\srz_{in}))^2A''(\theta(\srz_{in}))=\Omega_{\P}(1).
	\end{align*}
	We apply weak law of large numbers to triangular arrays to conclude the proof. In particular, we apply Lemma \ref{lem:wlln} with $\delta=1$ so we need to verify 
	\begin{align*}
		\sup_n\E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^4(A''(\theta(\srz_{in})))^4]<\infty.
	\end{align*}
	Since $|A''(x)|\leq 1$ for any $x\in\mathbb{R}$, by assumption \eqref{eq:bounded_moment_y}, we have
	\begin{align*}
		\sup_n\E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^4(A''(\theta(\srz_{in})))^2]
		&
		\leq \sup_n\E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^4]\\
		&
		\leq 32\sup_n\E[\sry_{in}^4]<\infty.
	\end{align*} 
	Therefore, applying Lemma \ref{lem:wlln} and assumption \ref{assu:non_degeneracy_variance} we obtain 
	\begin{align*}
		\frac{1}{n}\sum_{i=1}^n (\sry_{in}-\mu_{n,y}(\srz_{in}))^2A''(\theta(\srz_{in}))=o_{\P}(1)+\E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^2A''(\theta(\srz_{in}))]=\Omega_{\P}(1).
	\end{align*}
  This also proves the result \eqref{eq:dCRT_variance_convergence}.

  \paragraph{Verification of condition \eqref{eq:ccs_assumption}:}

  Since $\P[\srxk_{in}\in [-1,1]|\mathcal{F}_n]=1$ almost surely, it suffices to show 
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n a_{in}^4=\frac{1}{n}\sum_{i=1}^n (\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in}))^4=O_{\P}(1).
  \end{align*}
  This has been proved in conclusion \eqref{eq:fourth_moment_a}.

\subsection{Proof of Lemma \ref{lem:bernoulli_case} with almost sure condition}

Checking the proof with \textbf{fourth moment condition}, we know the condition \eqref{eq:upper_bound_theta_a} is the only part that differs. However, condition \eqref{eq:upper_bound_theta_a} has been directly assumed in condition \eqref{eq:almost_sure_bound_NB_case}. Therefore we complete the proof.

\subsection{Proof of Lemma \ref{lem:bernoulli_case} with bounded condition}

Verificaitons of conditions \eqref{eq:upper_bound_theta_a} and \eqref{eq:ccs_assumption} are straightforward. As for condition \eqref{eq:lower_bound_spacrt}, we just note that 
\begin{align*}
  \sup_n\E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^4]\leq (2S)^4=16S^4<\infty\quad\text{and}\quad \E[\sry_{in}^4]\leq S^4<\infty.
\end{align*}
Then the proof can go through as that with \textbf{fourth moment condition}. Thus we complete the proof.



\section{Proof of Theorem \ref{thm:low_dim_glm_spacrt}}

\subsection{Proof of the conclusion in Theorem \ref{thm:validity_spacrt}}\label{sec:NB_approximation_accuracy}

We need to apply Lemma \ref{lem:bernoulli_case} with \textbf{almost sure condition}. In particular, the condition \eqref{eq:almost_sure_bound_NB_case} is trivially satisfied by the finiteness of the maximum likelihood estimate $\widehat{\beta},\widehat{\theta}$. Also condition \eqref{eq:bounded_moment_y_NB_case} is true in the low-dimensional setup. Thus it suffices to prove condition \eqref{eq:Lyap-consistency}. We divide the proof into two steps.

\paragraph{Verificaiton of $\frac{1}{n}\sum_{i=1}^n (\mu_{n,y}(\srz_{in})-\widehat{\mu}_{n,y}(\srz_{in}))^{4}=o_{\P}(1)$}

We will prove a stronger result
\begin{align}\label{eq:GLM_L4_convergence}
  \frac{1}{n}\sum_{i=1}^n (\mu_y(\srz_i)-\widehat{\mu}_y(\srz_i))^4=O_\P(1/n^2).
\end{align}
Define the set $C\equiv \{t:|t|\leq C_Z(\|\beta\|_1+\|\theta\|_1)+1\}$. Then consider the event 
\begin{align*}
  \mathcal{C}\equiv \{\srz_i^\top\widehat{\beta},\srz_i^\top\widehat{\theta}\in C,\ \forall i\in[n]\}.
\end{align*}
On the event $\mathcal{C}$, we know 
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n (\mu_y(\srz_i)-\widehat{\mu}_y(\srz_i))^4
  &
  =\frac{1}{n}\sum_{i=1}^n (A'(\srz_i^\top \beta)-A'(\srz_i^\top\widehat{\beta}))^4\\
  &
  \leq \frac{1}{n}\sum_{i=1}^n\sup_{t\in C}(A''(t))^4|\srz_i^\top(\beta-\widehat\beta)|^4\tag{By mean value theorem}\\
  &
  \leq \sup_{t\in C}(A''(t))^4\sup_i\|\srz_i\|_{\infty}^4\|\widehat{\beta}-\beta\|_1^4\tag{By H\"older's inequality.}\\
  &
  \leq \sup_{t\in C}(A''(t))^4C_Z^4\|\widehat{\beta}-\beta\|_1^4\tag{By Assumption \ref{assu:compact_support_Z}}\\
  &
  =O_\P(1/n^2)\tag{By condition \eqref{eq:GLM_theta_consistency}}.
\end{align*}
Now it suffices to prove $\P[\mathcal{C}]\rightarrow1$. To see this, we compute 
\begin{align*}
  |\srz_i^\top \widehat{\beta}|\leq \sup_i\|\srz_i\|_{\infty}\|\widehat{\beta}\|_1\leq C_Z\|\widehat{\beta}\|_1\leq C_Z(\|\beta\|_1+\|\widehat{\beta}-\beta\|_1)
\end{align*}
and 
\begin{align*}
  |\srz_i^\top \widehat{\theta}|\leq \sup_i\|\srz_i\|_{\infty}\|\widehat{\theta}\|_1\leq C_Z\|\widehat{\theta}\|_1\leq C_Z(\|\theta\|_1+\|\widehat{\theta}-\theta\|_1).
\end{align*}
By condition \eqref{eq:GLM_theta_consistency}, we have $\|\widehat{\beta}-\beta\|_1=o_\P(1)$ and $\|\widehat{\theta}-\theta\|_1=o_\P(1)$. Thus $\P[\mathcal{C}]\rightarrow1$.

\paragraph{Verificaiton of $\frac{1}{n}\sum_{i=1}^n (\theta(\srz_{in})-\widehat \theta_{n,x}(\srz_{in}))^{2}=o_{\P}(1)$}

We will also prove a stronger result:
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n (\theta(\srz_{in})-\widehat \theta_{n,x}(\srz_{in}))^{2}=\frac{1}{n}\sum_{i=1}^n (\srz_i^\top\widehat{\theta}-\srz_i^\top\theta)^{2}=O_{\P}(1/n).
\end{align*}
We H\"older's inequality, we have 
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n (\srz_i^\top\widehat{\theta}-\srz_i^\top\theta)^{2}\leq \frac{1}{n}\sum_{i=1}^n\|\srz_i\|_{\infty}^2\|\widehat{\theta}-\theta\|_1^2\leq C_Z^2\|\widehat{\theta}-\theta\|_1^2=O_{\P}(1/n).
\end{align*}

\subsection{Proof of the asymptotic validity under null}

We have verified the conditions for Lemma \ref{lem:bernoulli_case} to hold in section \ref{sec:NB_approximation_accuracy}. Thus we know by conclusion \eqref{eq:dCRT_variance_convergence}
\begin{align*}
  (\widehat{S}_n^{\dCRT})^2\convp \E[(\sry_{i}-\E[\sry_i|\srz_i])^2(\srx_{i}-\E[\srx_i|\srz_i])^2]\equiv \sigma_{\dCRT}^2.
\end{align*}
Thus by Lemma \ref{lem:dCRT_convergence}, it is sufficient to prove 
\begin{align*}
  \sqrt{n}T_n^{\dCRT}\convp N(0,\sigma_{\dCRT}^2).
\end{align*}
It is sufficient to prove the following
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n(\mu_x(\srz_i)-\widehat{\mu}_x(\srz_i))^2=O_\P(1/n)\quad\text{and}\quad\frac{1}{n}\sum_{i=1}^n(\mu_y(\srz_i)-\widehat{\mu}_y(\srz_i))^2=O_\P(1/n).
\end{align*}
For the first claim, by Cauchy-Schwarz inequality and result \eqref{eq:GLM_L4_convergence}, we have
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n(\mu_x(\srz_i)-\widehat{\mu}_x(\srz_i))^2\leq\sqrt{\frac{1}{n}\sum_{i=1}^n(\mu_x(\srz_i)-\widehat{\mu}_x(\srz_i))^4}=O_{\P}(1/n).
\end{align*}
For the second claim, we have 
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n(\mu_y(\srz_i)-\widehat{\mu}_y(\srz_i))^2
  &
  =\frac{1}{n}\sum_{i=1}^n(\expit(\srz_i^\top\theta)-\expit(\srz_i^\top\widehat\theta))^2\\
  &
  \leq \frac{1}{n}\sum_{i=1}^n(\srz_i^\top\theta-\srz_i^\top\widehat\theta)^2\tag{By Lipschitz continuity of $\expit$}\\
  &
  \leq C_Z\|\widehat{\theta}-\theta\|_1^2=O_{\P}(1/n).\tag{By Assumption \ref{assu:compact_support_Z} and \eqref{eq:GLM_theta_consistency}}
\end{align*}
Therefore we complete the proof.


\section{Proof of Theorem \ref{thm:high_dim_glm_spacrt}}

Let us first explicitly define the estimators $\widehat{\beta}$ and $\widehat{\theta}$.
\begin{align}\label{eq:Y_on_X_Z_lasso}
  \widehat{\beta}_n=\arg\min_{\beta}\left\{\frac{1}{n}\sum_{i=1}^n \{A_y(\srz_{in}^\top\beta)-Y_{in}\cdot (\srz_{in}^\top\beta)\}+\lambda_n\|\beta\|_1\right\}
\end{align}
and 
\begin{align}\label{eq:X_on_Z_lasso}
  \widehat{\theta}_n=\arg\min_{\theta}\left\{\frac{1}{n}\sum_{i=1}^n \{\log(1 + \exp(\srz_{in}^\top\theta))-X_i\cdot (\srz_{in}^\top\theta)\}+\nu_n\|\theta\|_1\right\}.
\end{align}


\subsection{Proof of the conclusion in Theorem \ref{thm:validity_spacrt}}

We first present a lemma which acts as a building block for proving Theorem \ref{thm:high_dim_glm_spacrt}.


\begin{lemma}\label{lem:high_dim_glm_spacrt}
  Suppose all the assumptions in Theorem \ref{thm:high_dim_glm_spacrt} except for \eqref{eq:product_sparsity_condition} hold. Recall $\lambda_n$ and $\nu_n$ as in models \eqref{eq:Y_on_X_Z_lasso} and \eqref{eq:X_on_Z_lasso}. Then if we choose 
  \begin{align*}
    \lambda_n=C_{\lambda} \sqrt{\log(d)/n}\quad\text{and}\quad\nu_n=C_{\nu}\sqrt{\log(d)/n}
  \end{align*}
  for some universal constants $C_\lambda,C_\nu$, then conclusion in Theorem \ref{thm:validity_spacrt} hold. Furthermore, the variance convergence \eqref{eq:dCRT_variance_convergence} holds. If additionaly, the condition \eqref{eq:nonaccumulant_condition} is true, the conclusion in Theorem \ref{thm:asymptotic_equivalence} holds.
\end{lemma}
\noindent The proof of Lemma \ref{lem:high_dim_glm_spacrt} will be postponed to section \ref{sec:proof_high_dim_spacrt_equivalence}. 

\subsection{Proof of the asymptotic validity under null}

By Lemma \ref{lem:high_dim_glm_spacrt} and Theorem \ref{thm:asymptotic_equivalence}, in order to prove the power and validity of $\spacrt$, it suffices to show the following conditions are satisfied:
\begin{align*}
  \frac{n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)}{\widehat{S}_n^{\dCRT}}\overset{d}{\rightarrow}N(0,1).
\end{align*}
The proof is then divided to two parts: 
\begin{enumerate}
  \item We first show the convergence of the test statistic $n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)$ to a normal distribution; 
  \item We then show the convergence of the variance $(\widehat{S}_n^{\dCRT})^2$ to the variance of the normal distribution derived.
\end{enumerate}
Then we finish the proof by Slutsky's theorem. In fact, the variance convergence $(\widehat{S}_n^{\dCRT})^2$ has been proved as in Lemma \ref{lem:high_dim_glm_spacrt}. We will dedicate the rest of the proof to show the convergence of $n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)$. We first decompose $n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)$ as follows:
\begin{align*}
  n^{1/2}T_n^{\dCRT}(\srx,\sry,\srz)=\frac{1}{\sqrt{n}}\sum_{i=1}^n (\srx_{in}-\E[\srx_{in}|\srz_{in}])(\sry_{in}-\E[\sry_{in}|\srz_{in}])+\mathrm{Bias}_{1}+\mathrm{Bias}_{2}+\mathrm{Bias}_{3}
\end{align*}
where 
\begin{align*}
  \mathrm{Bias}_1
  &
  =\frac{1}{\sqrt{n}}\sum_{i=1}^n (\srx_{in}-\E[\srx_{in}|\srz_{in}])(\E[\sry_{in}|\srz_{in}]-\widehat{\mu}_{n,y}(\srz_{in}))\\
  \mathrm{Bias}_2
  &
  =\frac{1}{\sqrt{n}}\sum_{i=1}^n (\E[\srx_{in}|\srz_{in}]-\widehat{\mu}_{n,x}(\srz_{in}))(\sry_{in}-\E[\sry_{in}|\srz_{in}])\\
  \mathrm{Bias}_3
  &
  =\frac{1}{\sqrt{n}}\sum_{i=1}^n (\E[\srx_{in}|\srz_{in}]-\widehat{\mu}_{n,x}(\srz_{in}))(\E[\sry_{in}|\srz_{in}]-\widehat{\mu}_{n,y}(\srz_{in})).
\end{align*}
We will now show that these biases will go to $0$ in probability. 
\begin{lemma}[Bias term convergence]\label{lem:lasso_bias_convergence}
  Suppose all the assumptions in Theorem \ref{thm:high_dim_glm_spacrt} hold. Then we have 
  \begin{align*}
    \mathrm{Bias}_1,\mathrm{Bias}_2,\mathrm{Bias}_3=o_{\P}(1).
  \end{align*}
\end{lemma}
Now we finish the proof by applying Lemma \ref{lem:lyapunov-clt} with $W_{in}=(\srx_{in}-\mu_{n,x}(\srz_{in}))(\sry_{in}-\mu_{n,y}(\srz_{in}))/\sqrt{n},\mathcal{F}_n=\{\varnothing,\Omega\}$ and $\delta=2$
\begin{align*}
  \frac{\E[(\srx_{in}-\mu_{n,x}(\srz_{in}))^4(\sry_{in}-\mu_{n,y}(\srz_{in}))^4]}{\E[(\srx_{in}-\mu_{n,x}(\srz_{in}))^2(\sry_{in}-\mu_{n,y}(\srz_{in}))^2]^{2}n} \convp 0.
\end{align*}
converges to $0$ in probability. This is true because of the bound 
\begin{align*}
  &
  \E[(\srx_{in}-\mu_{n,x}(\srz_{in}))^4(\sry_{in}-\mu_{n,y}(\srz_{in}))^4]\leq \E[(\sry_{in}-\mu_{n,y}(\srz_{in}))^4]\\
  &
  =\E[A^{(4)}_y(\srz^\top \theta_n)+3(A''_y(\srz^\top \theta_n))^2]\leq \sup_{\|t\|_2\leq C_Z\sup_n\|\theta\|_n}(A_y^{(4)}(t)+3(A''_y(t))^2)
\end{align*}
and $\inf_n\E[(\srx_{in}-\mu_{n,x}(\srz_{in}))^2(\sry_{in}-\mu_{n,y}(\srz_{in}))^2]>0$ by Assumption \ref{assu:non_degeneracy_variance}. Then we have 
\begin{align*}
  \frac{\sum_{i=1}^n(\srx_{in}-\mu_{n,x}(\srz_{in}))(\sry_{in}-\mu_{n,y}(\srz_{in}))}{\sqrt{n\E[(\srx_{in}-\mu_{n,x}(\srz_{in}))^2(\sry_{in}-\mu_{n,y}(\srz_{in}))^2]}}\convd N(0,1).
\end{align*}
In other words, by Slutsky's theorem, we have proved
\begin{align*}
  \frac{n^{1/2}T_n^{\dCRT}}{\widehat{S}_n^{\dCRT}}\convd N(0,1).
\end{align*}
Then we know \eqref{eq:non-accumulation-app} is satisfied. Thus by Theorem \ref{thm:asymptotic_equivalence_stronger}, we know 
\begin{align*}
  \lim_{n\rightarrow\infty}\E[\phi_{n,\alpha}^{\spacrt}]=\lim_{n\rightarrow\infty}\E[\phi_{n,\alpha}^{\dCRT}]=\lim_{n\rightarrow\infty}\P[\phi_{n,\alpha}^{\asy}]=\alpha.
\end{align*}


\subsection{A strong consistency result for the lasso estimators}

Proof of Lemma \ref{lem:high_dim_glm_spacrt} hinges on a general consistency results proved in \citep{wainwright2019high}.
  
  \begin{lemma}[A modified version of Corollary 9.26 in \citep{wainwright2019high}]\label{lem:oracle_inequality_lasso}
    Consider the lasso estimators \eqref{eq:Y_on_X_Z_lasso} and \eqref{eq:X_on_Z_lasso}. Suppose assumptions \ref{assu:compact_support_Z}-\ref{assu:design_assumption} and condition \eqref{eq:sparsity_condition} hold. If we choose $\lambda_n=C_{\lambda} \sqrt{\log(d)/n}$ and $\nu_n=C_{\nu}\sqrt{\log(d)/n}$ for some universal constants $C_\lambda,C_\nu$, then for any $\varepsilon>0$, there exists $N(\varepsilon)\in\mathbb{N}$ such that wheevern $n\geq N(\varepsilon)$, we have 
    \begin{align*}
      \P[\|\widehat{\theta}_n-\theta\|_1>\varepsilon]\leq 2\exp(-2n^{1-\delta})
    \end{align*}
    and 
    \begin{align*}
      \P[\|\widehat{\beta}_n-\beta_n\|_1>\varepsilon]\leq 2\exp(-2n^{1-\delta}).
    \end{align*}
    Consequently, $\|\widehat{\theta}_n-\theta\|_1$ and $\|\widehat{\beta}_n-\beta_n\|_1$ converge to $0$ almost surely.
  \end{lemma}
  
  \noindent The result is a direct consequence of the concentration inequality for the lasso estimators proved in Corollary 9.26 in \citep{wainwright2019high}. We now give the proof of the lemma.
  \begin{proof}[Proof of Lemma \ref{lem:oracle_inequality_lasso}]
    We need a combination of of Corollary 9.26 and Theorem 9.36 in \citep{wainwright2019high} to prove Lemma \ref{lem:oracle_inequality_lasso}. 
    \begin{lemma}[A combination of Corollary 9.26 and Theorem 9.36 in \citep{wainwright2019high}]\label{lem:exponential_decay_lasso}
      Consider the lasso estimators \eqref{eq:Y_on_X_Z_lasso} and \eqref{eq:X_on_Z_lasso}. Suppose assumptions \ref{assu:compact_support_Z}-\ref{assu:design_assumption} holds. If we choose $\lambda_n=C_{\lambda} \sqrt{\log(d)/n}$ and $\nu_n=C_{\nu}\sqrt{\log(d)/n}$ for some universal constants $C_\lambda,C_\nu$, then we have 
      \begin{align*}
        \P[\|\widehat{\theta}_n-\theta_n\|_1>C_{1}s_{\theta_n}\sqrt{\log(d)/n}]\leq 2\exp(-2\log(d))
      \end{align*}
      and 
      \begin{align*}
        \P[\|\widehat{\beta}_n-\beta_n\|_1>C_{2}s_{\beta_n}\sqrt{\log(d)/n}]\leq 2\exp(-2\log(d)).
      \end{align*}
    \end{lemma}
    \noindent To prove Lemma \ref{lem:oracle_inequality_lasso}, it is sufficient to show for some $\delta\in (0,1)$,
    \begin{align*}
      s_{\theta_n}\sqrt{\log(d)/n}=o(1),\ s_{\beta_n}\sqrt{\log(d)/n}=o(1),\ \log(d)\sim n^{1-\delta}.
    \end{align*}
    These conditions are satisfied by condition \eqref{eq:sparsity_condition}.
  \end{proof}



\subsection{Proof of Lemma \ref{lem:high_dim_glm_spacrt}}\label{sec:proof_high_dim_spacrt_equivalence}


We prove the results by applying Lemma \ref{lem:bernoulli_case} combined with the result in Lemma \ref{lem:oracle_inequality_lasso}. We need to verify conditions \eqref{eq:x_n_convergence_spacrt}, \eqref{eq:Lyap-consistency}-\eqref{eq:bounded_moment_y}. 


\paragraph{Verificaiton of \eqref{eq:x_n_convergence_spacrt}}

Defining $b_{in}\equiv \srx_{in}-\widehat \E[\srx_{in}|\srz_{in}]$, we consider the following decomposition:
\begin{align*}
  T_n^{\dCRT}
  &
  =\frac{1}{n}\sum_{i=1}^n b_{in}(\sry_{in}-\E[\sry_{in}|\srz_{in}])+\frac{1}{n}\sum_{i=1}^n b_{in}(\E[\sry_{in}|\srz_{in}]-\widehat{\mu}_{n,y}(\srz_{in}))\\
  &
  \equiv A_n+B_n.
\end{align*}
It thus suffices to show $A_n=o_\P(1)$ and $B_n=o_{\P}(1)$. We first show $A_n=o_{\P}(1)$ by observing that $A_n$ is just a sample average of conditioally independent random variables on data $(X,Z)$. Since 
\begin{align*}
  \E[A_n^2|X,Z]
  &
  =\frac{1}{n^2}\sum_{i=1}^n b_{in}^2\E[(\sry_{in}-\E[\sry_{in}|\srz_{in}])^2|X,Z]\\
  &
  \leq \frac{1}{n^2}\sum_{i=1}^n \E[(\sry_{in}-\E[\sry_{in}|\srz_{in}])^2|X,Z]\tag{$|b_{in}|\leq 1$ almost surely}\\
  &
  =\frac{1}{n^2}\sum_{i=1}^nA_y''(\srz_{in}^\top\beta_n)\tag{Conditional independence}\\
  &
  \leq \frac{1}{n}\max_{t\in B}A''_y(t)\quad\text{where}\quad B\equiv \{t\in\mathbb{R}:|t|\leq \sup_n\|\beta_n\|_1C_Z + 1\}\tag{Compactness of $X,Z$ and H\"older's inequality},
\end{align*}
then we know $\E[A_n^2]<\max_{t\in B}A''_y(t)$. By conditional Markov's inequality (Lemma \ref{lem:conditional_Markov}): for any $\varepsilon>0$,
\begin{align*}
  \P[|A_n|>\varepsilon|X,Z]\leq \frac{\E[A_n^2|X,Z]}{\varepsilon^2}\leq \frac{\max_{t\in B}A''_y(t)}{n\varepsilon^2}
\end{align*} 
almost surely. Taking expectation on both sides, we have $A_n=o_\P(1)$. Now we show $B_n=o_{\P}(1)$. We observe
\begin{align}\label{eq:uniform_bound_on_support}
  |\E[Y_{in}|\srz_{in}]-\widehat{\mu}_{n,y}(\srz_{in})|=|A'_y(\srz_{in}^\top\beta_n)-A'_y(\srz_{in}^\top \widehat \beta_n)|.
\end{align}
On the event 
\begin{align}\label{eq:magnitude_predictor}
  E_n\equiv \left\{\srz_{in}^\top\beta_n,\srz_{in}^\top \widehat \beta_n\in B\text{ for any }i\in[n]\right\},
\end{align}
we know
\begin{align*}
  |B_n|
  &
  \leq\frac{1}{n}\sum_{i=1}^n |b_{in}(\E[\sry_{in}|\srz_{in}]-\widehat{\mu}_{n,y}(\srz_{in}))|\tag{Traingle inequality}\\
  &
  \leq\frac{1}{n}\sum_{i=1}^n |(\E[\sry_{in}|\srz_{in}]-\widehat{\mu}_{n,y}(\srz_{in}))|\tag{$|b_{in}|\leq 1$ almost surely}\\
  &
  = \frac{1}{n}\sum_{i=1}^n|A'_y(\srz_{in}^\top\beta_n)-A'_y(\srz_{in}^\top \widehat \beta_n)|\tag{By result \eqref{eq:uniform_bound_on_support}}\\
  &
  \leq \frac{1}{n}\sum_{i=1}^n\max_{t\in B}A''_y(t)|\srz_{in}^\top(\widehat{\beta}_n-\beta_n)|\tag{Mean value theorem}\\
  &
  \leq \max_{t\in B}A''_y(t)C_Z\|\widehat{\beta}_n-\beta_n\|_1\tag{H\"older's inequality}.
\end{align*}
By Lemma \ref{lem:oracle_inequality_lasso}, we know $\|\widehat{\beta}_n-\beta_n\|_1$ coverge to $0$ almost surely. Thus it suffices to show $\P[E_n]\rightarrow0$. In fact, we can prove a stronger result.
\begin{lemma}\label{lem:E_n_c_i.o.}
  Suppose the assumptions in Theorem \ref{lem:high_dim_glm_spacrt} hold. Then
  \begin{align*}
    \P[E_n^c \text{ happens infinitely often}]=0.
  \end{align*}
\end{lemma}
\noindent Proof of Lemma \ref{lem:E_n_c_i.o.} can be found in section \ref{sec:proof_E_n_c_i.o.}. With Lemma \ref{lem:E_n_c_i.o.}, we conclude the verificaiton for \eqref{eq:x_n_convergence_spacrt}. 

\paragraph{Verificaiton of \eqref{eq:Lyap-consistency}}

We first show $(1/n)\sum_{i=1}^n (\mu_{n,y}(\srz_{in})-\widehat{\mu}_{n,y}(\srz_{in}))^4=o_\P(1)$. We know one the event $E_n$ (defined in \eqref{eq:magnitude_predictor}),
\begin{align*}
  |A'_y(\srz_{in}^\top\widehat{\beta}_n)|\leq \max_{t\in B}A''_y(t)(C_Z\|\widehat{\beta}_n-\beta_n\|_1).
\end{align*}
Thus we know on the event $E_n$, 
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n (\mu_{n,y}(\srz_{in})-\widehat{\mu}_{n,y}(\srz_{in}))^4
  &
  =\frac{1}{n}\sum_{i=1}^n (A'(\srz_{in}^\top\beta_n)-A'(\srz_{in}^\top\widehat{\beta}_n))^4\\
  &
  \leq \sup_{t\in B}(A''(t))^4\frac{1}{n}\sum_{i=1}^n |\srz_{in}^\top(\widehat{\beta}_n-\beta_n)|^4\tag{By mean value theorem}\\
  &
  \leq \sup_{t\in B}(A''(t))^4C_Z^4\|\widehat{\beta}_n-\beta_n\|_1^4.\tag{By H\"older's inequality}
\end{align*}
Thus by Lemma \ref{lem:oracle_inequality_lasso}, we know $\|\widehat{\beta}_n-\beta_n\|_1\convp 0$ under the given assumption.

\paragraph{Verification of \eqref{eq:almost-sure-convergence}}

We first show $|\widehat{\mu}_{n,y}(\srz_{in})-\mu_{n,y}(\srz_{in})|\rightarrow 0$ almost surely. It suffices to prove these claims on the event $E_n$ by Lemma \ref{lem:E_n_c_i.o.}. In fact, on $E_n$, using again the mean value theorem, we can prove
\begin{align*}
  |\widehat{\mu}_{n,y}(\srz_{in})-\mu_{n,y}(\srz_{in})|
  &
  =|A'(\srz_{in}^\top\widehat{\beta}_n)-A'(\srz_{in}^\top\beta_n)|\\
  &
  \leq \sup_{t\in B}|A''(t)||\srz_{in}^\top(\widehat{\beta}_n-\beta_n)|\\
  &
  \leq \sup_{t\in B}|A''(t)|C_Z\|\widehat{\beta}_n-\beta_n\|_1\rightarrow 0
\end{align*}
The last convergence holds almost surely by Lemma \ref{lem:oracle_inequality_lasso}. Now we prove the claim that $|\widehat{\theta}_{n,x}(\srz_{in})-\theta(\srz_{in})|\rightarrow0$ almost surely. We can bound, using H\"older's inequality,
\begin{align*}
  |\widehat{\theta}_{n,x}(\srz_{in})-\theta(\srz_{in})|\leq |\srz_{in}^\top\widehat{\theta}_n-\srz_{in}^\top\theta_n|\leq C_Z\|\widehat{\theta}_n-\theta_n\|_1\rightarrow 0
\end{align*}
almost surley by Lemma \ref{lem:oracle_inequality_lasso}. This concludes the verification of \eqref{eq:almost-sure-convergence}.


\paragraph{Verification of \eqref{eq:bounded_moment_y}}

We first show $|\theta(\srz_{in})|<\infty$ almost surely. This is because of the following bound:
\begin{align*}
  |\theta(\srz_{in})|=|\srz_{in}^\top\theta_n|\leq \|\srz_{in}\|_\infty\|\theta_n\|_{1}\leq C_Z\sup_n\|\theta_n\|_1<\infty.
\end{align*}
Noe we prove the claim $\sup_n\E_{\law_n}[\pry^4]<\infty$. By the representation $\E[(\pry-\E[\pry|\prz])^4]=\E[A^{(4)}_y(\prz^\top \theta_n)+3(A''_y(\prz^\top \theta_n))^2]$, we can bound
\begin{align*}
  \E[\pry^4]
  &
  \leq 16\E[(\pry-\E[\pry|\prz])^4]+16\E[(\E[\pry|\prz])^4]\\
  &
  \leq 16\E[A^{(4)}_y(\srz^\top \theta_n)]+48\E[(A''_y(\srz^\top \theta_n))^2]+16\E[(A'_y(\srz^\top \theta_n))^4].
\end{align*}
It suffices to prove there exits a universal constant $C_M$ such that the following three statements:
\begin{align*}
  \max\{\sup_n A_y^{(4)}(\srz^\top \theta_n), \sup_n A_y''(\srz^\top \theta_n),\sup_n A_y'(\srz^\top \theta_n)\}\leq C_M<\infty.
\end{align*}
This can be shown by noticing 
\begin{align*}
  \sup_n A_y^{(4)}(\srz^\top \theta_n)\leq \sup_{t\in B}|A_y^{(4)}(t)|,\ \sup_n A_y^{''}(\srz^\top \theta_n)\leq \sup_{t\in B}|A_y^{''}(t)|
\end{align*}
and 
\begin{align*}
  \sup_n A_y'(\srz^\top \theta_n)\leq \sup_{t\in B}|A_y'(t)|.
\end{align*}
Thus $C_M$ can be chosen to be $\max\{\sup_{t\in B}|A_y^{(4)}(t)|,\sup_{t\in B}|A_y^{''}(t)|,\sup_{t\in B}|A_y'(t)|\}$. This concludes the verification of \eqref{eq:bounded_moment_y}.


\subsection{Proof of Lemma \ref{lem:lasso_bias_convergence}}

For $\mathrm{Bias}_1$, by conditional Markov's inequality (Lemma \ref{lem:conditional_Markov}) we have
\begin{align*}
  \P[\mathrm{Bias}_1\geq \varepsilon|\sry,\srz]\leq \frac{\E[\mathrm{Bias}_1^2|\sry,\srz]}{\varepsilon^2}.
\end{align*}
Then we have 
\begin{align*}
  \E[\mathrm{Bias}_1^2|\sry,\srz]
  &
  =\frac{1}{n}\sum_{i=1}^n(\expit)'(\srz_{in}^\top\theta_n)(\mu_{n,y}(\srz_{in})-\widehat{\mu}_{n,y}(\srz_{in}))^2\\
  &
  \leq \frac{1}{n}\sum_{i=1}^n(\mu_{n,y}(\srz_{in})-\widehat{\mu}_{n,y}(\srz_{in}))^2.\tag{Derivative of $\expit(\cdot)$ is bounded.}
\end{align*}
Similarly, for $\mathrm{Bias}_2$, we have 
\begin{align*}
  \P[\mathrm{Bias}_2\geq \varepsilon|\srx,\srz]
  &
  \leq \frac{\E[\mathrm{Bias}_2^2|\srx,\srz]}{\varepsilon^2}\\
  &
  \leq \frac{1}{\varepsilon^2n}\sum_{i=1}^n \E[(\sry_{in}-\E[\sry_{in}|\srz_{in}])^2|\srz_{in}](\mu_{n,x}(\srz_{in})-\widehat{\mu}_{n,x}(\srz_{in}))^2\\
  &
  =\frac{1}{\varepsilon^2n}\sum_{i=1}^n A''_y(\srz_{in}^\top\theta_n)(\mu_{n,x}(\srz_{in})-\widehat{\mu}_{n,x}(\srz_{in}))^2\\
  &
  \leq \frac{\sup_{\|t\|_2\leq C_Z\|\theta_n\|_1}A''_y(t)}{\varepsilon^2n}\sum_{i=1}^n(\mu_{n,x}(\srz_{in})-\widehat{\mu}_{n,x}(\srz_{in}))^2\tag{By H\"older's inequality}.
\end{align*}
For $\mathrm{Bias}_3$, we use Cauchy-Schwarz inequality to bound
\begin{align*}
  |\mathrm{Bias}_3|\leq \sqrt{n}\sqrt{\frac{1}{n}\sum_{i=1}^n(\mu_{n,x}(\srz_{in})-\widehat{\mu}_{n,x}(\srz_{in}))^2}\sqrt{\frac{1}{n}\sum_{i=1}^n(\mu_{n,y}(\srz_{in})-\widehat{\mu}_{n,y}(\srz_{in}))^2}.
\end{align*}
Thus combining above observations, it suffice to prove 
\begin{align}\label{eq:estimation_error_hd_glm}
  \frac{1}{n}\sum_{i=1}^n(\mu_{n,x}(\srz_{in})-\widehat{\mu}_{n,x}(\srz_{in}))^2=o_p(1),\quad \frac{1}{n}\sum_{i=1}^n(\mu_{n,y}(\srz_{in})-\widehat{\mu}_{n,y}(\srz_{in}))^2=o_p(1)
\end{align}
and 
\begin{align}\label{eq:product_esitmation_error_hd_glm}
  \left(\frac{1}{n}\sum_{i=1}^n(\mu_{n,x}(\srz_{in})-\widehat{\mu}_{n,x}(\srz_{in}))^2\right)\cdot\left(\frac{1}{n}\sum_{i=1}^n(\mu_{n,y}(\srz_{in})-\widehat{\mu}_{n,y}(\srz_{in}))^2\right)=o_p(1/n).
\end{align}
We compute 
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n(\mu_{n,x}(\srz_{in})-\widehat{\mu}_{n,x}(\srz_{in}))^2
  &
  \leq \frac{1}{n}\sum_{i=1}^n(\srz_{in}^\top \hat\theta_n-\srz_{in}^\top\theta_n)^2\tag{Lipschitz property}\\
  &
  \leq C_Z\|\hat\theta_n-\theta_n\|_1^2\tag{H\"older's inequality}
\end{align*}
Then by Lemma \ref{lem:oracle_inequality_lasso}, we know $\|\hat\theta_n-\theta_n\|_1^2=O_p(s_{\theta_n}^2\log(d)/n)$. Thus we have 
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n(\mu_{n,x}(\srz_{in})-\widehat{\mu}_{n,x}(\srz_{in}))^2=O_p(s_{\theta_n}^2\log(d)/n).
\end{align*}
We can compute 
\begin{align*}
  &
  \frac{1}{n}\sum_{i=1}^n(\mu_{n,y}(\srz_{in})-\widehat{\mu}_{n,y}(\srz_{in}))^2\\
  &
  =\frac{1}{n}\sum_{i=1}^n (\E[\sry_{in}|\srz_{in}]-A'_y(\srz_{in}^\top \widehat\beta_n)\expit(\srz_{in}^\top\widehat{\theta}_n)-A'_y(\srz_{in}^\top \widehat\beta_n)(1-\expit(\srz_{in}^\top\widehat{\theta}_n)))^2\\
  &
  \leq \frac{2}{n}\sum_{i=1}^n(A'_y(\srz_{in}^\top\beta_n)-A'_y(\srz_{in}^\top\widehat\beta_n))^2+\frac{2}{n}\sum_{i=1}^n(A'_y(\srz_{in}^\top\beta_n)-A'_y(\srz_{in}^\top\widehat\beta_n))^2\\
  &
  \leq \frac{2}{n}\sum_{i=1}^n \sup_{t\in B}(A''_y(t))^2(\srz_{in}^\top \widehat\beta_n-\srz_{in}^\top \beta_n)^2+\frac{2}{n}\sum_{i=1}^n \sup_{t\in B}(A''_y(t))^2(\srz_{in}^\top \widehat\beta_n-\srz_{in}^\top \beta_n)^2\\
  &
  =O(\|\widehat{\beta}_n-\beta_n\|_1^2)\\
  &
  =O_p(s_{\beta_n}^2\log(d)/n).
\end{align*}
where the last inequality is due to mean value theorem and $B$ is defined as in the event \eqref{eq:magnitude_predictor}. Therefore combining the above results and conditions \eqref{eq:sparsity_condition} and \eqref{eq:product_sparsity_condition}, we know claims \eqref{eq:estimation_error_hd_glm} and \eqref{eq:product_esitmation_error_hd_glm} hold so that we finish the proof.

\subsection{Proof of Lemma \ref{lem:E_n_c_i.o.}}\label{sec:proof_E_n_c_i.o.}

\begin{proof}[Proof of Lemma \ref{lem:E_n_c_i.o.}]
  First consider $\srz_{in}^\top\beta_n$. By H\"older's iequality, we have
  \begin{align*}
    |\srz_{in}^\top\beta_n|\leq C_Z\sup_n\|\beta_n\|_1\leq C_Z\sup_n\|\beta_n\|_n+1.
  \end{align*}
  Now we consider $Z_{in}^\top \widehat{\beta}_n$. Similarly, we have 
  \begin{align*}
    |\srz_{in}^\top\widehat{\beta}_n|\leq C_Z\|\widehat{\beta}_n-\beta_n\|_1+C_Z\|\beta_n\|_1.
  \end{align*}
  Since $\|\widehat{\beta}_n-\beta_n\|_1\rightarrow 0$ almost surely by Lemma \ref{lem:oracle_inequality_lasso}, we know 
  \begin{align*}
    \P[\srz_{in}^\top\widehat{\beta}_n\in B,\forall i\in [n]]
    &
    \geq \P[C_Z\|\widehat{\beta}_n-\beta_n\|_1+C_Z\|\beta_n\|_1\leq C_Z\sup_n\|\beta_n\|_1+1]\\
    &
    \geq \P[C_Z\|\widehat{\beta}_n-\beta_n\|_1\leq|1]\geq \P[C_Z\|\widehat{\beta}_n-\beta_n\|_1\leq 1].
  \end{align*}
  Thus we know 
  \begin{align*}
    \P[E_n^c]\leq 2\P[\|\widehat{\beta}_n-\beta_n\|_1>1/C_Z].
  \end{align*}
  By Lemma \ref{lem:oracle_inequality_lasso}, we know there exists $N(1/C_Z)$ such that for any $n\geq N(1/C_Z)$,
  \begin{align*}
    \P[\|\widehat{\beta}_n-\beta_n\|_1>1/C_Z]\leq \exp(-2n^{1-\delta}).
  \end{align*}
  Then we have
  \begin{align*}
    \sum_{n=1}^{\infty}\P[E_n^c]=\sum_{n=1}^{\infty}\P[\|\widehat{\beta}_n-\beta_n\|_1>1/C_Z]\leq N(1/C_Z) + 2\sum_{n=1}^{\infty}\exp(-2n^{1-\delta})<\infty.
  \end{align*}
  This concludes the proof.
\end{proof}




\section{Proof of Theorem \ref{thm:nonparametric_ml_spacrt}}\label{sec:KRR_proof}

We divide the proof to two parts: proof of the approximation accuracy conclusion in Theorem \ref{thm:validity_spacrt} and proof of the asymptotic validity of $\spacrt$ under the null hypothesis. They will be presented in section \ref{sec:KRR_proof_approximation_accuracy} and section \ref{sec:KRR_proof_validity_under_null} respectively. We first present a general consistency result on the KRR estimator which will be used in both proofs.

\subsection{A general upper bound on the KRR estimator}

Before proceeding to the proof, we present a key lemma which states the finite-sample bound on the in-sample mean square error of the KRR estimator. The statement is a combination of Lemma 30 and 31 in \citet{Shah2018}.
\begin{lemma}[KRR finite-sample bound]\label{lem:KRR_mse}
  Let $\srz_{1},\ldots,\srz_{n}\in\mathbb{R}^d$ be independently and identically distributed observations and suppose for any $j\neq i$,
  \begin{align*}
    X_{i}=f(\srz_{i})+\varepsilon_{i}\quad\text{where}\quad \mathrm{Var}[\varepsilon_{i}]<\sigma^2\text{ and }\mathrm{Cov}[\varepsilon_{i},\varepsilon_{i}]=0
  \end{align*}
  for $f\in\mathcal{H}$ for some RKHS $(\mathcal{H},\|\cdot\|_{\mathcal{H}})$ with reproducing kernel $k:\mathbb{R}^d\times\mathbb{R}^d\rightarrow\mathbb{R}$. Consider the kernel matrix $K\in\mathbb{R}^{n\times n}$ have $ij$th $K_{ij}=k(\srz_{i},\srz_{j})/n$ and denote the eigvenvalue in the eigen-expansion \eqref{eq:eigven_expansion} of kernel $k$ as $\kappa_{1}\geq \kappa_2\geq\cdots\geq \kappa_n\geq 0$, then, for any $\lambda>0$, the regularization estimator takes the form
  \begin{align*}
    \widehat f_{\lambda}\equiv K(K+\lambda I)^{-1}X\quad\text{where}\quad X=(X_1,\ldots,X_n)^\top\in\mathbb{R}^n,
  \end{align*}
  and satisfies
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n \E[(f(\srz_i)-\widehat{f}_{\lambda}(\srz_i))^2|\srz_1,\ldots,\srz_n]\leq \frac{\sigma^2}{\lambda}\frac{1}{n}\sum_{i=1}^n\min\{\kappa_i/4,\lambda\}+\|f\|_{\mathcal{H}}^2\frac{\lambda}{4}.
  \end{align*}
\end{lemma}
\noindent A direct corollary for Lemma \ref{lem:KRR_mse} is the convergence of mean square error as a consequence of conditional Markov's inequality (Lemma \ref{lem:conditional_Markov}).
\begin{corollary}[Convergence of KRR estimator]\label{cor:KRR_consistency}
  Suppose the conditions in Lemma \ref{lem:KRR_mse} hold. Then if condition \eqref{eq:KRR_kernel_function} holds, we have 
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n (f(\srz_i)-\widehat{f}_{\lambda}(\srz_i))^2=o_\P(1).
  \end{align*}
\end{corollary}

\subsection{Proof of the conclusion in Theorem \ref{thm:validity_spacrt}}\label{sec:KRR_proof_approximation_accuracy}

We will verify the conditions in the version of Lemma \ref{lem:bernoulli_case} with \textbf{bounded condition}. In fact, condition \eqref{eq:compact_support_y} is clearly satisfied by the boundedness of $\pry$ (condition \eqref{eq:KRR_compact_support_Y}) and we now prove the other conditions in the lemma.

\paragraph{Verification of condition \eqref{eq:Lyap-consistency}}

We first show, by Corollary \ref{cor:KRR_consistency} and condition \eqref{eq:KRR_boundedness_mu_y},
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n (\mu_{y}(\srz_i)-\widehat{\mu}_y(\srz_i))^4
  &
  \leq \frac{1}{n}\sum_{i=1}^n(\mu_{y}(\srz_i)-\widehat{\mu}_y(\srz_i))^2(S+\sup_{z\in\mathbb{R}^d}|\widehat{\mu}_y(z)|)^2=o_\P(1).
\end{align*}
Then we show
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n (\theta(\srz_{in})-\widehat{\theta}_{n,x}(\srz_{in}))^2
  &
  =\frac{1}{n}\sum_{i=1}^n (\srz_i^\top\widehat{\theta}-\srz_i^\top\theta)^2\\
  &
  \leq \sup_i\|\srz_i\|_{\infty}\|\widehat{\theta}-\theta\|_1\tag{By H\"older's inequality}\\
  &
  \leq C_Z\|\widehat{\theta}-\theta\|_1\tag{By Assumption \ref{assu:compact_support_Z}}\\
  &
  =o_\P(1)\tag{By condition \eqref{eq:KRR_mu_x_consistency}}.
\end{align*}
This completes the verification of condition \eqref{eq:Lyap-consistency}.


\paragraph{Verificaiton of condition \eqref{eq:almost_sure_bound}}

The $\widehat{\theta}_{n,x}(\srz_{in})=\srz_i^\top\widehat{\theta}$ is finite almost surely by the definition of maximum likelihood estimator. We will show that $|\widehat{\mu}_{y}(\srz_i)|<\infty$ almost surely for any $i$. This is obvious by noticing the KRR estimators:
\begin{align*}
  \widehat{\mu}_{y}(\srz_i)=K_{\srz_i}(K+\lambda_n I)^{-1}Y,\quad\text{where}\quad K_{Z_i}=(K_{1i},\ldots,K_{ni}).
\end{align*}
Then by the finiteness and positivity of $\lambda_n$ and the positive semidefiniteness of the kernel matrix $K$, we know the claim is true.

\paragraph{Verificaiton of condition \eqref{eq:x_n_convergence_spacrt}}

We prove a stronger result: 
\begin{align}\label{eq:KRR_x_n_convergence_spacrt}
  \sqrt{n}T_n^{\dCRT}\convd N(0,\sigma^2_{\dCRT})\quad\text{where}\quad
  \sigma^2_{\dCRT}=\E[(\srx_i-\mu_x(\srz_i))^2(\sry_i-\mu_y(\srz_i))^2].
\end{align}
By Lemma \ref{lem:dCRT_convergence}, it suffices to show the following 
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n(\mu_x(\srz_i)-\widehat{\mu}_x(\srz_i))^2=o_\P(1),
  &
  \quad\frac{1}{n}\sum_{i=1}^n(\mu_y(\srz_i)-\widehat{\mu}_y(\srz_i))^2=o_\P(1)\\
  \left(\frac{1}{n}\sum_{i=1}^n(\mu_x(\srz_i)-\widehat{\mu}_x(\srz_i))^2\right)
  &
  \left(\frac{1}{n}\sum_{i=1}^n(\mu_y(\srz_i)-\widehat{\mu}_y(\srz_i))^2\right)=o_\P(1/n),
\end{align*}
and 
\begin{align*}
  \frac{1}{\sqrt{n}}\sum_{i=1}^n (\srx_i-\mu_x(\srz_i))(\sry_i-\mu_y(\srz_i))\convd N(0,\sigma^2_{\dCRT}).
\end{align*}
The last claim is justfied by a application of central limit Theorem under the existence of the second moment. We now focus on proving the other claims. For the first claim, we have 
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n(\mu_x(\srz_i)-\widehat{\mu}_x(\srz_i))^2\leq \sup_i\|\srz_i\|_{\infty}\|\widehat{\theta}-\theta\|_1\leq C_Z\|\widehat{\theta}-\theta\|_1=o_\P(1).
\end{align*}
For the second claim, we know it is true by Corollary \ref{cor:KRR_consistency}. Next, we prove the third claim. This is obvious by the above arugments and condition \eqref{eq:KRR_mu_x_consistency}.



\subsection{Proof of the asymptotic validity under null}\label{sec:KRR_proof_validity_under_null}

By Theorem \ref{thm:asymptotic_equivalence_stronger}, we just need to show that 
\begin{align*}
  \frac{\sqrt{n}T_n^{\dCRT}}{\widehat{S}_n^{\dCRT}}\convd N(0,1)
\end{align*}
Note that this statement implies condition \eqref{eq:non-accumulation-app} holds. We have proved the conditions required in Lemma \ref{lem:bernoulli_case} in section \ref{sec:KRR_proof_approximation_accuracy} so we know by the conclusion \eqref{eq:dCRT_variance_convergence} that 
\begin{align*}
  (\widehat S_n^{\dCRT})^2\convp \sigma_{\dCRT}^2\equiv \E[(\srx_i-\mu_x(\srz_i))^2(\sry_i-\mu_y(\srz_i))^2].
\end{align*}
We have proved the conclusiogn \eqref{eq:KRR_x_n_convergence_spacrt} so by Slutsky's theorem, the desired claim is true.



\section{A preliminary introduction to HMM}\label{sec:HMM}


\subsection{Generating genetic variable from multinomial HMM}\label{sec:gen_snp}

Consider matrix $X\in\mathbb{R}^{n\times p}$ with i.i.d. rows following the law of $\prx\in\{0,1\}^d$. Each variable of $\prx$, $\prx_j$, can be thought as a copy inherited from either paternal side or maternal side. Multinomial HMM (mHMM) is a mathematical model to model $\prx_j$ because of its probabilistic structure mimicking the hereditary nature. Before introducing the mHHM, let us first define a multinomial Markov chain (mMC) as follows:

\begin{definition}[multinomial Makrov chain]\label{def:Markov_chain}
	We say a random variable $\pru\in\{0,1,\ldots,K\}^d$ follows a multinomial Marko Chain distribution, $\pru\sim \text{mMC}(q,Q)$, if 
	\begin{align*}
		\pru_1\sim q\quad\text{and}\quad \pru_{j}|\pru_{j-1}\sim Q(\cdot|\pru_{j-1}),
	\end{align*}
	where $q$ is some multinomial distribution supported on $\{0,1,\ldots,K\}$ and $Q(\cdot|\cdot)\in\mathbb{R}^{K\times K}$ is a transition matrix so that $Q(\cdot|\pru_{j-1}=u)$ is a transition distribution given the observation $\pru_{j-1}$ is $u$.
\end{definition}
Then we consider the following definition for mHMM:
\begin{definition}[mHMM]\label{def:mhhm}
	Consider a $p$-dimensional random variable $\bm X\in\{0,1\}^d$. We say $\bm X\sim \text{mHMM}(\pru(q,Q),e)$ is an observation from a multinomial hidden Markov model with a latent \textit{multinomial Markov chain} $\bm U\equiv (\pru_1,\ldots,\pru_p)^\top \sim \text{mMC}(q,Q)$ and a multinomial emission distribution $\bm X_j|\pru_j\sim e$. In particular, we denote $\P[\bm X_j=x_j|\pru_j=u_j]\equiv e(x_j|u_j)$.
\end{definition}
\noindent In particular, we assume $\prx\equiv (\prx_1,\ldots,\prx_p)^\top$ is generated from a mHMM:
\begin{align*}
	\prx\sim \text{mHMM}(q,\pru(Q,e)).
\end{align*}
The graphical illustration of the latent DGP for observed genetic variable $\prx=(\prx_1,\ldots,\prx_p)^\top$ is shown in Figure \ref{fig:HHM}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.7\textwidth]{figures-and-tables/HMM/mHHM.pdf}
	\caption{Graphical illustration of a multinomial hidden Markov model. Dashed rounded rectangles: unobserved variables.}\label{fig:HHM}
\end{figure}
\noindent The following identities can be easily obtained:
\begin{align}
	\P[\prx_j,\pru_{j+1}|\pru_j]
	&\label{eq:factorization}\tag{CI} 
	=\P[\prx_j|\pru_j]\P[\pru_{j+1}|\pru_j];\\
	\P[\prx_{j-1},\pru_j|\pru_{1:(j-1)},\prx_{1:(j-2)}]
	&\label{eq:Markov}\tag{Markov}
	=\P[\prx_{j-1},\pru_{j}|\pru_{j-1}],
\end{align}
where we use $\pru_{1:j}$ to denote $(\pru_1,\ldots,\pru_j)^\top\in\mathbb{R}^j$ and same notation is applied to $\prx_{1:j}$.

\subsection{Computing conditional distribution-related quantities}\label{sec:computing_conditional_quantities}

Define $x_{-j}\equiv (x_1,\ldots,x_{j-1},x_{j+1},\ldots,x_p)^\top\in\mathbb{R}^{p-1}$. After the joint distribution $\prx$ has been estimated from fitting the mHMMs, we can compute the conditional distribution $\prx_j|\prx_{-j}$ for any $j\in[p]$ with the estimated parameters. We will dedicate this section to discussing how the conditional expectation, conditional cumulant generating function (CCGF) and its derivatives, required by the $\spacrt$ method, can be easily computed. We first boil these quantities down to the conditional probability $\P[\prx_j=x_j|\prx_{-j}=x_{-j}]$. For the ease of notation, we now define 
\begin{align*}
	p(x_j,x_{-j})\equiv \P[\prx_j=x_j|\prx_{-j}=x_{-j}].
\end{align*}
\begin{enumerate}
	\item\textbf{Conditional expectation:} For any given $x_{-j}\in\mathbb{R}^{p-1}$, we can compute
	\begin{align*}
		\E[\prx_j|\prx_{-j}=x_{-j}]=p(1,x_{-j})+2\cdot p(2,x_{-j}).
	\end{align*}
	
	\item\textbf{Function value and derivatives of CCGF:} $\spacrt$ method requires the knowledge of the CCGF function and its derivatives up to second order. Defining $D(t,x_{-j})\equiv \E[\exp(t\prx_j)|\prx_{-j}=x_{-j}]$, we can compute
	\begin{align*}
		D(t,x_{-j})=p(0,x_{-j})+\exp(t)p(1,x_{-j})+\exp(2t)p(2,x_{-j}).
	\end{align*}
	Thus we can compute the CCGF value:
	\begin{align*}
		K(t,x_{-j})
		&
		\equiv\log\E[\exp(t\prx_j)|\prx_{-j}=x_{-j}]=\log(D(t,x_{-j}))\quad\forall t\in\mathbb{R}.
	\end{align*}
	Then we compute the first derivative of CCGF:
	\begin{align*}
		\nabla_t K(t,x_{-j})=\frac{\exp(t)p(1,x_{-j})+2\exp(2t)p(2,x_{-j})}{D(t,x_{-j})}
	\end{align*}
	and the second derivative of CCGF:
	\begin{align*}
		\nabla_t^2 K(t,x_{-j})=\frac{\exp(t)p(1,x_{-j})+4\exp(2t)p(2,x_{-j})}{D(t,x_{-j})}-\big[\nabla_t K(t,x_{-j})\big]^2.
	\end{align*}
\end{enumerate}

Thus from the above computation, we know it is sufficient to compute the conditional probability $p(x_j,x_{-j})$. In fact, the following Proposition shows that it can be computed iteratively. To state the Proposition, we need to introduce necessary notation. For $u,\bar u\in\{0,1,\ldots,K\} $ and $x\in\{0,1\}^d$, we define 
\begin{align}
	A_j(u,x)\equiv \P[\prx_{1:(j-1)}=x_{1:(j-1)},\pru_j=u],\ A_1(u,x)\equiv \P[\pru_1=u];\label{eq:def_A}\\
  B_j(u,x)\equiv \P[\prx_{(j+1):p}=x_{(j+1):p}|\pru_j=u],\ B_p(u,x)\equiv1.\label{eq:def_B}
\end{align}

\begin{proposition}[Iterative computation of $p(x_j,x_{-j})$]\label{prop:conditional_prob}
	Suppose $\prx\sim \text{mHMM}(\pru(q,Q),e)$. Then we have
	\begin{align}\label{eq:conditional_prob_A_B}
		p(x_j,x_{-j})=\frac{\sum_{\bar u\in\{0,1,\ldots,K\}}e(x_j|\bar u)\cdot A_j(\bar u,x)\cdot B_j(\bar u,x)}{\sum_{\bar u\in \{0,1,\ldots,K\}}A_j(\bar u,x)\cdot B_j(\bar u,x)}.
	\end{align}
	In particular, $A_j$ and $B_j$ can be computed in the following recursive manner: for any $u\in\{0,1,\ldots,K\}$ and $x\in\{0,1\}^{d}$, 
	\begin{align*}
		A_j(z,x)=\sum_{\bar u\in\{0,1,\ldots,K\}}A_{j-1}(\bar u,x)\cdot r(x_{j-1}|\bar u)\cdot Q(u|\bar u);&\\
		B_j(u,x)=\sum_{\bar u\in\{0,1,\ldots,K\}}B_{j+1}(\bar u,x)\cdot r(x_{j+1}|\bar u)\cdot Q(\bar u|u).&
	\end{align*}
\end{proposition}
The proof can be found in Appendix \ref{sec:conditional_prob}.


\subsection{Proof of Proposition \ref{prop:conditional_prob}}\label{sec:conditional_prob}

\paragraph{Proof of conclusion \eqref{eq:conditional_prob_A_B}}

First, let us consider the following marginalization:
\begin{align*}
	&
	\P[\bm X_j=x_j|\bm X_{-j}=x_{-j}]\\
	&
	=\sum_{\bar u\in \{0,1,\ldots,K\}}\P[\prx_j=x_j|\prx_{-j}=x_{-j},\pru_{j}=\bar u]\P[\pru_j=\bar u|\prx_{-j}=x_{-j}]\\
	&\tag{\ref{eq:Markov} property}
	=\sum_{\bar u\in \{0,1,\ldots,K\}}\P[\bm X_j=x_j|\pru_{j}=\bar u]\P[\pru_j=\bar u|\bm X_{-j}=x_{-j}]\\
	&
	=\sum_{\bar u\in \{0,1,\ldots,K\}}e(x_j|\bar u)\P[\pru_j=\bar u|\bm X_{-j}=x_{-j}].
\end{align*}
Now we compute the conditioanl probability $\P[\pru_j=u|\bm X_{-j}=x_{-j}]$. In particular, we consider the following decomposition
\begin{align*}
	\P[\pru_j=u|\bm X_{-j}=x_{-j}]=\frac{\P[\pru_j=u,\bm X_{-j}=x_{-j}]}{\P[\bm X_{-j}=x_{-j}]}=\frac{\P[\pru_j=u,\bm X_{-j}=x_{-j}]}{\sum_{\bar u\in \{0,1,\ldots,K\}}\P[\pru_j=\bar u,\bm X_{-j}=x_{-j}]}.
\end{align*}
Thus we have 
\begin{align*}
	\P[\bm X_j=x_j|\bm X_{-j}=x_{-j}]=\frac{\sum_{\bar u\in \{0,1,\ldots,K\}}e(x_j|\bar u)\P[\pru_j=\bar u,\bm X_{-j}=x_{-j}]}{\sum_{\bar u\in \{0,1,\ldots,K\}}\P[\pru_j=\bar u,\bm X_{-j}=x_{-j}]}
\end{align*}
so that we only need to compute the probability $\P[\pru_j=u,\bm X_{-j}=x_{-j}]$. Further, we can do the following calculation 
\begin{align*}
	\P[\pru_j=u,\bm X_{-j}=x_{-j}]
	&
	=\P[\bm X_{1}=x_1,\ldots, \prx_{j-1}=x_{j-1},\pru_{j}=u]\\
	&
	\quad\times\P[\prx_{j+1}=x_{j+1},\ldots, \prx_{p}=x_{p}|\prx_{1}=x_1,\ldots, \prx_{j-1}=x_{j-1},\pru_{j}=u]\\
	&
	=\P[\prx_{1}=x_1,\ldots, \prx_{j-1}=x_{j-1},\pru_{j}=u]\\
	&\tag{\ref{eq:Markov} property}
	\quad\times \P[\bm X_{j+1}=x_{j+1},\ldots, \prx_{p}=x_{p}|\pru_{j}=u]\\
	&
	= A_{j}(u,x)\cdot B_j(u,x).
\end{align*}
The detailed derivation of $A_j(u,x)$ and $B_j(u,x)$ will be present in the next two sections.

\paragraph{Computing $A_j(u,x)$ using forward algorithm}

We use prove-by-induction to derive $A_j(u,x)$ and the induction is on index $j$. Since $A_1(u,x)=\P[\pru_1=u]$ and we can compute:
\begin{align*}
	A_2(u,x)
	&
	=\P[\prx_1=x_1,\pru_2=u]\\
	&
	=\sum_{\bar u\in\{0,1,\ldots,K\}}\P[\pru_1=\bar u]\P[\prx_1=x_1,\prz_2=z|\pru_1=\bar u]\\
	&\tag{\ref{eq:factorization} property}
	=\sum_{\bar u\in\{0,1,\ldots,K\}}\P[\pru_1=\bar u]\P[\prx_1=x_1|\pru_1=\bar u]\P[\pru_2=u|\pru_1=\bar u]\\
	&\tag{Definition \eqref{eq:def_A}}
	=\sum_{\bar u\in\{0,1,\ldots,K\}}A_1(\bar u,x)\cdot e(x_1|\bar u)\cdot Q(u|\bar u).
\end{align*}
Then we can easily show that 
\begin{align*}
	A_j(u,x)
	&
	=\P[\prx_{1}=x_1,\ldots, \prx_{j-1}=x_{j-1},\pru_{j}=u]\\
	&
	=\sum_{\bar u\in\{0,1,\ldots,K\}}\P[\prx_{1}=x_1,\ldots, \prx_{j-2}=x_{j-2},\pru_{j-1}=\bar u]\\
	&
	\qquad\times\P[\prx_{j-1}=x_{j-1},\prz_{j}=z|\prx_{1}=x_1,\ldots, \prx_{j-2}=x_{j-2},\pru_{j-1}=\bar u]\\
	&
	=\sum_{\bar u\in\{0,1,\ldots,K\}}\P[\prx_{1}=x_1,\ldots, \prx_{j-2}=x_{j-2},\pru_{j-1}=\bar u]\\
	&\tag{\ref{eq:Markov} property}
	\qquad\times\P[\prx_{j-1}=x_{j-1},\pru_{j}=u|\pru_{j-1}=\bar u]\\
	&\tag{Definition \eqref{eq:def_A}}
	=\sum_{\bar u\in\{0,1,\ldots,K\}}A_{j-1}(\bar u,x)\cdot e(x_{j-1}|\bar u)\cdot Q(u|\bar u).
\end{align*}
With iterative computation, one can obtain the probability $A_j(u,x)$ for any $j\in[d]$ and $u\in \{0,1,\ldots,K\}$ and $x\in\{0,1\}^d$.

\paragraph{Computing $B_j(u)$ using backward algorithm}

Using $B_p(u,x)=1$ for any $u$, we can compute 
\begin{align*}
	B_{p-1}(u,x)
	&
	=\P[\prx_{p}=x_p|\pru_{p-1}=u]\\
	&
	=\sum_{\bar u\in\{0,1,\ldots,K\}}\P[\prx_p=x_p,\pru_{p}=\bar u|\pru_{p-1}=u]\\
	&
	=\sum_{\bar u\in\{0,1,\ldots,K\}} \P[\prx_p=x_p|\pru_p=\bar u,\pru_{p-1}=u]\P[\pru_p=\bar u|\pru_{p-1}=u]\\
	&\tag{\ref{eq:Markov} property}
	=\sum_{\bar u\in\{0,1,\ldots,K\}} \P[\prx_p=x_p|\pru_p=\bar u]\P[\pru_p=\bar u|\pru_{p-1}=u]\\
	&\tag{Definition \eqref{eq:def_B}}
	=\sum_{\bar u\in\{0,1,\ldots,K\}} B_{p}(\bar u)\cdot r(x_p|\bar u)\cdot s(\bar u|u).
\end{align*}
By induction, we can compute 
\begin{align*}
	B_{j}(u,x)
	&
	=\P[\bm X_{j+1}=x_{j+1},\ldots, \prx_{p}=x_{p}|\pru_{j}=u]\\
	&
	=\sum_{\bar u\in\{0,1,\ldots,K\}}\P[\bm X_{j+1}=x_{j+1},\ldots, \prx_{p}=x_{p},\pru_{j+1}=\bar u|\pru_{j}=u]\\
	&
	=\sum_{\bar u\in\{0,1,\ldots,K\}}\P[\prx_{j+2}=x_{j+2},\ldots, \prx_{p}=x_{p}|\pru_{j+1}=\bar u,\pru_{j}=u,\prx_{j+1}=x_{j+1}]\\
	&
	\qquad\times\P[\prx_{j+1}=x_{j+1},\prz_{j+1}=\bar u|\pru_{j}=u]\\
	&\tag{\ref{eq:Markov} property}
	=\sum_{\bar u\in\{0,1,\ldots,K\}}\P[\prx_{j+2}=x_{j+2},\ldots, \prx_{p}=x_{p}|\pru_{j+1}=\bar u]\\
	&\tag{\ref{eq:factorization} property}
	\qquad\times \P[\prx_{j+1}=x_{j+1}|\pru_{j+1}=\bar u]\cdot \P[\pru_{j+1}=\bar u|\pru_{j}=u]\\
	&\tag{Definition \eqref{eq:def_B}}
	=\sum_{\bar u\in\{0,1,\ldots,K\}}B_{j+1}(\bar u,x)\cdot e(x_{j+1}|\bar u)\cdot Q(\bar u|u).
\end{align*}
With iterative computation, one can obtain the probability $B_j(u,x)$ for any $j\in[d]$ and $u\in \{0,1,\ldots,K\}$ and $x\in\{0,1\}^d$.

\paragraph{Final form: combining $A_j(u,x)$ and $B_j(u,x)$}

Now we compute the probability $\P[\pru_j=u_j,\prx_{-j}=x_{-j}]$ using the derivation in the above sections. In partiuclar, fixing a set of value $x_{-j}$, we compute 
\begin{align*}
	\P[\prz_j=z,\prx_{-j}=x_{-j}]=\frac{A_j(z,x)\cdot B_j(z,x)}{\sum_{\bar z\in \{0,1,\ldots,K\}}A_j(\bar u,x)\cdot B_j(\bar u,x)}\quad\forall j\in[d],
\end{align*}
where $A_j(u,x),B_j(u,x)$ can be computed iteratively as before.

\section{Additional simulation details in Section \ref{sec:CRISPR-screens}}\label{sec:additional_details_CRISPR}



\subsection{Source of sparsity in single-cell CRISPR screens}\label{sec:source_sparsity_CRISPR}

Recall in such experiments, each cell receives several perturbations targeting different genome elements. Due to such pooling of a large number of perturbations in a single experiment, most perturbations are present in only a small fraction of cells. Furthermore, gene expression data are measured as RNA molecule counts, and when measured at single-cell resolution, the relatively small number of total RNA molecules measured per cell and the large number of genes result in many genes having zero expression in most cells \citep{Svensson2020}.

\subsection{Parameters and methods implementation in Section \ref{sec:CRISPR-screens}}\label{sec:simulation_methods_CRISPR_screens}



\paragraph{Parameters used in Section \ref{sec:CRISPR-screens}}

We adopt the parameter settings displayed in Table \ref{tab:simulation_parameter}. Note that the bolded values of -5 for $\gamma_0$ and $\beta_0$ are the default parameter values. Instead of testing all combinations of these two parameters, we vary one of them while fixing the other to -5. Furthermore, note that our choices of $\rho$ differ based on whether we are carrying out left- or right-sided tests.
\begin{table}[!h]
  \centering
  \begin{tabular}{c|c|c|c|c|c}
  $\gamma_0$ & $\beta_0$ & $\rho$ (left-sided) & $\rho$ (right-sided) & $r$ & $n$ \\
  \hline
  $-6$ & $-6$ & $-4$ & $0$ & $0.05$ & $5000$ \\
  $\bm{-5}$ & $\bm{-5}$ & $-3$ & $0.5$ & $1$ & \\
  $-4$ & $-4$ & $-2$ & $1$ & $10$ & \\
  $-3$ & $-3$ & $-1$ & $1.5$ & & \\
  $-2$ & $-2$ & $0$ & $2$ & & \\
  \end{tabular}
\caption{Simulation parameter choices.}
\label{tab:simulation_parameter}
\end{table}

\paragraph{Methods details in Section \ref{sec:CRISPR-screens}}
\begin{itemize}
  \item The \textbf{spaCRT} (Algorithm~\ref{alg:spacrt}), where $\prx \mid \prz$ is fit based on a logistic regression model and $\pry \mid \prz$ is fit based on a negative binomial regression model. The size parameter $r$ is estimated by applying the method of moments to the residuals of the Poisson regression of $Y$ on $Z$ \citep{Katsevich2020c,Barry2024}. This method (called ``precomputed'' in Table~\ref{tab:methodology_summary}) is fast but less accurate than maximum likelihood estimation, but is sufficient for the $\spacrt$, which does not require accurate estimation of the size parameter. We use the \texttt{uniroot} function in \texttt{R} to solve the equation saddlepoint equation. When the solution is not found or the resulting $p$-value $p_{\spacrt}$ is not in the range $[0,1]$, we use the $p$-value based on the $\GCM$ test as a backup (see below). We found the failure to solve the saddlepoint equation quite rare, occurring in at most $1.3\%$ of replications across all simulation settings.
  \item The \textbf{dCRT} (Algorithm~\ref{alg:dcrt-hat}), with the same fitting procedures as the $\spacrt$ and $M =$ 10,000.
  \item The \textbf{GCM test} \citep{Shah2018}, which is based on the asymptotically normal test statistic
  \begin{align*}
    T_n^{\GCM}\equiv\frac{T_n^{\dCRT}(X,Y,Z)}{\widehat{S}_n},\ \widehat{S}_n^2\equiv \frac{1}{n}\sum_{i=1}^n R_{in}^2-\left(\frac{1}{n}\sum_{i=1}^n R_{in}\right)^2,
  \end{align*}
  where $T_n^{\dCRT}(X,Y,Z)$ is defined as in \eqref{eq:dCRThat} and
  \begin{align*}
    R_{in} \equiv (\srx_{in}-\widehat{\mu}_{n,x}(\srz_{in}))(\sry_{in}-\widehat{\mu}_{n,y}(\srz_{in})).
  \end{align*}
  We use the same fitting procedures for the GCM test as for $\spacrt$ and dCRT.
  \item The \textbf{negative binomial regression score test} (implemented via the \verb|glm.nb()| function in the \verb|MASS| package). This function computes the  maximum likelihood estimate of the size parameter iteratively, which is slower than the precomputed approach but more accurate. We choose this approach since the score test relies more heavily on the accuracy of the size parameter estimate.
  \end{itemize}
  For negative binomial regression, the estimation of size parameter is a crucial step. $\GCM,\dCRT$ and $\spacrt$ use the simple method of moments estimator, while the score test uses a more sophisticated estimator that requires iterative computation. The detailed discussion of these dispersion estimation methods and other details of testing methods can be found in Appendix \ref{sec:simulation_methods_CRISPR_screens}. The comparison of different methods applied is summarized in Table~\ref{tab:methodology_summary}. We applied both left- and right-sided variants of each test. All simulations are repeated $10,000$ times for accurate Type-I error estimation for small $p$-value thresholds. 

  \begin{table}[!h]
    \centering
    \begin{tabular}{l|c|c|c}
    Test & Dispersion estimation & Resampling required & Normality based \\
    \hline
    $\GCM$ test & Precomputed & No & Yes \\
    Score test & Iterative & No & Yes \\
    $\dCRT$ & Precomputed & Yes & No \\
    $\spacrt$ & Precomputed & No & No
    \end{tabular}
  \caption{Summary table for testing methods compared.}
  \label{tab:methodology_summary}
  \end{table}


\subsection{Additional simulation results in Section \ref{sec:CRISPR-screens}}\label{sec:additional_simulation_results_CRISPR}
  
  \begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures-and-tables/simulation/NB-regression/FDR/disp-0.05-multiple_testing_rejection_plot.pdf}
    \caption{Averaged number of rejections after BH and Bonferroni corrections under the setup $r=0.05$.}
    \label{fig:simulation-CRISPR-screens-high-multiplicity-disp-0.05}
  \end{figure}
  
  
\clearpage

\section{Additional simulation details in Section \ref{sec:GWAS}}\label{sec:additional_details_GWAS}

\subsection{Source of sparsity in GWAS with rare phenotypes}\label{sec:source_sparsity_GWAS}

GWAS aims to investigate if genetic variation is associated with a phenotype of interest. There can be two sources of sparsity in such analysis. On one hand, the phenotype of interest ($Y$) can be rare, i.e. only a small fraction of the population has the phenotype. Typical examples include certain rare diseases which have low prevalence in the population. On the other hand, the sparsity can come from the rare genetic variation ($X$), i.e., only a small fraction of the population has the genetic variation. 


\subsection{Parameters and methods implementation in Section \ref{sec:GWAS}}\label{sec:simulation_methods_GWAS}

\paragraph{Parameters used in Section \ref{sec:GWAS}}
Recall the logistic regression model:
\begin{align*}
	\law(\pry|\prx)\overset{d}{=}\text{Ber}(\expit(\gamma_0 + \prx^\top \beta))
\end{align*}
where $\gamma_0$ is an intercept term, $\beta$ is a vector of coefficient and $g$ is a smooth function. For the concrete choice of $\beta$, we consider 
\begin{align*}
	\beta = (\underbrace{\eta,\ldots,\eta}_{0.05 * p},\underbrace{-\eta,\ldots,-\eta}_{0.05 * p},\underbrace{0,\ldots,0}_{0.9 * p})^\top
\end{align*}
where $\eta>0$ is a signal strength. We vary $\eta\in\{0, 0.25, 0.5, 0.75, 1\}$. For $\gamma_0$, we consider $\{-3,-2\}$ for \textit{high} and \textit{low} sparsity settings. For the distribution of $\prx$, we consider the mHHM (Definition \ref{def:mhhm}) where we set the support of the hidden variable $\pru_j$, $K$, to be $10$ and that of $\prx$, $M$, to be $2$. The Markov transition matrix is 
\begin{align*}
	Q\equiv 
	\begin{pmatrix}
		\gamma & 1-\gamma & 0 & \ldots & 0 & 0\\
		0 & \gamma & 1-\gamma & \ldots & 0 & 0\\
		\vdots & &  \ddots & & &\vdots\\
		\vdots & &  & \ddots & &\vdots\\
		0 & 0 & 0 & \ldots & \gamma & 1 - \gamma\\
		0 & 0 & 0 & \ldots & 0 & 1\\
	\end{pmatrix}
	\in\mathbb{R}^{K\times K}.
\end{align*}
We set $\gamma=0.9$ to create non-trivial correlation between $\prx_j$. The intial distribution $q$ is a uniform distribution over the support of $\pru_1$. Besides the transition matrix, we also vary the emission distribution $\prx_j|\pru_j$ by considering a beta-piror emission distribution:
\begin{align}\label{eq:beta_emission}
  \P[\prx_j=1|\pru_j=k]\overset{\mathrm{i.i.d.}}{\sim} \mathrm{Beta}(\alpha,\beta).
  \tag{beta-emission}
\end{align}
Hyperparameter $(\alpha,\beta)$ controls the shape of the Beta distribution. We consider two choices: $(\alpha,\beta)=(1,3)$ and $(\alpha,\beta)=(1,1)$. The first set of parameters will put more mass towards small values close to $0$, which induce high sparsity and thus mimic the rare genetic variation setup. The second choice is a uniform distribution over $[0,1]$ with no skewness. Thus this induce low sparsity in $X$ and thus mimic the common genetic variation scenario. The simulation setup can be summarized in Table \ref{tab:simulation_parameter_GWAS}.

\begin{table}[!ht]
  \centering
  \caption{\label{tab:simulation_parameter_GWAS}Parameters considered in GWAS simulation.}
  \centering
  \begin{tabular}[t]{cc}
  \toprule
  Parmeters $(\alpha,\beta,\gamma_0)$ & Sparsity level for $(X,Y)$\\
  \midrule
  $(1,3,-2)$ & (high, low) \\
  $(1,3,-3)$ & (high, high) \\
  $(1,1,-2)$ & (low, low) \\
  $(1,1,-3)$ & (low, high) \\
  \bottomrule
  \end{tabular}
\end{table}
\noindent We will consider regularization parameters $\lambda$ used in \texttt{glmnet} to be $\lambda=\texttt{lambda.1se}$ or $\lambda=\texttt{lambda.min}$. 


\paragraph{Methods details in Section \ref{sec:GWAS}}


\begin{itemize}
  \item The \textbf{spaCRT} (Algorithm~\ref{alg:spacrt}), where the parameters of the HMM, the distribution $\prx$, is fitted based on expectation-maximization (EM) algorithm implemented by \texttt{fastPhase} software \citep{scheet2006fast} and \texttt{fastPhase} has also been a popular method in the recent variable selection literatures \citep{sesia2019gene}. Conditional distribution $\pry \mid \prx_j$ is fitted based on a modified high-dimensional logistic regression with lasso penalization \citep{tibshirani1996regression}. We refer the details of methods implementation to Appendix~\ref{sec:tower-trick}.
  \item The \textbf{Knockoffs} \citep{barber2015controlling,sesia2019gene}, where the distribution of $\prx$ is fitted in the same way as in $\spacrt$ and knockoff variables are constructed via backford sampling algorithm proposed in \citet{sesia2019gene}. The test statistic is chosen to be the difference of absolute coefficient values between the variable of interest $\prx_j$ and its corresponding knockoff variable $\tilde \prx_j$, which are obtained in the high-dimensional logistic regression with lasso penalization for fitting $\pry|\prx,\tilde \prx$, where $\tilde\prx=(\tilde \prx_1,\ldots,\tilde \prx_d)^\top\in\mathbb{R}^d$.
\end{itemize}
We also include \textbf{dCRT} (Algorithm~\ref{alg:dcrt-hat}) with resample $M = 5000$ and \textbf{GCM test} \citep{Shah2018}, with the same fitting procedures as the $\spacrt$. 

\subsection{Tower-trick for spaCRT, dCRT and GCM}\label{sec:tower-trick}

We devote this section to discussing the computational details of each method and we will discuss a simple yet powerful computational trick to compute the leave-one-out conditional expectations $\widehat \E[\pry|\prx_{-j}]$ for $\dCRT,\spacrt$ and $\GCM$. We first observe the following identity:
\begin{align*}
	\E[\pry|\prx_{-j}]=\E[\E[\pry|\prx]|\prx_{-j}]
\end{align*}
where the outer expectation is taken with respect to the measure $\prx_j|\prx_{-j}$. If we can estimate the joint distribution of $\prx$ from data $X$ and one regression estimate for $\E[\pry|\prx]$, computing the conditional expectation $\E[\pry|\prx_{-j}]$ for any $j\in[p]$ is straightforward via the integral evaluation with respect to measure $\prx_j|\prx_{-j}$, without any additional regression fit. In other words, we only need one regression fit for $\E[\pry|\prx]$ and one joint distribution fit for $\prx$. In practice, we consider using the following algorithm:
\begin{enumerate}
  \item \textbf{Estimate distribution $\prx$:} this can be done by using \texttt{fastPhase} \citep{scheet2006fast};
  \item \textbf{Compute regression estimate $\widehat \E[\pry|\prx=x]$:} this can be done by using \texttt{glmnet} \citep{tibshirani1996regression} with the family set to be \texttt{binomial};
  \item \textbf{Compute $\widehat\E[\pry|\prx_{-j}]$ for any $j\in[d]$:} this can be done by computing the following integral:
  \begin{align*}
    \widehat\E[\pry|\prx_{-j}=x_{-j}]=\sum_{x_j\in\{0,1\}}\widehat\E[\pry|\prx=x]\widehat \P[\prx_j=x_j|\prx_{-j}=x_{-j}],
  \end{align*}
  where $\widehat{\P}[\prx_j=x_j|\prx_{-j}=\cdot]$ is estimated using the \texttt{fastPhase} algorithm.
\end{enumerate}


\subsection{Additional simulation results in Section \ref{sec:GWAS}}\label{sec:simulation_results_GWAS}

We present four simulation results with each corresponding to the choice of \texttt{lambda.min} or \texttt{l
ambda.1se} and the sparsity level in $X$. High sparsity in $X$ with \texttt{lambda.1se} has been presented in Figure \ref{fig:simulation-summary-GWAS-skew-lambda-1se}. The other three plots can be found in Figure~\ref{fig:simulation-summary-GWAS-uniform-lambda-1se},\ref{fig:simulation-summary-GWAS-uniform-lambda-min} and \ref{fig:simulation-summary-GWAS-skew-lambda-min}.

We want to point out that the choice of $\lambda$ seems to affect both the FDR and power of the methods. In particular, comparing Figure~\ref{fig:simulation-summary-GWAS-uniform-lambda-1se} and Figure~\ref{fig:simulation-summary-GWAS-uniform-lambda-min}, we can find the FDR can be slightly inflated when \texttt{lambda.min} is used for $\dCRT,\GCM$ and $\spacrt$ methods whereas the FDR is well controlled when \texttt{lambda.1se} is used. The inflation of false positive rate can be because of the tower trick used for these methods. The intuition is that when \texttt{lambda.1se} is used for $\dCRT,\GCM$ and $\spacrt$, the estimated regression coefficients are more sparse and the models obtained from \texttt{glmnet} is less variable whereas \texttt{lambda.min} will lead to more variable models due to the relatively dense model it will produce. On the power side, we can see that the power of $\dCRT,\GCM$ and $\spacrt$ is slightly improved when \texttt{lambda.min} is used. Interestingly, the FDR of Knockoff procedure seems to be more robust to the choice of $\lambda$ whereas the power is worse when \texttt{lambda.min} is used, which is different from the behavior of $\dCRT,\GCM$ and $\spacrt$. 

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1.0\textwidth]{figures-and-tables/simulation/HMM-variable-selection/HMM_simulation_uniform_lambda.1se.pdf}
  \caption{Summary of numerical simulation results for variable selection  with low sparsity in data $X$. All the results are obtained with the regularization parameter $\lambda=\texttt{lambda.1se}$. (a) FDR for $\gamma_0=-3$ (high sparsity) and $\gamma_0=-2$ (low sparsity). (b) Power for the same set of $\gamma_0$. (c) Computation times consumed by different methods. }
  \label{fig:simulation-summary-GWAS-uniform-lambda-1se}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1.0\textwidth]{figures-and-tables/simulation/HMM-variable-selection/HMM_simulation_uniform_lambda.min.pdf}
  \caption{Summary of numerical simulation results for variable selection  with low sparsity in data $X$. All the results are obtained with the regularization parameter $\lambda=\texttt{lambda.min}$. (a) FDR for $\gamma_0=-3$ (high sparsity) and $\gamma_0=-2$ (low sparsity). (b) Power for the same set of $\gamma_0$. (c) Computation times consumed by different methods. }
  \label{fig:simulation-summary-GWAS-uniform-lambda-min}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=1.0\textwidth]{figures-and-tables/simulation/HMM-variable-selection/HMM_simulation_skew_lambda.min.pdf}
  \caption{Summary of numerical simulation results for variable selection with high sparsity in data $X$. All the results are obtained with the regularization parameter $\lambda=\texttt{lambda.min}$. (a) FDR for $\gamma_0=-3$ (high sparsity) and $\gamma_0=-2$ (low sparsity). (b) Power for the same set of $\gamma_0$. (c) Computation times consumed by different methods. }
  \label{fig:simulation-summary-GWAS-skew-lambda-min}
\end{figure}

\clearpage


\section{Application to nonparametric regression}

\subsection{Theoretical results on kernel ridge regression}\label{sec:theory_nonparametric_RF_classification}

In this section, we study the validity of $\spacrt$ when the conditional distribution $\pry|\prz$ is modeled using \textit{kernel ridge regression} (KRR), a representative of nonparametric machine learning methods. Throughout this section, we will assume we are under the classical low-dimensional setup so that we can simplify the subscript $(\srz_{in},\srx_{in},\sry_{in})=(\srz_{i},\srx_{i},\sry_{i})$ and $\theta_n=\theta$. 

Suppose the conditional expectations $\mu_{n,y}\in\mathcal{H}$ for some RKHS $(\mathcal{H},\|\cdot\|_{\mathcal{H}})$ with reproducing kernel $k\in\mathbb{R}^{d}\times \mathbb{R}^d\rightarrow\mathbb{R}$. Let $K\in\mathbb{R}^{n\times n}$ have $ij$th entry $K_{ij}=k(\srz_{i},\srz_{j})/n$ and denote the eigenvalues of $K$ by $\widehat{\kappa}_{1}\geq\widehat{\kappa}_2\geq\cdots\geq\widehat{\kappa}_n\geq 0$. We will assume that kernel function $k$ admits an eigen-expansion of the form 
\begin{align}\label{eq:eigven_expansion}
  k(z,z')=\sum_{j=1}^{\infty}\kappa_{j}e_j(z)e_j(z')
\end{align}
with orthonormal eigenfunctions $\{e_{j}\}_{j=1}^{\infty},$ so $\E[e_je_k]=\indicator(k=j),$ and summable eigenvalues $\kappa_1\geq\kappa_2\geq\cdots\geq0$. Such expansion can be guaranteed by Mercer's theorem if mild conditions are satisfied. For a sequence of regularization parameter $\lambda_n$, we consider the following estimator:
\begin{align}\label{eq:KRR_mu_y}
  \widehat{\mu}_{y}\equiv\arg\min_{\mu_{y}\in\mathcal{H}} \left\{\frac{1}{n}\sum_{i=1}^n (\sry_{i}-\mu_{y}(\srz_{i}))^2+\lambda_n\|\mu_{y}\|_{\mathcal{H}}^2\right\}.
\end{align}
We consider selecting $\lambda_n$ in the following data-dependent way:
\begin{align}\label{eq:lambda_n}
  \lambda_n=\arg\min_{\lambda>0}\left\{\frac{1}{n}\sum_{i=1}^n\frac{\widehat{\kappa}_i^2}{(\widehat{\kappa}_i+\lambda)^2}+\lambda\right\}
\end{align}
We want to emphasize that the way we select the tuning parameter is mainly for the ease of theoretical analysis and similar data-dependent hyperparameter selection has been adopted in previous work \citet{Niu2022a,Shah2018}. As for the estimator $\widehat{\theta}$ for $\theta$, we consider using the maximum likelihood estimator $\widehat{\theta}$. With the estimators $\widehat{\mu}_{y}(\srz_{i})$ and $\widehat{\theta}_{n,x}(\srz_{i})=\srz_{i}^\top\widehat{\theta}$, the $\spacrt$ can be applied with Algorithm \ref{alg:spacrt}. Now we state our main results on the validity guarantee of $\spacrt$.

\begin{theorem}\label{thm:nonparametric_ml_spacrt}
  Consider $\prx\indep\pry|\prz$. Suppose assumptions \ref{assu:non_degeneracy_variance}-\ref{assu:compact_support_Z} hold. Then if the following conditions hold:
  \begin{align}
    \text{support of $\pry$ is compact, i.e., there exists }S, \P[\pry\in[-S,S]]=1;\label{eq:KRR_compact_support_Y}\\
    \|\widehat{\theta}-\theta\|_1=O_\P(1/\sqrt{n});\label{eq:KRR_mu_x_consistency}\\
    \|\widehat{\mu}_{y}\|_{\infty}=O_\P(1);\label{eq:KRR_boundedness_mu_y}\\
    \sum_{j=1}^{\infty}\kappa_j<\infty,\label{eq:KRR_kernel_function}
  \end{align}
  then the conclusion in Theorem \ref{thm:validity_spacrt} hold and $\lim_{n\rightarrow\infty}\E[\phi_{n,\alpha}^{\spacrt}]=\alpha$.
\end{theorem}

Conditions in Theorem \ref{thm:nonparametric_ml_spacrt} are mild conditions. Condition \eqref{eq:KRR_boundedness_mu_y} can be easily verified when linear kernel, i.e. linear ridge regression, is considered. For general choice of kernel, condition \eqref{eq:KRR_boundedness_mu_y} can hold under extra conditions on the kernel function $k$.


\subsection{Simulation on unbalanced nonparametric classification}\label{sec:simulation_nonparametric_RF_classification}



\newpage

\section{Additional figures and tables for real data anlaysis}

We present relevant information about Gasperinit dataset in Section~\ref{sec:additional-details-real-data}. In Section \ref{sec:additional_table_realdata}, we show a table including the number of rejections when applying Bonferroni or BH method to the pairs invlovling the non-targeting perturbations (thus under the null). The total number of hypotheses is $153000$. In Section~\ref{sec:additional_figure_realdata}, we present additional figures for real data analysis including QQ-plots faceting across different effective sample size (Figure \ref{fig:qqplot_lowess}) and QQ-plots faceting across different dispersion parameters (Figure \ref{fig:qqplot_dispersion}). 



\subsection{Overview of the data}\label{sec:additional-details-real-data}

The Gasperini data contain expression measurements on 13,135 genes and CRISPR perturbations targeting 6,105 regulatory elements in $n =$ 207,324 cells. They also contain CRISPR perturbations intended as negative and positive controls. In particular, the data contain 51 non-targeting CRISPR perturbations, which do not target any regulatory element and therefore should have no effect on the expressions of any genes. Furthermore, the data contain 754 CRISPR perturbations targeting genes, rather than regulatory elements. These serve as positive controls, because they are known a priori to have effects on the expressions of the genes they target. Finally, the data contain measurements on six covariates, including four count-based covariates related to library size, one binary covariate indicating the experimental batch, and one continuous covariate indicating the proportion of reads mapping to mitochondrial genes in each cell.


\subsection{Additional tables for the real data analysis}\label{sec:additional_table_realdata}

\input{figures-and-tables/real-data-num-rejection.tex}


\subsection{Additional figures for the real data analysis}\label{sec:additional_figure_realdata}

\begin{figure*}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figures-and-tables/facet_plot_different_withglmnb_100.pdf}
	\caption{QQ-plots for the $p$-values of right-sided test from different methods under low effective sample size.}
	\label{fig:qqplot_lowess}
\end{figure*}

\begin{figure*}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figures-and-tables/facet_plot_different_withglmnb_dispersion.pdf}
	\caption{QQ-plots for the $p$-values of left-sided test from different methods stratified by dispersion parameter.}
	\label{fig:qqplot_dispersion}
\end{figure*}

\clearpage


\end{document}






















 





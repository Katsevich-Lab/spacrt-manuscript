Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Huang2018,
abstract = {In single-cell RNA sequencing (scRNA-seq) studies, only a small fraction of the transcripts present in each cell are sequenced. This leads to unreliable quantification of genes with low or moderate expression, which hinders downstream analysis. To address this challenge, we developed SAVER (single-cell analysis via expression recovery), an expression recovery method for unique molecule index (UMI)-based scRNA-seq data that borrows information across genes and cells to provide accurate expression estimates for all genes.},
author = {Huang, Mo and Wang, Jingshu and Torre, Eduardo and Dueck, Hannah and Shaffer, Sydney and Bonasio, Roberto and Murray, John I. and Raj, Arjun and Li, Mingyao and Zhang, Nancy R.},
doi = {10.1038/s41592-018-0033-z},
file = {:Users/ekatsevi/papers/Nature Methods/Huang et al. - 2018 - SAVER Gene expression recovery for single-cell RNA sequencing.pdf:pdf},
issn = {15487105},
journal = {Nature Methods},
keywords = {denoising,single cell},
mendeley-tags = {denoising,single cell},
number = {7},
pages = {539--542},
pmid = {29941873},
publisher = {Springer US},
title = {{SAVER: Gene expression recovery for single-cell RNA sequencing}},
url = {http://dx.doi.org/10.1038/s41592-018-0033-z},
volume = {15},
year = {2018}
}
@article{Robinson1982,
author = {Robinson, J.},
doi = {10.1111/j.2517-6161.1982.tb01191.x},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society Series B (Methodological)/Robinson - 1982 - Saddlepoint Approximations for Permutation Tests and Confidence Intervals.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {asymptotic approximations,confidence intervals,edgeworth expansions,large deviations,permutation tests,saddlepoint methods},
number = {1},
pages = {91--101},
title = {{Saddlepoint Approximations for Permutation Tests and Confidence Intervals}},
volume = {44},
year = {1982}
}
@article{Reid1988a,
abstract = {This paper reviews Daniels' saddlepoint approximation to the distribution of the mean of a random sample, and the many aspects of second order asymptotic inference that have been developed from it. These include Barndorff-Nielsen's approximation to the distribution of the maximum likelihood estimate, Bartlett factors for the likelihood ratio statistic and approximations to predictive and conditional likelihood. The emphasis is on statistical applications of the saddlepoint method. The intention is to provide fairly broad coverage of the literature and to indicate possibilities for future development. An annotated bibliography is included. {\textcopyright} 1988, Institute of Mathematical Statistics. All Rights Reserved.},
author = {Reid, N.},
doi = {10.1214/ss/1177012906},
file = {:Users/ekatsevi/papers/Statistical Science/Reid - 1988 - Saddlepoint methods and statistical inference(2).pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Asymptotic expansion,Bartlett factor,Conditional inference,Edgeworth series,Exponential families,Likelihood ratio statistic,Maximum likelihood estimate,Saddlepoint approximation,Score statistic,Second order approximations},
number = {2},
pages = {213--238},
title = {{Saddlepoint methods and statistical inference}},
volume = {3},
year = {1988}
}
@article{Liu2022a,
abstract = {We consider the problem of conditional independence testing: given a response {\$}Y{\$} and covariates {\$}(X,Z){\$}, we test the null hypothesis that {\$}Y {\{}\backslashperp\backslash!\backslash!\backslash!\backslashperp{\}} X \backslashmid Z{\$}. The conditional randomization test was recently proposed as a way to use distributional information about {\$}X\backslashmid Z{\$} to exactly and nonasymptotically control Type-I error using any test statistic in any dimensionality without assuming anything about {\$}Y\backslashmid (X,Z){\$}. This flexibility, in principle, allows one to derive powerful test statistics from complex prediction algorithms while maintaining statistical validity. Yet the direct use of such advanced test statistics in the conditional randomization test is prohibitively computationally expensive, especially with multiple testing, due to the requirement to recompute the test statistic many times on resampled data. We propose the distilled conditional randomization test, a novel approach to using state-of-the-art machine learning algorithms in the conditional randomization test while drastically reducing the number of times those algorithms need to be run, thereby taking advantage of their power and the conditional randomization test's statistical guarantees without suffering the usual computational expense. In addition to distillation, we propose a number of other tricks, like screening and recycling computations, to further speed up the conditional randomization test without sacrificing its high power and exact validity. Indeed, we show in simulations that all our proposals combined lead to a test that has similar power to most powerful existing conditional randomization test implementations, but requires orders of magnitude less computation, making it a practical tool even for large datasets. We demonstrate these benefits on a breast cancer dataset by identifying biomarkers related to cancer stage.},
author = {Liu, Molei and Katsevich, Eugene and Janson, Lucas and Ramdas, Aaditya},
doi = {10.1093/biomet/asab039},
file = {:Users/ekatsevi/papers/Biometrika/Liu et al. - 2022 - Fast and powerful conditional randomization testing via distillation(3).pdf:pdf;:Users/ekatsevi/papers/Biometrika/Liu et al. - 2022 - Fast and powerful conditional randomization testing via distillation(4).pdf:pdf},
issn = {14643510},
journal = {Biometrika},
keywords = {Conditional independence test,Conditional randomization test,High-dimensional inference,Machine learning,Model-X},
number = {2},
pages = {277--293},
title = {{Fast and powerful conditional randomization testing via distillation}},
volume = {109},
year = {2022}
}
@techreport{Swanson2019,
author = {Swanson, Jason},
file = {:Users/ekatsevi/papers/Unknown/Swanson - 2019 - Lecture notes on probability theory.pdf:pdf},
title = {{Lecture notes on probability theory}},
url = {http://math.swansonsite.com/19s6245notes.pdf},
year = {2019}
}
@article{Robinson1990,
author = {Robinson, J . and Hoglund, T . and Holst, L . and Quine, M . P .},
file = {:Users/ekatsevi/papers/The Annals of Probability/Robinson et al. - 1990 - On Approximating Probabilities for Small and Large Deviations in Rd.pdf:pdf},
journal = {The Annals of Probability},
number = {2},
pages = {727--753},
title = {{On Approximating Probabilities for Small and Large Deviations in Rd}},
volume = {18},
year = {1990}
}
@article{Butler2008,
abstract = {A general saddlepoint/Monte Carlo method to approximate (conditional) multivariate probabilities is presented. This method requires a tractable joint moment generating function (m.g.f.), but does not require a tractable distribution or density. The method is easy to program and has a third-order accuracy with respect to increasing sample size in contrast to standard asymptotic approximations which are typically only accurate to the first order. The method is most easily described in the context of a continuous regular exponential family. Here, inferences can be formulated as probabilities with respect to the joint density of the sufficient statistics or the conditional density of some sufficient statistics given the others. Analytical expressions for these densities are not generally available, and it is often not possible to simulate exactly from the conditional distributions to obtain a direct Monte Carlo approximation of the required integral. A solution to the first of these problems is to replace the intractable density by a highly accurate saddlepoint approximation. The second problem can be addressed via importance sampling, that is, an indirect Monte Carlo approximation involving simulation from a crude approximation to the true density. Asymptotic normality of the sufficient statistics suggests an obvious candidate for an importance distribution. The more general problem considers the computation of a joint probability for a subvector of random T, given its complementary subvector, when its distribution is intractable, but its joint m.g.f. is computable. For such settings, the distribution may be tilted, maintaining T as the sufficient statistic. Within this tilted family, the computation of such multivariate probabilities proceeds as described for the exponential family setting.},
author = {Butler, R. W. and Sutton, R. K. and Booth, J. G. and Strickland, P. Ohman},
doi = {10.1080/00949650601119833},
file = {:Users/ekatsevi/papers/Journal of Statistical Computation and Simulation/Butler et al. - 2008 - Simulation-assisted saddlepoint approximation.pdf:pdf},
issn = {00949655},
journal = {Journal of Statistical Computation and Simulation},
keywords = {-formula,Conditional inference,Importance sampling,Monte Carlo error,Multivariate t-distribution,Nuisance parameters,P,Saddlepoint density},
number = {8},
pages = {731--745},
title = {{Simulation-assisted saddlepoint approximation}},
volume = {78},
year = {2008}
}
@article{Katsevich2020c,
author = {Barry, Timothy and Wang, Xuran and Morris, John A. and Roeder, Kathryn and Katsevich, Eugene},
file = {:Users/ekatsevi/papers/Genome Biology/Barry et al. - 2021 - SCEPTRE improves calibration and sensitivity in single-cell CRISPR screen analysis.pdf:pdf},
journal = {Genome Biology},
keywords = {CRISPR,gene-enhancer},
mendeley-tags = {CRISPR,gene-enhancer},
title = {{SCEPTRE improves calibration and sensitivity in single-cell CRISPR screen analysis}},
url = {https://doi.org/10.1101/2020.08.13.250092},
volume = {22},
year = {2021}
}
@article{Barndorff1990,
author = {Barndorff-Nielsen, Ole},
file = {::},
journal = {Journal of the Royal Statistical Society, Series B},
number = {3},
pages = {485--496},
title = {{Approximate Interval Probabilities}},
volume = {52},
year = {1990}
}
@article{Daniels1955,
author = {Daniels, Henry E.},
doi = {10.1111/j.2517-6161.1955.tb00177.x},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society Series B (Methodological)/Daniels - 1955 - Discussion on the Paper by Dr. Box and Dr. Andersen.pdf:pdf},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
number = {1},
pages = {26--34},
title = {{Discussion on the Paper by Dr. Box and Dr. Andersen}},
volume = {17},
year = {1955}
}
@article{Abd-Elfattah2009,
abstract = {One of the most popular class of tests for independence between two random variables is the general class of rank statistics which are invariant under permutations. This class contains Spearman's coefficient of rank correlation statistic, Fisher-Yates statistic, weighted Mann statistic and others. Under the null hypothesis of independence these test statistics have a permutation distribution that is usually approximated by using asymptotic normal theory to determine p-values for these tests. In this note we suggest using a saddlepoint approach that is almost exact and needs no simulations in order to calculate the p-value for tests in this class. {\textcopyright} 2009, Institute of Mathematical Statistics. All rights reserved.},
author = {Abd-Elfattah, Ehab F.},
doi = {10.1214/09-EJS371},
file = {:Users/ekatsevi/papers/Electronic Journal of Statistics/Abd-Elfattah - 2009 - Testing for independence Saddlepoint approximation to associated permutation distributions.pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Independence tests,Linear rank test,Permutation distribution,Saddlepoint approximation},
pages = {625--632},
title = {{Testing for independence: Saddlepoint approximation to associated permutation distributions}},
volume = {3},
year = {2009}
}
@article{Goldstein2005,
abstract = {Berry-Esseen-type bounds to the normal, based on zero- and size-bias couplings, are derived using Stein's method. The zero biasing bounds are illustrated in an application to combinatorial central limit theorems in which the random permutation has either the uniform distribution or one that is constant over permutations with the same cycle type, with no fixed points. The size biasing bounds are applied to the occurrences of fixed, relatively ordered subsequences (such as rising sequences) in a random permutation, and to the occurrences of patterns, extreme values, and subgraphs in finite graphs. {\textcopyright} Applied Probability Trust 2005.},
archivePrefix = {arXiv},
arxivId = {math/0511510},
author = {Goldstein, Larry},
doi = {10.1239/jap/1127322019},
eprint = {0511510},
file = {:Users/ekatsevi/Library/Application Support/Mendeley Desktop/Downloaded/Goldstein - 2005 - Berry-esseen bounds for combinatorial central limit theorems and pattern occurrences, using zero and size biasing.pdf:pdf},
issn = {00219002},
journal = {Journal of Applied Probability},
keywords = {Graph,Permutation,Smoothing inequality,Stein's method},
number = {3},
pages = {661--683},
primaryClass = {math},
title = {{Berry-esseen bounds for combinatorial central limit theorems and pattern occurrences, using zero and size biasing}},
volume = {42},
year = {2005}
}
@article{Hemerik2018,
abstract = {When permutation methods are used in practice, often a limited number of random permutations are used to decrease the computational burden. However, most theoretical literature assumes that the whole permutation group is used, and methods based on random permutations tend to be seen as approximate. There exists a very limited amount of literature on exact testing with random permutations, and only recently a thorough proof of exactness was given. In this paper, we provide an alternative proof, viewing the test as a “conditional Monte Carlo test” as it has been called in the literature. We also provide extensions of the result. Importantly, our results can be used to prove properties of various multiple testing procedures based on random permutations.},
author = {Hemerik, Jesse and Goeman, Jelle},
doi = {10.1007/s11749-017-0571-1},
file = {:Users/ekatsevi/papers/Test/Hemerik, Goeman - 2018 - Exact testing with random permutations.pdf:pdf},
isbn = {1174901705711},
issn = {11330686},
journal = {Test},
keywords = {Nonparametric test,Permutation test,Resampling},
number = {4},
pages = {811--825},
publisher = {Springer Berlin Heidelberg},
title = {{Exact testing with random permutations}},
volume = {27},
year = {2018}
}
@article{Skovgaard1987,
author = {Skovgaard, Ib M.},
file = {:Users/ekatsevi/papers/Journal of Applied Probability/Skovgaard - 1987 - Saddlepoint Expansions for Conditional Distributions.pdf:pdf},
journal = {Journal of Applied Probability},
number = {4},
pages = {875--887},
title = {{Saddlepoint Expansions for Conditional Distributions}},
volume = {24},
year = {1987}
}
@article{Silverman1981,
author = {Silverman, B. W.},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society, Series B/Silverman - 1981 - Using kernel density estimates to investigate multimodality.pdf:pdf},
journal = {Journal of the Royal Statistical Society, Series B},
keywords = {bootstrap,bump,chondrites,density estimate,mode,total positivity},
number = {1},
pages = {97--99},
title = {{Using kernel density estimates to investigate multimodality}},
volume = {43},
year = {1981}
}
@article{Daniels1991,
abstract = {SUMMARY: A saddlepoint technique is used to approximate to the density and tail probability of the studentized mean of a random sample. The motivation was to replace bootstrapping of the studentized mean in the way Davison {\&} Hinkley (1988) used the saddlepoint approximation for the unstudentized mean. The method involves first obtaining a bivariate saddlepoint approximation, then, after a nonlinear transformation, integrating out an unwanted variable either numerically or by a Laplace approximation. The tail probability is similarly evaluated either by a further numerical integration or by a Laplace approximation of the Temme type. Two difficulties arise.(i) The nonlinearity of the transformation may result in Laplace approximations failing in the tail when the sample is not large. But numerical integration always works.(ii) In the bootstrap application the saddlepoint approximation may itself break down when the data set contains an outlier. {\textcopyright} 1991 Biometrika Trust.},
author = {Daniels, Henry E. and Young, G. Alastair},
doi = {10.1093/biomet/78.1.169},
file = {:Users/ekatsevi/papers/Biometrika/Daniels, Young - 1991 - Saddlepoint approximation for the studentized mean, with an application to the bootstrap.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Bootstrap,Nonlinear transformation,Saddlepoint approximation,Studentized mean},
number = {1},
pages = {169--179},
title = {{Saddlepoint approximation for the studentized mean, with an application to the bootstrap}},
volume = {78},
year = {1991}
}
@article{Broda2015,
abstract = {Inversion formulae are derived that express the density and distribution function of a ratio of random variables in terms of the joint characteristic function of the numerator and denominator. The resulting expressions are amenable to numerical evaluation and lead to simple asymptotic expansions. The expansions reduce to known results when the denominator is almost surely positive. Their accuracy is demonstrated with numerical examples.},
author = {Broda, Simon A. and Kan, Raymond},
doi = {10.1093/biomet/asv052},
file = {:Users/ekatsevi/papers/Biometrika/Broda, Kan - 2015 - On distributions of ratios.pdf:pdf},
issn = {14643510},
journal = {Biometrika},
keywords = {Characteristic function,Fieller-Creasy problem,Inversion formula,Saddlepoint approximation},
number = {1},
pages = {205--218},
title = {{On distributions of ratios}},
volume = {103},
year = {2015}
}
@article{Gandy2014,
abstract = {Consider testing multiple hypotheses using tests that can only be evaluated by simulation, such as permutation tests or bootstrap tests. This article introduces MMCTest, a sequential algorithm which gives, with arbitrarily high probability, the same classification as a specific multiple testing procedure applied to ideal p-values. The method can be used with a class of multiple testing procedures which includes the Benjamini {\&} Hochberg False Discovery Rate (FDR) procedure and the Bonferroni correction controlling the Familywise Error Rate. One of the key features of the algorithm is that it stops sampling for all the hypotheses which can already be decided as being rejected or non-rejected. MMCTest can be interrupted at any stage and then returns three sets of hypotheses: the rejected, the non-rejected and the undecided hypotheses. A simulation study motivated by actual biological data shows that MMCTest is usable in practice and that, despite the additional guarantee, it can be computationally more efficient than other methods.},
author = {Gandy, Axel and Hahn, Georg},
doi = {10.1111/sjos.12085},
file = {:Users/ekatsevi/papers/Scandinavian Journal of Statistics/Gandy, Hahn - 2014 - MMCTest-A safe algorithm for implementing multiple monte carlo tests.pdf:pdf},
issn = {14679469},
journal = {Scandinavian Journal of Statistics},
keywords = {Benjamini-Hochberg,Bonferroni correction,Bootstrap,False discovery rate,Monte Carlo,Multiple comparisons,Multiple testing,Resampling,Sequential algorithm},
mendeley-tags = {Monte Carlo,Multiple testing},
number = {4},
pages = {1083--1101},
title = {{MMCTest-A safe algorithm for implementing multiple monte carlo tests}},
volume = {41},
year = {2014}
}
@article{Robinson2003a,
abstract = {We consider multidimensional M-functional parameters defined by expectations of score functions associated with multivariate M -estimators and tests for hypotheses concerning multidimensional smooth functions of these parameters. We propose a test statistic suggested by the exponent in the saddlepoint approximation to the density of the function of the M -estimates. This statistic is analogous to the log likelihood ratio in the parametric case. We show that this statistic is approximately distributed as a chi-squared variate and obtain a Lugannani-Rice style adjustment giving a relative error of order n-1. We propose an empirical exponential likelihood statistic and consider a test based on this statistic. Finally we present numerical results for three examples including one in robust regression.},
author = {Robinson, J. and Ronchetti, E. and Young, G. A.},
doi = {10.1214/aos/1059655909},
file = {:Users/ekatsevi/papers/Annals of Statistics/Robinson, Ronchetti, Young - 2003 - Saddlepoint approximations and tests based on multivariate M -estimates.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bootstrap tests,Composite hypothesis,Nonparametric likelihood,Relative error,Smooth functions of M -estimators},
number = {4},
pages = {1154--1169},
title = {{Saddlepoint approximations and tests based on multivariate M -estimates}},
volume = {31},
year = {2003}
}
@unpublished{Law2024,
author = {Law, Philip J and Smith, James and Vijayakrishnan, Jayaram and Barry, Timothy and Chubb, Daniel and Mandelia, Maria and Cornish, Alex J and Katsevich, Eugene and Houlston, Richard S},
file = {:Users/ekatsevi/papers/Unknown/Law et al. - 2024 - Identification of enhancer-gene regulatory interactions in colorectal cancer identified through genome-wide CRISPRi.pdf:pdf},
isbn = {4780908647},
title = {{Identification of enhancer-gene regulatory interactions in colorectal cancer identified through genome-wide CRISPRi perturbations}},
year = {2024}
}
@article{Chen2015,
abstract = {Let X = {\{}Xij: 1 ≤ i, j ≤ n{\}} be an n × n array of independent random variables where n ≥ 2. Let $\Pi$be a uniform random permutation of {\{}1, 2, . . . , n{\}}, independent of X, and let W =$\Sigma$ni=1 Xi$\pi$(i). Suppose X is standardized so that EW = 0, Var(W) = 1.We prove that the Kolmogorov distance between the distribution ofW and the standard normal distribution is bounded by 451$\Sigma$ni,j=1 E|Xij|3/n. Our approach is by Stein's method of exchangeable pairs and the use of a concentration inequality.},
author = {Chen, Louis H.Y. and Fang, Xiao},
doi = {10.3150/13-BEJ569},
file = {:Users/ekatsevi/papers/Bernoulli/Chen, Fang - 2015 - On the error bound in a combinatorial central limit theorem.pdf:pdf},
issn = {13507265},
journal = {Bernoulli},
keywords = {Combinatorial central limit theorem,Concentration inequality,Exchangeable pairs,Stein's method},
number = {1},
pages = {335--359},
title = {{On the error bound in a combinatorial central limit theorem}},
volume = {21},
year = {2015}
}
@article{Adamson2016,
abstract = {Functional genomics efforts face tradeoffs between number of perturbations examined and complexity of phenotypes measured. We bridge this gap with Perturb-seq, which combines droplet-based single-cell RNA-seq with a strategy for barcoding CRISPR-mediated perturbations, allowing many perturbations to be profiled in pooled format. We applied Perturb-seq to dissect the mammalian unfolded protein response (UPR) using single and combinatorial CRISPR perturbations. Two genome-scale CRISPR interference (CRISPRi) screens identified genes whose repression perturbs ER homeostasis. Subjecting ∼100 hits to Perturb-seq enabled high-precision functional clustering of genes. Single-cell analyses decoupled the three UPR branches, revealed bifurcated UPR branch activation among cells subject to the same perturbation, and uncovered differential activation of the branches across hits, including an isolated feedback loop between the translocon and IRE1$\alpha$. These studies provide insight into how the three sensors of ER homeostasis monitor distinct types of stress and highlight the ability of Perturb-seq to dissect complex cellular responses.},
author = {Adamson, Britt and Norman, Thomas M. and Jost, Marco and Cho, Min Y. and Nu{\~{n}}ez, James K. and Chen, Yuwen and Villalta, Jacqueline E. and Gilbert, Luke A. and Horlbeck, Max A. and Hein, Marco Y. and Pak, Ryan A. and Gray, Andrew N. and Gross, Carol A. and Dixit, Atray and Parnas, Oren and Regev, Aviv and Weissman, Jonathan S.},
doi = {10.1016/j.cell.2016.11.048},
file = {:Users/ekatsevi/papers/Cell/Adamson et al. - 2016 - A Multiplexed Single-Cell CRISPR Screening Platform Enables Systematic Dissection of the Unfolded Protein Respon.pdf:pdf},
issn = {10974172},
journal = {Cell},
keywords = {CRIPSRi,CRISPR,Kathryn,Single-cell RNA-seq,cell-to-cell heterogeneity,genome-scale screening,single-cell genomics,unfolded protein response},
mendeley-tags = {CRISPR,Kathryn},
month = {dec},
number = {7},
pages = {1867--1882.e21},
publisher = {Cell Press},
title = {{A Multiplexed Single-Cell CRISPR Screening Platform Enables Systematic Dissection of the Unfolded Protein Response}},
volume = {167},
year = {2016}
}
@article{Ma1999,
author = {Ma, Chunsheng and Robinson, John},
file = {::},
journal = {Journal of the Royal Statistical Society, Series B},
keywords = {difference of order statistics,laplace method,saddlepoint approximation,studentized},
number = {3},
pages = {563--577},
title = {{Saddlepoint Approximations for the Difference of Order Statistics and Studentized Sample Quantiles}},
volume = {61},
year = {1999}
}
@article{Kolassa2007a,
abstract = {This manuscript presents an approximation to the distribution function of a smooth transformation of a random vector, conditional on the event that values of other smooth transformations of the same random vector lie in a small rectangle. This approximation is used to extend the application of standard saddlepoint conditional tail area approximations in circumstances beyond continuous and lattice cases currently justified in the literature. We consider application to two examples, finite sampling and score testing in logistic regression, where conditioning on a rectangle is essential. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Kolassa, John E. and Robinson, John},
doi = {10.1016/j.jspi.2005.11.003},
file = {:Users/ekatsevi/papers/Journal of Statistical Planning and Inference/Kolassa, Robinson - 2007 - Conditional saddlepoint approximations for non-continuous and non-lattice distributions.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Conditional inference,Saddlepoint approximation},
number = {1},
pages = {133--147},
title = {{Conditional saddlepoint approximations for non-continuous and non-lattice distributions}},
volume = {137},
year = {2007}
}
@article{Cox1979a,
author = {Cox, D. R. and Barndorff-Nielsen, O .},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society, Series BSeries B/Cox, Barndorff-Nielsen - 1979 - Edgeworth and Saddle-Point Approximations with Statistical Applications.pdf:pdf},
journal = {Journal of the Royal Statistical Society, Series BSeries B},
keywords = {asymptotic expansion,circular normal distribution,conditional inference,conditional maximum likelihood,edgeworth series,exponential family,hermite polynomials,large deviation,maximum likelihood ratio test,saddle-point,saddlepoint approximation,steepest descent,time-dependent poisson process,von mises},
mendeley-tags = {saddlepoint approximation},
number = {3},
pages = {279--312},
title = {{Edgeworth and Saddle-Point Approximations with Statistical Applications}},
volume = {41},
year = {1979}
}
@article{HeraY.HeKinjalBasu2019,
author = {{Hera Y . He , Kinjal Basu}, Qingyuan Zhao and Art B . Owen},
file = {:Users/ekatsevi/papers/The Annals of Statistics/Hera Y . He , Kinjal Basu - 2019 - PERMUTATION p-VALUE APPROXIMATION VIA GENERALIZED STOLARSKY INVARIANCE.pdf:pdf},
journal = {The Annals of Statistics},
number = {1},
pages = {583--611},
title = {{PERMUTATION p-VALUE APPROXIMATION VIA GENERALIZED STOLARSKY INVARIANCE}},
volume = {47},
year = {2019}
}
@article{Kelbert2023,
abstract = {We present a number of upper and lower bounds for the total variation distances between the most popular probability distributions. In particular, some estimates of the total variation distances in the cases of multivariate Gaussian distributions, Poisson distributions, binomial distributions, between a binomial and a Poisson distribution, and also in the case of negative binomial distributions are given. Next, the estimations of L{\'{e}}vy–Prohorov distance in terms of Wasserstein metrics are discussed, and Fr{\'{e}}chet, Wasserstein and Hellinger distances for multivariate Gaussian distributions are evaluated. Some novel context-sensitive distances are introduced and a number of bounds mimicking the classical results from the information theory are proved.},
author = {Kelbert, Mark},
doi = {10.3390/analytics2010012},
file = {::},
journal = {Analytics},
keywords = {context-sensitive metrics,jensen,le cam,l{\'{e}}vy,pinsker,probability distribution,prohorov distance,s inequalities,s inequality,shannon distance,total variation distance,wasserstein distance},
number = {1},
pages = {225--245},
title = {{Survey of Distances between the Most Popular Distributions}},
volume = {2},
year = {2023}
}
@article{Welch1937,
author = {Welch, B L},
file = {:Users/ekatsevi/papers/Biometrika/Welch - 1937 - On the z-Test in Randomized Blocks and Latin Squares.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {21--52},
title = {{On the z-Test in Randomized Blocks and Latin Squares}},
volume = {29},
year = {1937}
}
@article{Society2009,
author = {Society, Royal Statistical},
file = {:Users/ekatsevi/papers/Society/Society - 2009 - Edgeworth and Saddle-Point Approximations with Statistical Applications Author ( s ) O . Barndorff-Nielsen and D . R ..pdf:pdf},
journal = {Society},
keywords = {asymptotic expansion,circular normal distribution,conditional inference,conditional maximum likelihood,edgeworth series,exponential family,hermite polynomials,large deviation,maximum likelihood ratio test,saddle-point,steepest descent,time-dependent poisson process,von mises},
number = {3},
pages = {279--312},
title = {{Edgeworth and Saddle-Point Approximations with Statistical Applications Author ( s ): O . Barndorff-Nielsen and D . R . Cox Source : Journal of the Royal Statistical Society . Series B ( Methodological ), Vol . 41 , No . 3 Published by : Blackwell Publish}},
volume = {41},
year = {2009}
}
@article{Tong2022,
abstract = {In this article, we propose a model-free conditional feature screening method with false discovery rate (FDR) control for ultra-high dimensional data. The proposed method is built upon a new measure of conditional independence. Thus, the new method does not require a specific functional form of the regression function and is robust to heavy-tailed responses and predictors. The variables to be conditional on are allowed to be multivariate. The proposed method enjoys sure screening and ranking consistency properties under mild regularity conditions. To control the FDR, we apply the Reflection via Data Splitting method and prove its theoretical guarantee using martingale theory and empirical process techniques. Simulated examples and real data analysis show that the proposed method performs very well compared with existing works. Supplementary materials for this article are available online.},
author = {Tong, Zhaoxue and Cai, Zhanrui and Yang, Songshan and Li, Runze},
doi = {10.1080/01621459.2022.2063130},
file = {::},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {False discovery rate control,Ranking consistency,Sure screening,Ultra-high dimensional data analysis},
number = {0},
pages = {1--33},
publisher = {Taylor {\&} Francis},
title = {{Model-Free Conditional Feature Screening with FDR Control}},
url = {https://doi.org/10.1080/01621459.2022.2063130},
volume = {0},
year = {2022}
}
@article{Smucler2019,
abstract = {We consider inference about a scalar parameter under a non-parametric model based on a one-step estimator computed as a plug in estimator plus the empirical mean of an estimator of the parameter's influence function. We focus on a class of parameters that have influence function which depends on two infinite dimensional nuisance functions and such that the bias of the one-step estimator of the parameter of interest is the expectation of the product of the estimation errors of the two nuisance functions. Our class includes many important treatment effect contrasts of interest in causal inference and econometrics, such as ATE, ATT, an integrated causal contrast with a continuous treatment, and the mean of an outcome missing not at random. We propose estimators of the target parameter that entertain approximately sparse regression models for the nuisance functions allowing for the number of potential confounders to be even larger than the sample size. By employing sample splitting, cross-fitting and {\$}\backslashell{\_}1{\$}-regularized regression estimators of the nuisance functions based on objective functions whose directional derivatives agree with those of the parameter's influence function, we obtain estimators of the target parameter with two desirable robustness properties: (1) they are rate doubly-robust in that they are root-n consistent and asymptotically normal when both nuisance functions follow approximately sparse models, even if one function has a very non-sparse regression coefficient, so long as the other has a sufficiently sparse regression coefficient, and (2) they are model doubly-robust in that they are root-n consistent and asymptotically normal even if one of the nuisance functions does not follow an approximately sparse model so long as the other nuisance function follows an approximately sparse model with a sufficiently sparse regression coefficient.},
archivePrefix = {arXiv},
arxivId = {1904.03737},
author = {Smucler, Ezequiel and Rotnitzky, Andrea and Robins, James M.},
eprint = {1904.03737},
file = {:Users/ekatsevi/papers/arXiv/Smucler, Rotnitzky, Robins - 2019 - A unifying approach for doubly-robust {\$}ell{\_}1{\$} regularized estimation of causal contrasts.pdf:pdf},
journal = {arXiv},
title = {{A unifying approach for doubly-robust L1 regularized estimation of causal contrasts}},
url = {http://arxiv.org/abs/1904.03737},
year = {2019}
}
@article{Field2013,
abstract = {We consider the first serial correlation coefficient under an AR(1) model where errors are not assumed to be Gaussian. In this case it is necessary to consider bootstrap approximations for tests based on the statistic since the distribution of errors is unknown. We obtain saddle-point approximations for tail probabilities of the statistic and its bootstrap version and use these to show that the bootstrap tail probabilities approximate the true values with given relative errors, thus extending the classical results of Daniels [Biometrika 43 (1956) 169-185] for the Gaussian case. The methods require conditioning on the set of odd numbered observations and suggest a conditional bootstrap which we show has similar relative error properties. {\textcopyright} Institute of Mathematical Statistics, 2013.},
author = {Field, Chris and Robinson, John},
doi = {10.1214/13-AOS1111},
file = {::},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Autoregression,Saddle-point approximations},
number = {2},
pages = {1035--1053},
title = {{Relative errors for bootstrap approximations of the serial correlation coefficient}},
volume = {41},
year = {2013}
}
@article{Gasperini2019a,
abstract = {Over one million candidate regulatory elements have been identified across the human genome, but nearly all are unvalidated and their target genes uncertain. Approaches based on human genetics are limited in scope to common variants and in resolution by linkage disequilibrium. We present a multiplex, expression quantitative trait locus (eQTL)-inspired framework for mapping enhancer-gene pairs by introducing random combinations of CRISPR/Cas9-mediated perturbations to each of many cells, followed by single-cell RNA sequencing (RNA-seq). Across two experiments, we used dCas9-KRAB to perturb 5,920 candidate enhancers with no strong a priori hypothesis as to their target gene(s), measuring effects by profiling 254,974 single-cell transcriptomes. We identified 664 (470 high-confidence) cis enhancer-gene pairs, which were enriched for specific transcription factors, non-housekeeping status, and genomic and 3D conformational proximity to their target genes. This framework will facilitate the large-scale mapping of enhancer-gene regulatory interactions, a critical yet largely uncharted component of the cis-regulatory landscape of the human genome.},
author = {Gasperini, Molly and Hill, Andrew J. and McFaline-Figueroa, Jos{\'{e}} L. and Martin, Beth and Kim, Seungsoo and Zhang, Melissa D. and Jackson, Dana and Leith, Anh and Schreiber, Jacob and Noble, William S. and Trapnell, Cole and Ahituv, Nadav and Shendure, Jay},
doi = {10.1016/j.cell.2018.11.029},
file = {::},
issn = {10974172},
journal = {Cell},
keywords = {CRISPR,CRISPRi,RNA-seq,crisprQTL,eQTL,enhancer,gene regulation,genetic screen,human genetics,single cell},
number = {1-2},
pages = {377--390.e19},
pmid = {30612741},
title = {{A Genome-wide Framework for Mapping Gene Regulation via Cellular Genetic Screens}},
volume = {176},
year = {2019}
}
@article{Zhou2006,
abstract = {In this paper, we derive saddlepoint approximations for Student's t-statistics for strongly nonlattice random variables without moment conditions. Under very mild conditions, we show that saddlepoint equations always have solutions. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
author = {Zhou, Wang and Jing, Bing Yi},
doi = {10.1007/s00440-005-0494-8},
file = {:Users/ekatsevi/papers/Probability Theory and Related Fields/Zhou, Jing - 2006 - Tail probability approximations for Student's t-statistics.pdf:pdf},
issn = {01788051},
journal = {Probability Theory and Related Fields},
keywords = {Nonlattice and lattice random variables,Saddlepoint approximation,Self-normalized sum,Student's t-statistic,Tail probability},
number = {4},
pages = {541--559},
title = {{Tail probability approximations for Student's t-statistics}},
volume = {136},
year = {2006}
}
@article{DAVID1951,
author = {DAVID, S. T. and KENDALL, M. G. and STUART, A.},
doi = {10.1093/biomet/38.1-2.131},
file = {:Users/ekatsevi/papers/Biometrika/DAVID, KENDALL, STUART - 1951 - Some questions of distribution in the theory of rank correlation.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
number = {1-2},
pages = {131--140},
pmid = {14848118},
title = {{Some questions of distribution in the theory of rank correlation.}},
volume = {38},
year = {1951}
}
@book{Efron2022,
author = {Efron, Bradley},
pages = {1--250},
publisher = {Cambridge University Press},
title = {{Exponential Families in Theory and Practice}},
year = {2022}
}
@article{Knijnenburg2009a,
author = {Knijnenburg, Theo A and Wessels, Lodewyk F A and Reinders, Marcel J T and Shmulevich, Ilya},
doi = {10.1093/bioinformatics/btp211},
file = {:Users/ekatsevi/papers/Bioinformatics/Knijnenburg et al. - 2009 - Fewer permutations , more accurate P -values.pdf:pdf},
journal = {Bioinformatics},
keywords = {permutation test},
mendeley-tags = {permutation test},
pages = {161--168},
title = {{Fewer permutations, more accurate P-values}},
volume = {25},
year = {2009}
}
@book{Kolassa2006,
author = {Kolassa, John E.},
edition = {Third Edit},
file = {:Users/ekatsevi/papers/Unknown/Kolassa - 2006 - Series Approximation Methods in Statistics.pdf:pdf},
isbn = {2013206534},
publisher = {Springer},
title = {{Series Approximation Methods in Statistics}},
year = {2006}
}
@article{Kim2020a,
abstract = {Classical asymptotic theory for statistical hypothesis testing, for example Wilks' theorem for likelihood ratios, usually involves calibrating the test statistic by fixing the dimension d while letting the sample size n increase to infinity. In the last few decades, a great deal of effort has been dedicated towards understanding how these methods behave in high-dimensional settings, where dn and n both increase to infinity together at some prescribed relative rate. This often leads to different tests in the two settings, depending on the assumptions about the dimensionality. This leaves the practitioner in a bind: given a dataset with 100 samples in 20 dimensions, should they calibrate by assuming n {\textgreater}{\textgreater} d, or dn/n ≈ 0.2? This paper considers the goal of dimension-agnostic inference—developing methods whose validity does not depend on any assumption on dn. We describe one generic approach that uses variational representations of existing test statistics along with sample-splitting and self-normalization (studentization) to produce a Gaussian limiting null distribution. We exemplify this technique for a handful of classical problems, such as one-sample mean testing, testing if a covariance matrix equals the identity, and kernel methods for testing equality of distributions using degenerate U-statistics like the maximum mean discrepancy. Without explicitly targeting the high-dimensional setting, our tests are shown to be minimax rate-optimal, meaning that the power of our tests cannot be improved further up to a constant factor. A hidden advantage is that our proofs are simple and transparent. We end by describing several fruitful open directions.},
archivePrefix = {arXiv},
arxivId = {2011.05068},
author = {Kim, Ilmun and Ramdas, Aaditya},
eprint = {2011.05068},
file = {:Users/ekatsevi/papers/arXiv/Kim, Ramdas - 2020 - Dimension-agnostic inference.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--57},
title = {{Dimension-agnostic inference}},
year = {2020}
}
@article{Cox1979,
author = {Cox, D. R. and Barndorff-Nielsen, O .},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society, Series BSeries B/Cox, Barndorff-Nielsen - 1979 - Edgeworth and Saddle-Point Approximations with Statistical Applications.pdf:pdf},
journal = {Journal of the Royal Statistical Society, Series BSeries B},
keywords = {asymptotic expansion,circular normal distribution,conditional inference,conditional maximum likelihood,edgeworth series,exponential family,hermite polynomials,large deviation,maximum likelihood ratio test,saddle-point,saddlepoint approximation,steepest descent,time-dependent poisson process,von mises},
mendeley-tags = {saddlepoint approximation},
number = {3},
pages = {279--312},
title = {{Edgeworth and Saddle-Point Approximations with Statistical Applications}},
volume = {41},
year = {1979}
}
@article{Zhao2021c,
abstract = {Fisher's randomization test (FRT) delivers exact p-values under the strong null hypothesis of no treatment effect on any units whatsoever and allows for flexible covariate adjustment to improve the power. Of interest is whether the resulting covariate-adjusted procedure could also be valid for testing the weak null hypothesis of zero average treatment effect. To this end, we evaluate two general strategies for conducting covariate adjustment in FRTs: the pseudo-outcome strategy that uses the residuals from an outcome model with only the covariates as the pseudo, covariate-adjusted outcomes to form the test statistic, and the model-output strategy that directly uses the output from an outcome model with both the treatment and covariates as the covariate-adjusted test statistic. Based on theory and simulation, we recommend using the ordinary least squares (OLS) fit of the observed outcome on the treatment, centered covariates, and their interactions for covariate adjustment, and conducting FRT with the robust t-value of the treatment as the test statistic. The resulting FRT is finite-sample exact for testing the strong null hypothesis, asymptotically valid for testing the weak null hypothesis, and more powerful than the unadjusted counterpart under alternatives, all irrespective of whether the linear model is correctly specified or not. We start with complete randomization, and then extend the theory to cluster randomization, stratified randomization, and rerandomization, respectively, giving a recommendation for the test procedure and test statistic under each design. Our theory is design-based, also known as randomization-based, in which we condition on the potential outcomes but average over the random treatment assignment.},
archivePrefix = {arXiv},
arxivId = {2010.14555},
author = {Zhao, Anqi and Ding, Peng},
doi = {10.1016/j.jeconom.2021.04.007},
eprint = {2010.14555},
file = {:Users/ekatsevi/papers/Journal of Econometrics/Zhao, Ding - 2021 - Covariate-adjusted Fisher randomization tests for the average treatment effect.pdf:pdf},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Finite-population inference,Permutation test,Randomization distribution,Robust standard error,Studentization,Super-population inference},
number = {2},
pages = {278--294},
title = {{Covariate-adjusted Fisher randomization tests for the average treatment effect}},
volume = {225},
year = {2021}
}
@book{Barndorff-Nielson1989,
abstract = {This book sets out in detail mathematical techniques valuable for giving useful approximate solutions to a wide range of problems in statistical theory and methods as well as in applied probability. The emphasis throughout is on the relatively simple general concepts involved and on their illustration by a wide range of examples, chosen to be of intrinsic interest. The precise mathematical theorems with their associated, rather formidable technical conditions are given as appendices, but the emphasis in the body of the text is on applications. The first four chapters deal with univariate problems, where the key ideas are seen in their simplest, yet widely useful, form. The last three chapters deal with the corresponding multivariate problems. The notation, especially the use of tensor methods, has been chosen to emphasize the parallel with one dimensional results. In addition to the examples, which are an intrinsic part of the text, there are roughly 100 further results and exercises, many of whichoutline recent research results. The book is aimed at a number of different types of reader, including advanced statistics and probability students and research workers in these and related fields.},
author = {Barndorff-Nielson, Ole and Cox, David},
booktitle = {Biometrics},
doi = {10.2307/2532184},
file = {::},
isbn = {9780412314001},
issn = {0006341X},
number = {2},
publisher = {Springer Science $\backslash${\&} Business Media},
title = {{Asymptotic Techniques for Use in Statistics.}},
volume = {47},
year = {1989}
}
@article{Hoeffding1952a,
author = {Hoeffding, Wassily},
doi = {10.1007/978-1-4612-0865-5_13},
file = {:Users/ekatsevi/papers/Annals of Mathematical Statistics/Hoeffding - 1952 - The Large-Sample Power of Tests Based on Permutations of Observations.pdf:pdf},
journal = {Annals of Mathematical Statistics},
number = {2},
pages = {169--192},
title = {{The Large-Sample Power of Tests Based on Permutations of Observations}},
volume = {23},
year = {1952}
}
@article{Hall1987,
abstract = {The asymptotic behaviour of the residual life time at time t is investigated (for t rightarrow infty). We derive weak limit laws and their domains of attraction and treat rates of convergence and moment convergence. The presentation exploits the close similarity with extreme value theory.},
author = {Hall, Peter},
file = {:Users/ekatsevi/papers/The Annals of Probability/Hall - 1987 - Edgeworth expansion for student's t statistic under minimal moment conditions.pdf:pdf},
issn = {0091-1798},
journal = {The Annals of Probability},
number = {3},
pages = {920--931},
title = {{Edgeworth expansion for student's t statistic under minimal moment conditions}},
url = {http://projecteuclid.org/euclid.aop/1176996548},
volume = {15},
year = {1987}
}
@article{Hinkley1988,
author = {Davison, Anthony C . and Hinkley, David V.},
file = {:Users/ekatsevi/papers/Biometrika/Davison, Hinkley - 1988 - Saddlepoint Approximations in Resampling Methods.pdf:pdf},
journal = {Biometrika},
number = {3},
pages = {417--431},
title = {{Saddlepoint Approximations in Resampling Methods}},
volume = {75},
year = {1988}
}
@article{Gandy2009,
abstract = {This paper introduces an open-ended sequential algorithm for computing the p-value of a test using Monte Carlo simulation. It guarantees that the resampling risk, the probability of a different decision than the one based on the theoretical p-value, is uniformly bounded by an arbitrarily small constant. Previously suggested sequential or nonsequential algorithms, using a bounded sample size, do not have this property. Although the algorithm is open-ended, the expected number of steps is finite, except when the p-value is on the threshold between rejecting and not rejecting. The algorithm is suitable as standard for implementing tests that require (re)sampling. It can also be used in other situations: to check whether a test is conservative, iteratively to implement double bootstrap tests, and to determine the sample size required for a certain power. An R-package implementing the sequential algorithm is available online. {\textcopyright} 2009 American Statistical Association.},
archivePrefix = {arXiv},
arxivId = {math/0612488},
author = {Gandy, Axel},
doi = {10.1198/JASA.2009.TM08368},
eprint = {0612488},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Gandy - 2009 - Sequential implementation of Monte Carlo tests with uniformly bounded resampling risk.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Monte carlo testing,P-value,Sequential estimation,Sequential test,Significance test},
month = {dec},
number = {488},
pages = {1504--1511},
primaryClass = {math},
publisher = {Axel Gandy},
title = {{Sequential implementation of Monte Carlo tests with uniformly bounded resampling risk}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20http://pubs.amstat.org/toc/jasa/104/488.},
volume = {104},
year = {2009}
}
@article{Goutis1999,
abstract = {Saddlepoint approximations are powerful tools for obtaining accurate expressions for densities and distribution functions. We give an elementary motivation and explanation of approximation techniques, starting with Taylor series expansions and progressing to the Laplace approximation of integrals. These approximations are illustrated with examples of the convolution of simple densities. We then turn to the saddlepoint approximation and, using both the Fourier inversion formula and Edgeworth expansions, we derive the saddlepoint approximation to the density of a single random variable. We next approximate the density of the sample mean of iid random variables, and also demonstrate the technique for approximating the density of a maximum likelihood estimator in exponential families.},
author = {Goutis, Constantino and Casella, George},
doi = {10.2307/2686100},
file = {:Users/ekatsevi/papers/The American Statistician/Goutis, Casella - 1999 - Explaining the Saddlepoint Approximation(2).pdf:pdf},
issn = {00031305},
journal = {The American Statistician},
keywords = {Laplace method, maximum likelihood estimators, mom,maximum likelihood estimators,mom},
number = {3},
pages = {216},
title = {{Explaining the Saddlepoint Approximation}},
volume = {53},
year = {1999}
}
@article{Bean2004,
abstract = {In the present paper, we consider four approximations to the null-distribution of the two-sample Wilcoxon-Mann-Whitney statistic, namely a normal, an Edgeworth and a saddlepoint approximation, as well as an approximation by the sum of independent uniform random variables. We make numerical comparisons of these approximations for moderate sample sizes, namely for m = 20 and 20 ≤ n ≤ 80. It turns out that the saddlepoint improves on the Edgeworth and the uniform approximations only very far in the tails, while the Edgeworth outperforms the other three for less extreme cases. We also discuss the practical importance of our results in the era of statistical packages.},
author = {Bean, Rapha{\"{e}}l and Froda, Sorana and {Van Eeden}, Constance},
doi = {10.1080/10485250310001622677},
file = {:Users/ekatsevi/papers/Journal of Nonparametric Statistics/Bean, Froda, Van Eeden - 2004 - The normal, Edgeworth, saddlepoint and uniform approximations to the Wilcoxon-Mann-Whitney null-distribu.pdf:pdf},
isbn = {1048525031},
issn = {10485252},
journal = {Journal of Nonparametric Statistics},
keywords = {Edgeworth,Saddlepoint and uniform approximations,Wilcoxon-Mann-Whitney statistic},
number = {1-2},
pages = {279--288},
title = {{The normal, Edgeworth, saddlepoint and uniform approximations to the Wilcoxon-Mann-Whitney null-distribution: A numerical comparison}},
volume = {16},
year = {2004}
}
@article{Townes2019,
abstract = {Single-cell RNA-Seq (scRNA-Seq) profiles gene expression of individual cells. Recent scRNA-Seq datasets have incorporated unique molecular identifiers (UMIs). Using negative controls, we show UMI counts follow multinomial sampling with no zero inflation. Current normalization procedures such as log of counts per million and feature selection by highly variable genes produce false variability in dimension reduction. We propose simple multinomial methods, including generalized principal component analysis (GLM-PCA) for non-normal distributions, and feature selection using deviance. These methods outperform the current practice in a downstream clustering assessment using ground truth datasets.},
author = {Townes, F. William and Hicks, Stephanie C. and Aryee, Martin J. and Irizarry, Rafael A.},
doi = {10.1186/s13059-019-1861-6},
file = {:Users/ekatsevi/papers/Genome Biology/Townes et al. - 2019 - Feature selection and dimension reduction for single-cell RNA-Seq based on a multinomial model.pdf:pdf;:Users/ekatsevi/papers/Genome Biology/Townes et al. - 2019 - Feature selection and dimension reduction for single-cell RNA-Seq based on a multinomial model.pdf:pdf},
issn = {1474760X},
journal = {Genome Biology},
keywords = {Dimension reduction,GLM-PCA,Gene expression,Principal component analysis,RNA-Seq,Single cell,Variable genes,single cell},
mendeley-tags = {single cell},
number = {1},
pages = {1--16},
publisher = {Genome Biology},
title = {{Feature selection and dimension reduction for single-cell RNA-Seq based on a multinomial model}},
volume = {20},
year = {2019}
}
@article{Gandy2017a,
abstract = {Multiple hypothesis testing is widely used to evaluate scientific studies involving statistical tests. However, for many of these tests, p-values are not available and are thus often approximated using Monte Carlo tests such as permutation tests or bootstrap tests. This article presents a simple algorithm based on Thompson Sampling to test multiple hypotheses. It works with arbitrary multiple testing procedures, in particular with step-up and step-down procedures. Its main feature is to sequentially allocate Monte Carlo effort, generating more Monte Carlo samples for tests whose decisions are so far less certain. A simulation study demonstrates that for a low computational effort, the new approach yields a higher power and a higher degree of reproducibility of its results than previously suggested methods.},
author = {Gandy, Axel and Hahn, Georg},
doi = {10.1007/s11222-016-9656-z},
file = {:Users/ekatsevi/papers/Statistics and Computing/Gandy, Hahn - 2017 - QuickMMCTest quick multiple Monte Carlo testing.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Benjamini–Hochberg procedure,Bonferroni correction,Monte Carlo,Multiple hypothesis testing,Multiple testing,Thompson sampling},
mendeley-tags = {Monte Carlo,Multiple testing},
number = {3},
pages = {823--832},
publisher = {Springer US},
title = {{QuickMMCTest: quick multiple Monte Carlo testing}},
volume = {27},
year = {2017}
}
@article{Chen2021,
abstract = {This paper gives a review of concentration inequalities which are widely employed in non-asymptotical analyses of mathematical statistics in a wide range of settings, from distribution-free to distribution-dependent, from sub-Gaussian to sub-exponential, sub-Gamma, and sub-Weibull random variables, and from the mean to the maximum concentration. This review provides results in these settings with some fresh new results. Given the increasing popularity of high-dimensional data and inference, results in the context of high-dimensional linear and Poisson regressions are also provided. We aim to illustrate the concentration inequalities with known constants and to improve existing bounds with sharper constants.},
archivePrefix = {arXiv},
arxivId = {2011.02258},
author = {Chen, Huiming Zhang {\&} Song Xi},
doi = {10.4208/cmr.2020-0041},
eprint = {2011.02258},
file = {::},
issn = {1674-5647},
journal = {Communications in Mathematical Research},
keywords = {60f10,60g50,62e17,ams subject classifications,butions,constants-specified inequalities,finite-sample theory,heavy-tailed distri-,high-dimensional estimation and testing,random matrices,sub-weibull random variables},
number = {1},
pages = {1--85},
title = {{Concentration Inequalities for Statistical Inference}},
volume = {37},
year = {2021}
}
@article{Bickel1974,
author = {Bickel, P. J.},
file = {:Users/ekatsevi/papers/Annals of Statistics/Bickel - 1974 - Edgeworth expansions in nonparametric statistics.pdf:pdf},
issn = {0091-1798},
journal = {Annals of Statistics},
number = {1},
pages = {1--20},
title = {{Edgeworth expansions in nonparametric statistics}},
url = {http://projecteuclid.org/euclid.aop/1176996548},
volume = {2},
year = {1974}
}
@article{Tuano2023,
abstract = {Genome-wide association studies (GWAS) have identified {\textgreater}200 loci associated with breast cancer (BC) risk. The majority of candidate causal variants (CCVs) are in non-coding regions and are likely to modulate cancer risk by regulating gene expression. We recently developed a scoring system, INQUISIT, to predict candidate risk genes at BC-risk loci. Here, we used pooled CRISPR activation and suppression screens to validate INQUISIT predictions, and to define the cancer phenotypes they mediate. We measured proliferation in 2D, 3D, and in immune-deficient mice, as well as the effect on the DNA damage response. We performed 60 CRISPR screens and identified 21 high-confidence INQUISIT predictions that mediate a cancer phenotype. We validated the direct regulation of a subset of genes by BC-risk variants using HiCHIP and CRISPRqtl. Furthermore, we show the utility of expression profiling for drug repurposing against these targets. We provide a platform for identifying gene targets of risk variants, and lay a blueprint of interventions for BC risk reduction and treatment. {\#}{\#}{\#} Competing Interest Statement The authors have declared no competing interest.},
author = {Tuano, Natasha K and Beesley, Jonathan and Manning, Murray and Shi, Wei and Malaver-Ortega, Luis F and Paynter, Jacob and Black, Debra and Civitarese, Andrew and McCue, Karen and Hatzipantelis, Aaron and Hillman, Kristine and Kaufmann, Susanne and Sivakumaran, Haran and Polo, Jose M and Reddel, Roger and Band, Vimla and French, Juliet D and Edwards, Stacey L and Powell, David and Chenevix-Trench, Georgia and Rosenbluh, Joseph},
doi = {10.1186/s13059-023-02898-w},
file = {:Users/ekatsevi/papers/Genome Biology/Tuano et al. - 2023 - CRISPR screens identify gene targets and drug repositioning opportunities at breast cancer risk loci.pdf:pdf},
issn = {1474-760X},
journal = {Genome Biology},
keywords = {Breast cancer risk,CRISPR,Functional phenotypic screens,Post GWAS,Target discovery},
mendeley-tags = {CRISPR},
publisher = {BioMed Central},
title = {{CRISPR screens identify gene targets and drug repositioning opportunities at breast cancer risk loci}},
url = {https://www.biorxiv.org/content/10.1101/2021.09.07.459221v1{\%}0Ahttps://www.biorxiv.org/content/10.1101/2021.09.07.459221v1.abstract},
year = {2023}
}
@article{Barber2022,
abstract = {Permutation tests are an immensely popular statistical tool, used for testing hypotheses of independence between variables and other common inferential questions. When the number of observations is large, it is computationally infeasible to consider every possible permutation of the data, and it is typical to either take a random draw of permutations, or to restrict to a subgroup or subset of permutations. In this work, we extend beyond these possibilities to show how such tests can be run using any distribution over any subset of permutations, with all the previous options as a special case.},
archivePrefix = {arXiv},
arxivId = {2204.13581},
author = {Barber, Rina Foygel and Candes, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
eprint = {2204.13581},
file = {:Users/ekatsevi/papers/Unknown/Barber et al. - 2022 - Generalized permutation tests.pdf:pdf},
keywords = {permutation test},
mendeley-tags = {permutation test},
pages = {1--19},
title = {{Generalized permutation tests}},
url = {http://arxiv.org/abs/2204.13581},
year = {2022}
}
@article{Canty1999,
abstract = {In many situations saddlepoint approximations can replace the Monte Carlo simulation typically used to find the bootstrap distribution of a statistic. We explain how bootstrap and permutation distributions can be expressed as conditional distributions and how methods for linear programming and for fitting generalized linear models can be used to find saddlepoint approximations to these distributions. The ideas are illustrated using an example from insurance.},
author = {Canty, Angelo J. and Davison, A. C.},
doi = {10.1023/A:1008801807768},
file = {:Users/ekatsevi/papers/Statistics and Computing/Canty, Davison - 1999 - Implementation of saddlepoint approximations in resampling problems.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Bootstrap,Exponential family,Generalized linear model,Saddlepoint approximation,Simplex method,Sparre Anderson model,Test inversion,saddlepoint approximation},
mendeley-tags = {saddlepoint approximation},
number = {1},
pages = {9--15},
title = {{Implementation of saddlepoint approximations in resampling problems}},
volume = {9},
year = {1999}
}
@article{Jaitin2016,
abstract = {In multicellular organisms, dedicated regulatory circuits control cell type diversity and responses. The crosstalk and redundancies within these circuits and substantial cellular heterogeneity pose a major research challenge. Here, we present CRISP-seq, an integrated method for massively parallel single-cell RNA sequencing (RNA-seq) and clustered regularly interspaced short palindromic repeats (CRISPR)-pooled screens. We show that profiling the genomic perturbation and transcriptome in the same cell enables us to simultaneously elucidate the function of multiple factors and their interactions. We applied CRISP-seq to probe regulatory circuits of innate immunity. By sampling tens of thousands of perturbed cells in vitro and in mice, we identified interactions and redundancies between developmental and signaling-dependent factors. These include opposing effects of Cebpb and Irf8 in regulating the monocyte/macrophage versus dendritic cell lineages and differential functions for Rela and Stat1/2 in monocyte versus dendritic cell responses to pathogens. This study establishes CRISP-seq as a broadly applicable, comprehensive, and unbiased approach for elucidating mammalian regulatory circuits.},
author = {Jaitin, Diego Adhemar and Weiner, Assaf and Yofe, Ido and Lara-Astiaso, David and Keren-Shaul, Hadas and David, Eyal and Salame, Tomer Meir and Tanay, Amos and van Oudenaarden, Alexander and Amit, Ido},
doi = {10.1016/j.cell.2016.11.039},
file = {:Users/ekatsevi/papers/Cell/Jaitin et al. - 2016 - Dissecting Immune Circuits by Linking CRISPR-Pooled Screens with Single-Cell RNA-Seq.pdf:pdf},
issn = {10974172},
journal = {Cell},
keywords = {CRISPR,CRISPR/Cas9,Kathryn,RNA-seq,functional genomics,gene networks,genetic screen,immune response,innate immunity,single-cell RNA-seq,transcriptomics},
mendeley-tags = {CRISPR,Kathryn},
number = {7},
pages = {1883--1896.e15},
publisher = {Elsevier Inc.},
title = {{Dissecting Immune Circuits by Linking CRISPR-Pooled Screens with Single-Cell RNA-Seq}},
volume = {167},
year = {2016}
}
@article{Chernozhukov2013,
author = {Chernozhukov, Victor and Chetverikov, Denis and Kato, Kengo},
doi = {10.1214/13-AOS1161},
file = {:Users/ekatsevi/papers/The Annals of Statistics/Chernozhukov, Chetverikov, Kato - 2013 - Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional rand(2).pdf:pdf;:Users/ekatsevi/papers/The Annals of Statistics/Chernozhukov, Chetverikov, Kato - 2013 - Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {asymptotics},
mendeley-tags = {asymptotics},
number = {6},
pages = {2786--2819},
title = {{Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors}},
volume = {41},
year = {2013}
}
@article{Easton1986,
abstract = {Saddlepoint approximations are extended to general statistics. The technique is applied to derive approximations to the density of linear combinations of order statistics, including trimmed means. A comparison with exact results shows the accuracy of these approximations even in very small sample sizes. {\textcopyright} 1976 Taylor {\&} Francis Group, LLC.},
author = {Easton, George S. and Ronchetti, Elvezio},
doi = {10.1080/01621459.1986.10478286},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Easton, Ronchetti - 1986 - General saddlepoint approximations with applications to l statistics.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Asymptotically most,Edgeworth expansions,Efficient L estimator,Small sample asymptotics,Trimmed mean},
number = {394},
pages = {420--430},
title = {{General saddlepoint approximations with applications to l statistics}},
volume = {81},
year = {1986}
}
@phdthesis{Basna2010,
author = {Basna, Rani},
file = {:Users/ekatsevi/papers/Unknown/Basna - 2010 - Edgeworth Expansion and Saddle Point Approximation for Discrete Data with Application to Chance Games .pdf:pdf},
title = {{Edgeworth Expansion and Saddle Point Approximation for Discrete Data with Application to Chance Games .}},
year = {2010}
}
@article{Robinson2003,
author = {Jin, Rungao and Robinson, John},
file = {:Users/ekatsevi/papers/Lecture Notes-Monograph Series/Jin, Robinson - 2003 - Saddlepoint Approximations of the Two-Sample Wilcoxon Statistic.pdf:pdf},
journal = {Lecture Notes-Monograph Series},
pages = {149--158},
title = {{Saddlepoint Approximations of the Two-Sample Wilcoxon Statistic}},
year = {2003}
}
@book{Schmock2000,
abstract = {The theory of large deviations deals with the evaluation, for a family of probability measures parameterized by a real valued variable, of the probabilities of events which decay exponentially in the parameter. Originally developed in the context of statistical mechanics and of (random) dynamical systems, it proved to be a powerful tool in the analysis of systems where the combined effects of random perturbations lead to a behavior significantly different from the noiseless case. The volume complements the central elements of this theory with selected applications in communication and control systems, bio-molecular sequence analysis, hypothesis testing problems in statistics, and the Gibbs conditioning principle in statistical mechanics.},
author = {Schmock, Uwe and Dembo, Amir and Zeitouni, Ofer},
booktitle = {Journal of the American Statistical Association},
doi = {10.2307/2669805},
file = {::},
isbn = {9783642033100},
issn = {01621459},
number = {452},
pages = {1380},
title = {{Large Deviations Techniques and Applications}},
volume = {95},
year = {2000}
}
@article{Efron1990,
abstract = {This article concerns computational methods for the bootstrap that are more efficient than the straightforward Monte Carlo methods usually used. The bootstrap is considered in its simplest form: In a one-sample nonparametric problem, where the goal is to estimate the bias or variance of some statistic by bootstrap sampling, or to set approximate confidence intervals for a parameter of interest in terms of various percentiles of the bootstrap distribution. The methods of this article can, in favorable situations, reduce the necessary number of bootstrap replications manyfold. Moreover, simple diagnostics are available to see whether or not any particular case is accessible to these methods. {\textcopyright} 1990 Taylor {\&} Francis Group, LLC.},
author = {Efron, Bradley},
doi = {10.1080/01621459.1990.10475309},
file = {::},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {409},
pages = {79--89},
title = {{More efficient bootstrap computations}},
volume = {85},
year = {1990}
}
@article{CetL16,
author = {Cand{\`{e}}s, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society Series B (Statistical Methodology)/Cand{\`{e}}s et al. - 2018 - Panning for gold `model-X' knockoffs for high dimensional controlled variable selection.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {FDR,Multiple testing,high-dimensional regression,knockoffs,model-X,variable selection},
mendeley-tags = {FDR,Multiple testing,high-dimensional regression,knockoffs,model-X,variable selection},
number = {3},
pages = {551--577},
publisher = {Wiley Online Library},
title = {{Panning for gold: `model-X' knockoffs for high dimensional controlled variable selection}},
volume = {80},
year = {2018}
}
@article{Huzurbazar1999,
author = {Huzurbazar, S},
file = {:Users/ekatsevi/papers/The American Statistician/Huzurbazar - 1999 - Practical Saddlepoint Approximations.pdf:pdf},
journal = {The American Statistician},
keywords = {convolution,cumulant generating func-,distribution of the,dlepoint approximation,finite mixture distribution,general shape of the,ier to gauge the,p value computations,sad-,tion},
number = {3},
pages = {225--232},
title = {{Practical Saddlepoint Approximations}},
volume = {53},
year = {1999}
}
@article{LaVecchia2022,
author = {{La Vecchia}, Davide and Ronchetti, Elvezio and Ilievski, Andrej},
doi = {10.1214/21-sts847},
file = {:Users/ekatsevi/papers/Statistical Science/La Vecchia, Ronchetti, Ilievski - 2022 - On Some Connections Between Esscher's Tilting, Saddlepoint Approximations, and Optimal Transp.pdf:pdf},
issn = {21688745},
journal = {Statistical Science},
keywords = {Change of variable, Kullback-Leibler divergence, g,and phrases,change of variable,geodesic,kullback,leibler divergence,optimal transportation map,wasserstein distance},
number = {1},
pages = {30--51},
title = {{On Some Connections Between Esscher's Tilting, Saddlepoint Approximations, and Optimal Transportation: A Statistical Perspective}},
volume = {38},
year = {2022}
}
@article{Niu2022a,
abstract = {Model-X approaches to testing conditional independence between a predictor and an outcome variable given a vector of covariates usually assume exact knowledge of the conditional distribution of the predictor given the covariates. Nevertheless, model-X methodologies are often deployed with this conditional distribution learned in sample. We investigate the consequences of this choice through the lens of the distilled conditional randomization test (dCRT). We find that Type-I error control is still possible, but only if the mean of the outcome variable given the covariates is estimated well enough. This demonstrates that the dCRT is doubly robust, and motivates a comparison to the generalized covariance measure (GCM) test, another doubly robust conditional independence test. We prove that these two tests are asymptotically equivalent, and show that the GCM test is optimal against (generalized) partially linear alternatives by leveraging semiparametric efficiency theory. In an extensive simulation study, we compare the dCRT to the GCM test. These two tests have broadly similar Type-I error and power, though dCRT can have somewhat better Type-I error control but somewhat worse power in small samples or when the response is discrete. We also find that post-lasso based test statistics (as compared to lasso based statistics) can dramatically improve Type-I error control for both methods.},
author = {Niu, Ziang and Chakraborty, Abhinav and Dukes, Oliver and Katsevich, Eugene},
file = {:Users/ekatsevi/papers/arXiv/Niu et al. - 2022 - Reconciling model-X and doubly robust approaches to conditional independence testing.pdf:pdf},
journal = {Annals of Statistics, to appear},
title = {{Reconciling model-X and doubly robust approaches to conditional independence testing}},
url = {https://arxiv.org/abs/2211.14698},
year = {2024}
}
@article{Rocke1993,
abstract = {One impediment to wider use of bootstrap methods is the large amount of computer time often required to compute bootstrap estimates. This paper shows how the saddlepoint approximation method can be used in certain situations to give very accurate parametric bootstrap statistics with a much smaller amount of calculation. Essentially, the problem of computing an n-dimentional integral (where n is the sample size) by Monte Carlo is reduced to an integral whose dimension is that of the parameter space, which is usually much smaller. This allows accurate numerical integration techniques to be used in place of simulation. The method should be especially effective for maximum likelihood in linear exponential family problems; it is illustrated by application to a problem in reliability. {\textcopyright} 1993.},
author = {Rocke, David M.},
doi = {10.1016/0167-9473(93)90176-T},
file = {:Users/ekatsevi/papers/Computational Statistics and Data Analysis/Rocke - 1993 - Almost-exact parametric bootstrap calculation via the saddlepoint approximation.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Gaussian quadrature,Maximum likelihood,Numerical integration,Reliability,Stress-strength problems},
number = {4},
pages = {451--460},
title = {{Almost-exact parametric bootstrap calculation via the saddlepoint approximation}},
volume = {15},
year = {1993}
}
@article{Murakami2015a,
abstract = {Calculating the sum of independent non-identically distributed random variables is necessary in the scientific field. Computing the probability of the corresponding significance point is important in cases that have a finite sum of random variables. However, it is difficult to evaluate this probability when the number of random variables increases. Under these circumstances, consideration of a more accurate approximation of the distribution function is extremely important. A saddlepoint approximation is performed using upper probabilities from the distribution of the sum of independent non-identically gamma random variables under finite sample sizes. In this study, we compared the results from a saddlepoint approximation to those from normal and moment-based approximations to identify the most appropriate method to use for the distribution function.},
author = {Murakami, H.},
doi = {10.1007/s40096-015-0169-2},
file = {:Users/ekatsevi/papers/Mathematical Sciences/Murakami - 2015 - Approximations to the distribution of sum of independent non-identically gamma random variables.pdf:pdf},
issn = {22517456},
journal = {Mathematical Sciences},
keywords = {Independent and non-identically distributed,Saddlepoint approximation,Sum of gamma random variables,saddlepoint approximation},
mendeley-tags = {saddlepoint approximation},
number = {4},
pages = {205--213},
publisher = {Springer Berlin Heidelberg},
title = {{Approximations to the distribution of sum of independent non-identically gamma random variables}},
volume = {9},
year = {2015}
}
@article{Ho1978,
abstract = {The asymptotic behaviour of the residual life time at time t is investigated (for t rightarrow infty). We derive weak limit laws and their domains of attraction and treat rates of convergence and moment convergence. The presentation exploits the close similarity with extreme value theory.},
author = {Ho, Soo-Thong and Chen, Louis H.Y.},
file = {:Users/ekatsevi/Library/Application Support/Mendeley Desktop/Downloaded/Ho, Chen - 1978 - An Lp bound for the remainder in a combinatorial central limit theorem.pdf:pdf},
issn = {0091-1798},
journal = {Annals of Probability},
number = {2},
pages = {231--249},
title = {{An Lp bound for the remainder in a combinatorial central limit theorem}},
url = {http://projecteuclid.org/euclid.aop/1176996548},
volume = {6},
year = {1978}
}
@article{Tang2020,
abstract = {We examine a higher order approximation to the significance function with increasing numbers of nuisance parameters, based on the normal approximation to an adjusted log-likelihood root. We show that the rate of the correction for nuisance parameters is larger than the correction for non-normality, when the parameter dimension p is O(n$\alpha$) for (Formula presented.). We specialize the results to linear exponential families and location–scale families and illustrate these with simulations.},
author = {Tang, Yanbo and Reid, Nancy},
doi = {10.1111/rssb.12389},
file = {::},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {High dimensional data,Higher order asymptotics,Modified likelihood root,Nuisance parameter,Statistical inference},
number = {5},
pages = {1349--1369},
title = {{Modified likelihood root in high dimensions}},
volume = {82},
year = {2020}
}
@inproceedings{Zhang2019,
abstract = {Monte Carlo (MC) permutation test is considered the gold standard for statistical hypothesis testing, especially when standard parametric assumptions are not clear or likely to fail. However, in modern data science settings where a large number of hypothesis tests need to be performed simultaneously , it is rarely used due to its prohibitive computational cost. In genome-wide association studies, for example, the number of hypothesis tests m is around 10 6 while the number of MC samples n for each test could be greater than 10 8 , totaling more than nm=10 14 samples. In this paper , we propose Adaptive MC multiple Testing (AMT) to estimate MC p-values and control false discovery rate in multiple testing. The algorithm outputs the same result as the standard full MC approach with high probability while requiring only˜Oonly˜ only˜O(√ nm) samples. This sample complexity is shown to be optimal. On a Parkinson GWAS dataset, the algorithm reduces the running time from 2 months for full MC to an hour. The AMT algorithm is derived based on the theory of multi-armed bandits.},
author = {Zhang, Martin J and Zou, James and Tse, David},
booktitle = {ICML},
file = {:Users/ekatsevi/papers/ICML/Zhang, Zou, Tse - 2019 - Adaptive Monte Carlo Multiple Testing via Multi-Armed Bandits.pdf:pdf},
keywords = {GWAS,Monte Carlo,Multiple hypothesis testing,Multiple testing,multi-armed bandits,permutation test},
mendeley-tags = {GWAS,Monte Carlo,Multiple testing,multi-armed bandits,permutation test},
title = {{Adaptive Monte Carlo Multiple Testing via Multi-Armed Bandits}},
year = {2019}
}
@article{Dey2017,
abstract = {The availability of electronic health record (EHR)-based phenotypes allows for genome-wide association analyses in thousands of traits and has great potential to enable identification of genetic variants associated with clinical phenotypes. We can interpret the phenome-wide association study (PheWAS) result for a single genetic variant by observing its association across a landscape of phenotypes. Because a PheWAS can test thousands of binary phenotypes, and most of them have unbalanced or often extremely unbalanced case-control ratios (1:10 or 1:600, respectively), existing methods cannot provide an accurate and scalable way to test for associations. Here, we propose a computationally fast score-test-based method that estimates the distribution of the test statistic by using the saddlepoint approximation. Our method is much (∼100 times) faster than the state-of-the-art Firth's test. It can also adjust for covariates and control type I error rates even when the case-control ratio is extremely unbalanced. Through application to PheWAS data from the Michigan Genomics Initiative, we show that the proposed method can control type I error rates while replicating previously known association signals even for traits with a very small number of cases and a large number of controls.},
author = {Dey, Rounak and Schmidt, Ellen M. and Abecasis, Goncalo R. and Lee, Seunggeun},
doi = {10.1016/j.ajhg.2017.05.014},
file = {:Users/ekatsevi/papers/American Journal of Human Genetics/Dey et al. - 2017 - A Fast and Accurate Algorithm to Test for Binary Phenotypes and Its Application to PheWAS.pdf:pdf},
issn = {15376605},
journal = {American Journal of Human Genetics},
keywords = {GWAS,PheWAS,rare variants,saddlepoint approximation,single-variant test,unbalanced case-control},
number = {1},
pages = {37--49},
pmid = {28602423},
publisher = {ElsevierCompany.},
title = {{A Fast and Accurate Algorithm to Test for Binary Phenotypes and Its Application to PheWAS}},
url = {http://dx.doi.org/10.1016/j.ajhg.2017.05.014},
volume = {101},
year = {2017}
}
@article{Zipunnikov2009,
abstract = {A double-saddlepoint approximation is proposed for the number of contingency tables with counts satisfying certain linear constraints. Computation of the approximation involves fitting a generalized linear model for geometric responses which can be accomplished almost instantaneously using the iterated weighted least squares algorithm. The approximation is far superior to other analytical approximations that have been proposed, and is shown to be highly accurate in a range of examples, including some for which analytical approximations were previously unavailable. A similar approximation is proposed for tables consisting of only zeros and ones based on a logistic regression model. A higher order adjustment to the basic double saddlepoint further improves the accuracy of the approximation in almost all cases. Computer code for implementing methods described in the article is provided as supplemental material. {\textcopyright} 2009 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
author = {Zipunnikov, Vadim and Booth, James G. and Yoshida, Ruriko},
doi = {10.1198/jcgs.2009.07154},
file = {:Users/ekatsevi/papers/Journal of Computational and Graphical Statistics/Zipunnikov, Booth, Yoshida - 2009 - Counting tables using the double-saddlepoint approximation.pdf:pdf},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Contingency tables,Darwin's finch data,Generalized linear model,Quasi-independence,Uniform association},
number = {4},
pages = {915--929},
title = {{Counting tables using the double-saddlepoint approximation}},
volume = {18},
year = {2009}
}
@article{Hothorn2006,
abstract = {Recursive binary partitioning is a popular tool for regression analysis. Two fundamental problems of exhaustive search procedures usually applied to fit such models have been known for a long time: overfitting and a selection bias towards covariates with many possible splits or missing values. While pruning procedures are able to solve the overfitting problem, the variable selection bias still seriously affects the interpretability of tree-structured regression models. For some special cases unbiased procedures have been suggested, however lacking a common theoretical foundation. We propose a unified framework for recursive partitioning which embeds tree-structured regression models into a well defined theory of conditional inference procedures. Stopping criteria based on multiple test procedures are implemented and it is shown that the predictive performance of the resulting trees is as good as the performance of established exhaustive search procedures. It turns out that the partitions and therefore the models induced by both approaches are structurally different, confirming the need for an unbiased variable selection. Moreover, it is shown thai the prediction accuracy of trees with early stopping is equivalent to the prediction accuracy of pruned trees with unbiased variable selection. The methodology presented here is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Data from studies on glaucoma classification, node positive breast cancer survival and mammography experience are re-analyzed. {\textcopyright} 2006 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
doi = {10.1198/106186006X133933},
file = {::},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Multiple testing,Multivariate regression trees,Ordinal regression trees,Permutation tests,Variable selection},
number = {3},
pages = {651--674},
title = {{Unbiased recursive partitioning: A conditional inference framework}},
volume = {15},
year = {2006}
}
@article{Andrews2000,
author = {Andrews, Donald W K and Buchinsky, Moshe},
file = {::},
journal = {Econometrica},
number = {1},
pages = {23--51},
title = {{A Three-Step Method for Choosing the Number of Bootstrap Repetitions}},
volume = {68},
year = {2000}
}
@article{Brazzale2008,
abstract = {We outline how modern likelihood theory, which provides essentially exact inferences in a variety of parametric statistical problems, may routinely be applied in practice. Although the likelihood procedures are based on analytical asymptotic approximations, the focus of this paper is not on the ory but on implementation and applications. Numerical illustrations are given for logistic regression, nonlinear models, and linear non-normal models, and we describe a sampling approach for the third of these classes. In the case of logistic regression, we argue that approximations are often more appropriate than 'exact' procedures, even when these exist. {\textcopyright} Institute of Mathematical Statistics , 2008.},
author = {Brazzale, Alessandra R. and Davison, Anthony C.},
doi = {10.1214/08-STS273},
file = {::},
issn = {08834237},
journal = {Statistical Science},
keywords = {Conditional inference,Heteroscedasticity,Logistic regression,Lugannani-rice formula,Markov chain monte carlo,Nonlinear model,R, regression-scale model,Saddlepoint approximation,Spline,Statistical computing},
number = {4},
pages = {465--484},
title = {{Accurate parametric inference for small samples}},
volume = {23},
year = {2008}
}
@article{Gandy2016,
abstract = {We are concerned with a situation in which we would like to test multiple hypotheses with tests whose p-values cannot be computed explicitly but can be approximated using Monte Carlo simulation. This scenario occurs widely in practice. We are interested in obtaining the same rejections and non-rejections as the ones obtained if the p-values for all hypotheses had been available. The present article introduces a framework for this scenario by providing a generic algorithm for a general multiple testing procedure. We establish conditions which guarantee that the rejections and non-rejections obtained through Monte Carlo simulations are identical to the ones obtained with the p-values. Our framework is applicable to a general class of step-up and step-down procedures which includes many established multiple testing corrections such as the ones of Bonferroni, Holm, Sidak, Hochberg or Benjamini-Hochberg. Moreover, we show how to use our framework to improve algorithms available in the literature in such a way as to yield theoretical guarantees on their results. These modifications can easily be implemented in practice and lead to a particular way of reporting multiple testing results as three sets together with an error bound on their correctness, demonstrated exemplarily using a real biological dataset.},
author = {Gandy, Axel and Hahn, Georg},
doi = {10.1111/sjos.12228},
file = {:Users/ekatsevi/papers/Scandinavian Journal of Statistics/Gandy, Hahn - 2016 - A Framework for Monte Carlo based Multiple Testing.pdf:pdf},
issn = {14679469},
journal = {Scandinavian Journal of Statistics},
keywords = {Monte Carlo,algorithm,framework,hypothesis testing,multiple testing,multiple testing procedure,p-value},
mendeley-tags = {Monte Carlo,multiple testing},
number = {4},
pages = {1046--1063},
title = {{A Framework for Monte Carlo based Multiple Testing}},
volume = {43},
year = {2016}
}
@article{Hecker2020,
abstract = {In the analysis of current life science datasets, we often encounter scenarios in which the application of asymptotic theory to hypothesis testing can be problematic. Besides improved asymptotic results, permutation/simulation-based tests are a general approach to address this issue. However, these randomized tests can impose a massive computational burden, for example, in scenarios in which large numbers of statistical tests are computed, and the specified significance level is very small. Stopping rules aim to assess significance with the smallest possible number of draws while controlling the probabilities of errors due to statistical uncertainty. In this communication, we derive a general stopping rule, QUICK-STOP, based on the sequential testing theory that is easy to implement, controls the error probabilities rigorously, and is nearly optimal in terms of expected draws. In a simulation study, we show that our approach outperforms current stopping approaches for general randomized tests by factor 10 and does not impose an additional computational burden. We illustrate our approach by applying our stopping rule to a single-variant analysis of a whole-genome sequencing study for lung function.},
author = {Hecker, Julian and Ruczinski, Ingo and Cho, Michael H. and Silverman, Edwin K. and Coull, Brent and Lange, Christoph},
doi = {10.1002/GEPI.22268},
file = {:Users/ekatsevi/papers/Genetic Epidemiology/Hecker et al. - 2020 - A flexible and nearly optimal sequential testing approach to randomized testing QUICK-STOP.pdf:pdf},
issn = {10982272},
journal = {Genetic Epidemiology},
keywords = {association p-value,next-generation sequencing,permutation,randomized test,sequential testing},
month = {mar},
number = {2},
pages = {139--147},
pmid = {31713269},
publisher = {Wiley-Liss Inc.},
title = {{A flexible and nearly optimal sequential testing approach to randomized testing: QUICK-STOP}},
volume = {44},
year = {2020}
}
@article{Morris2021d,
abstract = {The majority of variants associated with complex traits and common diseases identified by genome-wide association studies (GWAS) map to noncoding regions of the genome with unknown regulatory effects in cis and trans. By leveraging biobank-scale GWAS data, massively parallel CRISPR screens and single cell transcriptome sequencing, we discovered target genes of noncoding variants for blood trait loci. The closest gene was often the target gene, but this was not always the case. We also identified trans-effects networks of noncoding variants when cis target genes encoded transcription factors, such as GFI1B and NFE2. We observed that GFI1B trans-target genes were enriched for GFI1B binding sites and fine-mapped GWAS variants, and expressed in human bone marrow progenitor cells, suggesting that GFI1B acts as a master regulator of blood traits. This platform will enable massively parallel assays to catalog the target genes of human noncoding variants in both cis and trans.},
author = {Morris, John A and Daniloski, Zharko and Domingo, J{\'{u}}lia and Barry, Timothy and Ziosi, Marcello and Glinos, Dafni A and Hao, Stephanie and Mimitou, Eleni P and Smibert, Peter and Roeder, Kathryn and Katsevich, Eugene and Lappalainen, Tuuli and Sanjana, Neville E},
file = {:Users/ekatsevi/papers/Science/Morris et al. - 2023 - Discovery of target genes and pathways of blood trait loci using pooled CRISPR screens and single cell RNA seq(2).pdf:pdf},
journal = {Science},
keywords = {CRISPR,GWAS,single cell},
mendeley-tags = {CRISPR,GWAS,single cell},
title = {{Discovery of target genes and pathways of blood trait loci using pooled CRISPR screens and single cell RNA sequencing}},
year = {2023}
}
@article{Janssen2003,
abstract = {Resampling methods are frequently used in practice to adjust critical values of nonparametric tests. In the present paper a comprehensive and unified approach for the conditional and unconditional analysis of linear resampling statistics is presented. Under fairly mild assumptions we prove tightness and an asymptotic series representation for their weak accumulation points. From this series it becomes clear which part of the resampling statistic is responsible for asymptotic normality. The results leads to a discussion of the asymptotic correctness of resampling methods as well as their applications in testing hypotheses. They are conditionally correct iff a central limit theorem holds for the original test statistic. We prove unconditional correctness iff the central limit theorem holds or when symmetric random variables are resampled by a scheme of asymptotically random signs. Special cases are the m(n) out of k(n) bootstrap, the weighted bootstrap, the wild bootstrap and all kinds of permutation statistics. The program is carried out for convergent partial sums of rowwise independent infinitesimal triangular arrays in detail. These results are used to compare power functions of conditional resampling tests and their unconditional counterparts. The proof uses the method of random scores for permutation type statistics.},
author = {Janssen, Arnold and Pauls, Thorsten},
doi = {10.1214/aos/1056562462},
file = {:Users/ekatsevi/papers/Annals of Statistics/Janssen, Pauls - 2003 - How do bootstrap and permutation tests work.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bootstrap,Conditional tests,Exchangeable variables,Infinitely divisible laws,Permutation statistics,Random scores,Rank statistics,Resampling tests,Sample mean,Two-sample test,Weighted bootstrap,Wild bootstrap},
number = {3},
pages = {768--806},
title = {{How do bootstrap and permutation tests work?}},
volume = {31},
year = {2003}
}
@article{Svensson2020,
author = {Svensson, Valentine},
doi = {10.1038/s41587-020-0413-7},
file = {:Users/ekatsevi/papers/Nature Biotechnology/Svensson - 2020 - Droplet scRNA-seq is not zero-inflated.pdf:pdf},
issn = {15461696},
journal = {Nature Biotechnology},
keywords = {single cell},
mendeley-tags = {single cell},
pages = {142--150},
pmid = {32034392},
title = {{Droplet scRNA-seq is not zero-inflated}},
volume = {38},
year = {2020}
}
@article{Segal2018,
abstract = {Researchers in genetics and other life sciences commonly use permutation tests to evaluate differences between groups. Permutation tests have desirable properties, including exactness if data are exchangeable, and are applicable even when the distribution of the test statistic is analytically intractable. However, permutation tests can be computationally intensive. We propose both an asymptotic approximation and a resampling algorithm for quickly estimating small permutation p-values (e.g. {\$}{\textless}10{\^{}}{\{}-6{\}}{\$}) for the difference and ratio of means in two-sample tests. Our methods are based on the distribution of test statistics within and across partitions of the permutations, which we define. In this article, we present our methods and demonstrate their use through simulations and an application to cancer genomic data. Through simulations, we find that our resampling algorithm is more computationally efficient than another leading alternative, particularly for extremely small p-values (e.g. {\$}{\textless}10{\^{}}{\{}-30{\}}{\$}). Through application to cancer genomic data, we find that our methods can successfully identify up- and down-regulated genes. While we focus on the difference and ratio of means, we speculate that our approaches may work in other settings.},
author = {Segal, Brian D. and Braun, Thomas and Elliott, Michael R. and Jiang, Hui},
doi = {10.1111/biom.12731},
file = {:Users/ekatsevi/papers/Biometrics/Segal et al. - 2018 - Fast approximation of small p-values in permutation tests by partitioning the permutations.pdf:pdf},
issn = {15410420},
journal = {Biometrics},
keywords = {Computational efficiency,Genomics,Monte Carlo,Multiple hypothesis tests,Multiple testing,Resampling methods,Two-sample tests},
mendeley-tags = {Monte Carlo,Multiple testing},
number = {1},
pages = {196--206},
title = {{Fast approximation of small p-values in permutation tests by partitioning the permutations}},
volume = {74},
year = {2018}
}
@article{Kuonen1999,
author = {Kuonen, D},
file = {::},
number = {4},
pages = {929--935},
title = {{Saddlepoint Approximations for Distributions of Quadratic Forms in Normal Variables Author ( s ): D . Kuonen Published by : Oxford University Press on behalf of Biometrika Trust Stable URL : http://www.jstor.com/stable/2673596 Saddlepoint approximations f}},
volume = {86},
year = {1999}
}
@article{DiCiccio1992,
abstract = {Standard algorithms for the construction of iterated bootstrap confidence intervals are computationally very demanding, requiring nested levels of bootstrap resampling. We propose an alternative approach to constructing double bootstrap confidence intervals that involves replacing the inner level of resampling by an analytical approximation. This approximation is based on saddlepoint methods and a tail probability approximation of DiCiccio and Martin (1991). Our technique significantly reduces the computational expense of iterated bootstrap calculations. A formal algorithm for the construction of our approximate iterated bootstrap confidence intervals is presented, and some crucial practical issues arising in its implementation are discussed. Our procedure is illustrated in the case of constructing confidence intervals for ratios of means using both real and simulated data. We repeat an experiment of Schenker (1985) involving the construction of bootstrap confidence intervals for a variance and demonstrate that our technique makes feasible the construction of accurate bootstrap confidence intervals in that context. Finally, we investigate the use of our technique in a more complex setting, that of constructing confidence intervals for a correlation coefficient. {\textcopyright} 1992 Chapman {\&} Hall.},
author = {DiCiccio, Thomas J. and Martin, Michael A. and Young, G. Alastair},
doi = {10.1007/BF01891208},
file = {:Users/ekatsevi/papers/Statistics and Computing/DiCiccio, Martin, Young - 1992 - Analytical approximations for iterated bootstrap confidence intervals.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Asymptotic approximations,bootstrap calibration,coverage accuracy,resampling,saddlepoint methods,tail probability approximations},
number = {3},
pages = {161--171},
title = {{Analytical approximations for iterated bootstrap confidence intervals}},
volume = {2},
year = {1992}
}
@article{Daniels1954,
abstract = {It is often required to approximate to the distribution of some statistic whose exact distribution cannot be conveniently obtained. When the first few moments are known, a common procedure is to fit a law of the Pearson or Edgeworth type having the same moments as far as they are given. Both these methods are often satisfactory in practice, but have the drawback that errors in the "tail" regions of the distribution are sometimes comparable with the frequencies themselves. The Edgeworth approximation in particular notoriously can assume negative values in such regions. The characteristic function of the statistic may be known, and the difficulty is then the analytical one of inverting a Fourier transform explicitly. In this paper we show that for a statistic such as the mean of a sample of size n, or the ratio of two such means, a satisfactory approximation to its probability density, when it exists, can be obtained nearly always by the method of steepest descents. This gives an asymptotic expansion in powers of n-1 whose dominant term, called the saddlepoint approximation, has a number of desirable features. The error incurred by its use is O(n-1) as against the more usual O(n-1/2) associated with the normal approximation. Moreover it is shown that in an important class of cases the relative error of the approximation is uniformly O(n-1) over the whole admissible range of the variable. The method of steepest descents was first used systematically by Debye for Bessel functions of large order (Watson [17]) and was introduced by Darwin and Fowler (Fowler [9]) into statistical mechanics, where it has remained an indispensable tool. Apart from the work of Jeffreys [12] and occasional isolated applications by other writers (e.g. Cox [2]), the technique has been largely ignored by writers on statistical theory. In the present paper, distributions having probability densities are discussed first, the saddlepoint approximation and its associated asymptotic expansion being obtained for the probability density of the mean x̄ of a sample of n. It is shown how the steepest descents technique is related to an alternative method used by Khinchin [14] and, in a slightly different context, by Cramer [5]. General conditions are established under which the relative error of the saddlepoint approximation is O(n-1) uniformly for all admissible x̄, with a corresponding result for the asymptotic expansion. The case of discrete variables is briefly discussed, and finally the method is used for approximating to the distribution of ratios.},
author = {Daniels, Henry E.},
doi = {10.1214/aoms/1177728652},
file = {:Users/ekatsevi/papers/The Annals of Mathematical Statistics/Daniels - 1954 - Saddlepoint Approximations in Statistics.pdf:pdf},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
keywords = {saddlepoint approximation},
mendeley-tags = {saddlepoint approximation},
number = {4},
pages = {631--650},
title = {{Saddlepoint Approximations in Statistics}},
volume = {25},
year = {1954}
}
@article{Hemerik2019a,
abstract = {Generalized linear models are often misspecified due to overdispersion, heteroscedasticity and ignored nuisance variables. Existing quasi-likelihood methods for testing in misspecified models often do not provide satisfactory type-I error rate control. We provide a novel semi-parametric test, based on sign-flipping individual score contributions. The tested parameter is allowed to be multi-dimensional and even high-dimensional. Our test is often robust against the mentioned forms of misspecification and provides better type-I error control than its competitors. When nuisance parameters are estimated, our basic test becomes conservative. We show how to take nuisance estimation into account to obtain an asymptotically exact test. Our proposed test is asymptotically equivalent to its parametric counterpart.},
author = {Hemerik, Jesse and Goeman, Jelle J and Finos, Livio},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society, Series B/Hemerik, Goeman, Finos - 2020 - Robust testing in generalized linear models by sign-flipping score contributions.pdf:pdf},
journal = {Journal of the Royal Statistical Society, Series B},
keywords = {bootstrap,generalized linear models,glm,high-dimensional,high-dimensional regression,permutation,robust,score,semi-parametric,sign-flipping,test},
mendeley-tags = {bootstrap,generalized linear models,high-dimensional regression},
number = {3},
pages = {841--864},
title = {{Robust testing in generalized linear models by sign-flipping score contributions}},
volume = {82},
year = {2020}
}
@article{Touchette2011,
abstract = {The theory of large deviations deals with the probabilities of rare events (or fluctuations) that are exponentially small as a function of some parameter, e.g., the number of random components of a system, the time over which a stochastic system is observed, the amplitude of the noise perturbing a dynamical system or the temperature of a chemical reaction. The theory has applications in many different scientific fields, ranging from queuing theory to statistics and from finance to engineering. It is also increasingly used in statistical physics for studying both equilibrium and nonequilibrium systems. In this context, deep analogies can be made between familiar concepts of statistical physics, such as the entropy and the free energy, and concepts of large deviation theory having more technical names, such as the rate function and the scaled cumulant generating function. The first part of these notes introduces the basic elements of large deviation theory at a level appropriate for advanced undergraduate and graduate students in physics, engineering, chemistry, and mathematics. The focus there is on the simple but powerful ideas behind large deviation theory, stated in non-technical terms, and on the application of these ideas in simple stochastic processes, such as sums of independent and identically distributed random variables and Markov processes. Some physical applications of these processes are covered in exercises contained at the end of each section. In the second part, the problem of numerically evaluating large deviation probabilities is treated at a basic level. The fundamental idea of importance sampling is introduced there together with its sister idea, the exponential change of measure. Other numerical methods based on sample means and generating functions, with applications to Markov processes, are also covered.},
archivePrefix = {arXiv},
arxivId = {1106.4146},
author = {Touchette, Hugo},
eprint = {1106.4146},
file = {:Users/ekatsevi/papers/Unknown/Touchette - 2011 - A basic introduction to large deviations Theory, applications, simulations.pdf:pdf},
pages = {1--58},
title = {{A basic introduction to large deviations: Theory, applications, simulations}},
url = {http://arxiv.org/abs/1106.4146},
year = {2011}
}
@article{Taleb2018,
abstract = {"The book investigates the misapplication of conventional statistical techniques to fat tailed distributions and looks for remedies, when possible. Switching from thin tailed to fat tailed distributions requires more than "changing the color of the dress." Traditional asymptotics deal mainly with either n=1 or n=∞, and the real world is in between, under the "laws of the medium numbers"-which vary widely across specific distributions. Both the law of large numbers and the generalized central limit mechanisms operate in highly idiosyncratic ways outside the standard Gaussian or Levy-Stable basins of convergence. A few examples: - The sample mean is rarely in line with the population mean, with effect on "naïve empiricism," but can be sometimes be estimated via parametric methods. - The "empirical distribution" is rarely empirical. - Parameter uncertainty has compounding effects on statistical metrics. - Dimension reduction (principal components) fails. - Inequality estimators (Gini or quantile contributions) are not additive and produce wrong results. - Many "biases" found in psychology become entirely rational under more sophisticated probability distributions. - Most of the failures of financial economics, econometrics, and behavioral economics can be attributed to using the wrong distributions"--Back cover.},
author = {Taleb, Nassim Nicholas},
file = {:Users/ekatsevi/papers/Unknown/Taleb - 2018 - Statistical Consequences Of Fat Tails.pdf:pdf},
isbn = {9781544508054},
pages = {441},
title = {{Statistical Consequences Of Fat Tails}},
year = {2018}
}
@article{Diciccio1992,
author = {DiCiccio, B Y Thomas J and Martin, Michael A},
file = {::},
journal = {Biometrika},
number = {2},
pages = {285--295},
title = {{Fast and Accurate Approximate Double Bootstrap Confidence Intervals}},
volume = {79},
year = {1992}
}
@article{Reid1988,
abstract = {The history, empirical evidence and classical explanations of the significant-digit (or Benford's) law are reviewed, followed by a summary of recent invariant-measure characterizations. Then a new statistical derivation of the law in the form of a CLT-like theorem for significant digits is presented. If distributions are selected at random (in any "unbiased" way) and random samples are then taken from each of these distributions, the significant digits of the combined sample will converge to the logarithmic (Benford) distribution. This helps explain and predict the appearance of the significant-digit phenomenon in many different emprical contexts and helps justify its recent application to computer design, mathematical modelling and detection of fraud in accounting data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Reid, N.},
doi = {10.2307/2246134},
eprint = {arXiv:1011.1669v3},
file = {:Users/ekatsevi/papers/Statistical Science/Reid - 1988 - Saddlepoint methods and statistical inference.pdf:pdf},
isbn = {0883-4237},
issn = {2168-8745},
journal = {Statistical Science},
number = {2},
pages = {213--238},
pmid = {20948974},
title = {{Saddlepoint methods and statistical inference}},
volume = {3},
year = {1988}
}
@book{Durrett2010,
abstract = {This lively introduction to measure-theoretic probability theory covers laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. Concentrating on results that are the most useful for applications, this comprehensive treatment is a rigorous graduate text and reference. Operating under the philosophy that the best way to learn probability is to see it in action, the book contains extended examples that apply the theory to concrete applications. This fifth edition contains a new chapter on multidimensional Brownian motion and its relationship to partial differential equations (PDEs), an advanced topic that is finding new applications. Setting the foundation for this expansion, Chapter 7 now features a proof of It{\^{o}}'s formula. Key exercises that previously were simply proofs left to the reader have been directly inserted into the text as lemmas. The new edition re-instates discussion about the central limit theorem for martingales and stationary sequences.},
author = {Durrett, Rick},
booktitle = {Probability: Theory and Examples},
doi = {10.1017/9781108591034},
edition = {4th},
file = {:Users/ekatsevi/papers/Probability Theory and Examples/Durrett - 2010 - Probability Theory and Examples.pdf:pdf},
isbn = {9781108591034},
publisher = {Cambridge University Press},
title = {{Probability: Theory and Examples}},
year = {2010}
}
@article{Daniels1987,
author = {Daniels, Henry E.},
doi = {10.1080/01621459.1926.10502160},
file = {:Users/ekatsevi/papers/International Statistical Review/Daniels - 1987 - Tail Probability Approximation.pdf:pdf},
issn = {1537274X},
journal = {International Statistical Review},
keywords = {asymptotic expansion,saddlepoint approximation,significance test,uniform relative},
number = {1},
pages = {37--48},
title = {{Tail Probability Approximation}},
volume = {55},
year = {1987}
}
@article{Nagaraja1982,
author = {Nagaraja, H. N.},
file = {:Users/ekatsevi/papers/Annals of Statistics/Nagaraja - 1982 - Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Anna.pdf:pdf},
journal = {Annals of Statistics},
number = {4},
pages = {1306--1310},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Statistics. {\textregistered} www.jstor.org}},
volume = {10},
year = {1982}
}
@techreport{Asymp-2013,
author = {Wood, Andrew},
file = {:Users/ekatsevi/papers/Unknown/Wood - 2013 - Statistical asymptotics.pdf:pdf},
title = {{Statistical asymptotics}},
year = {2013}
}
@article{Koning2022,
abstract = {Non-parametric tests based on permutation, rotation or sign-flipping are examples of group-invariance tests. These tests test invariance of the null distribution under a set of transformations that has a group structure, in the algebraic sense. Such groups are often huge, which makes it computationally infeasible to test using the entire group. Hence, it is standard practice to test using a randomly sampled set of transformations from the group. This random sample still needs to be substantial to obtain good power and replicability. We improve upon this standard practice by using a well-designed subgroup of transformations instead of a random sample. The resulting subgroup-invariance test is still exact, as invariance under a group implies invariance under its subgroups. We illustrate this in a generalized location model and obtain more powerful tests based on the same number of transformations. In particular, we show that a subgroup-invariance test is consistent for lower signal-to-noise ratios than a test based on a random sample. For the special case of a normal location model and a particular design of the subgroup, we show that the power improvement is equivalent to the power difference between a Monte Carlo {\$}Z{\$}-test and a Monte Carlo {\$}t{\$}-test.},
archivePrefix = {arXiv},
arxivId = {2202.00967},
author = {Koning, Nick W. and Hemerik, Jesse},
eprint = {2202.00967},
file = {::},
keywords = {group-invariance test,permutation test,randomization test,resampling,subgroup},
pages = {1--35},
title = {{More Efficient Exact Group-Invariance Testing: using a Representative Subgroup}},
url = {http://arxiv.org/abs/2202.00967},
year = {2022}
}
@article{Diciccio1991a,
abstract = {SUMMARY: This paper presents an asymptotic approximation of marginal tail probabilities for a real-valued function of a random vector, where the function has continuous gradient that does not vanish at the mode of the joint density of the random vector. This approximation has error O(n-3/2) and improves upon a related standard normal approximation which has error O(n-2-Jan). Derivation involves the application of a tail probability formula given by DiCiccio, Field {\&} Fraser (1990) to an approximation of a marginal density derived by Tierney, Kass {\&} Kadane (1989). The approximation can be applied for Bayesian and conditional inference as well as for approximating sampling distributions, and the accuracy of the approximation is illustrated through several numerical examples related to such applications. In the context of conditional inference, we develop refinements of the standard normal approximation to the distribution of two different signed root likelihood ratio statistics for a component of the natural parameter in exponential families. {\textcopyright} 1991 Biometrika Trust.},
author = {DiCiccio, Thomas J. and Martin, Michael A.},
doi = {10.1093/biomet/78.4.891},
file = {:Users/ekatsevi/papers/Biometrika/Diciccio, Martin - 1991 - Approximations of marginal tail probabilities for a class of smooth functions with applications to Bayesian an.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Asymptotic expansion,Conditional likelihood,Confidence limit,Exponential family,Exponential regression model,Marginal posterior distribution function,Natural parameter,Normal approximation,Signed root likelihood ratio statistic},
number = {4},
pages = {891--902},
title = {{Approximations of marginal tail probabilities for a class of smooth functions with applications to Bayesian and conditional inference}},
volume = {78},
year = {1991}
}
@article{Butler1990,
author = {Booth, James G . and Butler, Ronald W.},
file = {:Users/ekatsevi/papers/Biometrika/Booth, Butler - 1990 - Randomization Distributions and Saddlepoint Approximations in Generalized Linear Models.pdf:pdf},
journal = {Biometrika},
number = {4},
pages = {787--796},
title = {{Randomization Distributions and Saddlepoint Approximations in Generalized Linear Models}},
volume = {77},
year = {1990}
}
@article{Rosenbaum2002,
abstract = {By slightly reframing the concept of covariance adjustment in randomized experiments, a method of exact permutation inference is derived that is entirely free of distributional assumptions and uses the random assignment of treatments as the "reasoned basis for inference." This method of exact permutation inference may be used with many forms of covariance adjustment, including robust regression and locally weighted smoothers. The method is then generalized to observational studies where treatments were not randomly assigned, so that sensitivity to hidden biases must be examined. Adjustments using an instrumental variable are also discussed. The methods are illustrated using data from two observational studies.},
author = {Rosenbaum, Paul R.},
doi = {10.1214/ss/1042727942},
file = {:Users/ekatsevi/papers/Statistical Science/Rosenbaum - 2002 - Covariance adjustment in randomized experiments and observational studies.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Covariance adjustment,Matching,Observational studies,Permutation inference,Propensity score,Randomization inference,Sensitivity analysis},
number = {3},
pages = {286--327},
title = {{Covariance adjustment in randomized experiments and observational studies}},
volume = {17},
year = {2002}
}
@article{Kolassa2010,
abstract = {We extend known saddlepoint tail probability approximations to multivariate cases, including multivariate conditional cases. Our approximation applies to both continuous and lattice variables, and requires the existence of a cumulant generating function. The method is applied to some examples, including a real data set from a case-control study of endometrial cancer. The method contains less terms and is easier to implement than existing methods, while showing an accuracy comparable to those methods.},
author = {Kolassa, John and Li, Jixin},
doi = {10.3150/09-BEJ237},
file = {:Users/ekatsevi/papers/Bernoulli/Kolassa, Li - 2010 - Multivariate saddlepoint approximations in tail probability and conditional inference.pdf:pdf},
issn = {13507265},
journal = {Bernoulli},
keywords = {Conditional probability,Saddlepoint approximation,Tail probability,Watson's lemma},
number = {4},
pages = {1191--1207},
title = {{Multivariate saddlepoint approximations in tail probability and conditional inference}},
volume = {16},
year = {2010}
}
@article{Albers1976,
author = {Albers, W. and Bickel, P. J. and {Van Zwet}, W. R.},
file = {:Users/ekatsevi/papers/Annals of Statistics/Albers, Bickel, Van Zwet - 1976 - Asymptotic expansions for the power of distribution free tests in the one-sample problem.pdf:pdf},
journal = {Annals of Statistics},
title = {{Asymptotic expansions for the power of distribution free tests in the one-sample problem}},
volume = {4},
year = {1976}
}
@article{Fellingham1964,
author = {Fellingham, S . A. and Stoker, D. J.},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Fellingham, Stoker - 1964 - An Approximation for the Exact Distribution of the Wilcoxon Test for Symmetry.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {307},
pages = {899--905},
title = {{An Approximation for the Exact Distribution of the Wilcoxon Test for Symmetry}},
volume = {59},
year = {1964}
}
@article{Temme1982,
author = {Temme, N. M.},
file = {:Users/ekatsevi/papers/Siam J. Math. Anal/Temme - 1982 - The uniform asymptotic expansion of a class of integrals related to the cumulative distribution functions.pdf:pdf},
journal = {Siam J. Math. Anal.},
number = {2},
title = {{The uniform asymptotic expansion of a class of integrals related to the cumulative distribution functions}},
volume = {13},
year = {1982}
}
@article{Feuerverger1989,
abstract = {The properties of the saddlepoint approximation are investigated when the required cumulant generating function is obtained empirically. Properties of the empirical moment generating function and empirical cumulant generating function and derivatives of these processes which are needed for this study are derived first, in particular their uniform consistency, moment structure, and weak convergence to normality are established. A numerical investigation exploring use of the empirical saddlepoint approximation as a tool in density estimation is discussed briefly. {\textcopyright} 1989 Biometrika Trust.},
author = {Feuerverger, Andrey},
doi = {10.1093/biomet/76.3.457},
file = {:Users/ekatsevi/papers/Biometrika/Feuerverger - 1989 - On the empirical saddlepoint approximation.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Convergence,Density estimation,Empirical transform,Saddlepoint approximation,Stable law},
number = {3},
pages = {457--464},
title = {{On the empirical saddlepoint approximation}},
volume = {76},
year = {1989}
}
@article{Yu2011,
author = {Yu, Kai and Liang, Faming and Ciampa, Julia and Chatterjee, Nilanjan},
doi = {10.1093/biostatistics/kxq078},
file = {:Users/ekatsevi/papers/Biostatistics/Yu et al. - 2011 - Efficient p -value evaluation for resampling-based tests.pdf:pdf},
journal = {Biostatistics},
keywords = {bootstrap procedures,carlo,genetic association studies,mation markov chain monte,p -value,resampling-based tests,stochastic approxi-},
number = {3},
pages = {582--593},
title = {{Efficient p -value evaluation for resampling-based tests}},
volume = {12},
year = {2011}
}
@article{JingRobinson1994,
author = {Jing, Bing Yi and Robinson, John},
file = {:Users/ekatsevi/papers/Annals of Statistics/Jing, Robinson - 1994 - Saddlepoint Approximations for Marginal and Conditional Probabilities of Transformed Variables.pdf:pdf},
journal = {Annals of Statistics},
number = {3},
pages = {1115--1132},
title = {{Saddlepoint Approximations for Marginal and Conditional Probabilities of Transformed Variables}},
volume = {22},
year = {1994}
}
@article{Dixit2016,
abstract = {Genetic screens help infer gene function in mammalian cells, but it has remained difficult to assay complex phenotypes—such as transcriptional profiles—at scale. Here, we develop Perturb-seq, combining single-cell RNA sequencing (RNA-seq) and clustered regularly interspaced short palindromic repeats (CRISPR)-based perturbations to perform many such assays in a pool. We demonstrate Perturb-seq by analyzing 200,000 cells in immune cells and cell lines, focusing on transcription factors regulating the response of dendritic cells to lipopolysaccharide (LPS). Perturb-seq accurately identifies individual gene targets, gene signatures, and cell states affected by individual perturbations and their genetic interactions. We posit new functions for regulators of differentiation, the anti-viral response, and mitochondrial function during immune activation. By decomposing many high content measurements into the effects of perturbations, their interactions, and diverse cell metadata, Perturb-seq dramatically increases the scope of pooled genomic assays.},
author = {Dixit, Atray and Parnas, Oren and Li, Biyu and Chen, Jenny and Fulco, Charles P. and Jerby-Arnon, Livnat and Marjanovic, Nemanja D. and Dionne, Danielle and Burks, Tyler and Raychowdhury, Raktima and Adamson, Britt and Norman, Thomas M. and Lander, Eric S. and Weissman, Jonathan S. and Friedman, Nir and Regev, Aviv},
doi = {10.1016/j.cell.2016.11.038},
file = {:Users/ekatsevi/papers/Cell/Dixit et al. - 2016 - Perturb-Seq Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens.pdf:pdf},
issn = {10974172},
journal = {Cell},
keywords = {CRISPR,epistasis,genetic interactions,genetics,pooled screen,single-cell RNA-seq,to-read},
mendeley-tags = {CRISPR,genetics,to-read},
month = {dec},
pages = {1853--1866},
publisher = {Cell Press},
title = {{Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens}},
volume = {167},
year = {2016}
}
@article{Jiang2022,
abstract = {Estimation of the average treatment effect (ATE) is a central problem in causal inference. In recent times, inference for the ATE in the presence of high-dimensional covariates has been extensively studied. Among the diverse approaches that have been proposed, augmented inverse probability weighting (AIPW) with cross-fitting has emerged as a popular choice in practice. In this work, we study this cross-fit AIPW estimator under well-specified outcome regression and propensity score models in a high-dimensional regime where the number of features and samples are both large and comparable. Under assumptions on the covariate distribution, we establish a new CLT for the suitably scaled cross-fit AIPW that applies without any sparsity assumptions on the underlying high-dimensional parameters. Our CLT uncovers two crucial phenomena among others: (i) the AIPW exhibits a substantial variance inflation that can be precisely quantified in terms of the signal-to-noise ratio and other problem parameters, (ii) the asymptotic covariance between the pre-cross-fit estimates is non-negligible even on the root-n scale. In fact, these cross-covariances turn out to be negative in our setting. These findings are strikingly different from their classical counterparts. On the technical front, our work utilizes a novel interplay between three distinct tools--approximate message passing theory, the theory of deterministic equivalents, and the leave-one-out approach. We believe our proof techniques should be useful for analyzing other two-stage estimators in this high-dimensional regime. Finally, we complement our theoretical results with simulations that demonstrate both the finite sample efficacy of our CLT and its robustness to our assumptions.},
archivePrefix = {arXiv},
arxivId = {2205.10198},
author = {Jiang, Kuanhao and Mukherjee, Rajarshi and Sen, Subhabrata and Sur, Pragya},
eprint = {2205.10198},
file = {:Users/ekatsevi/papers/Unknown/Jiang et al. - 2022 - A New Central Limit Theorem for the Augmented IPW Estimator Variance Inflation, Cross-Fit Covariance and Beyond.pdf:pdf},
title = {{A New Central Limit Theorem for the Augmented IPW Estimator: Variance Inflation, Cross-Fit Covariance and Beyond}},
url = {http://arxiv.org/abs/2205.10198},
year = {2022}
}
@article{Kolassa2003,
author = {Kolassa, John E},
file = {::},
journal = {Annals of Statistics},
number = {1},
pages = {274--286},
title = {{Multivariate Saddlepoint Tail Probability Approximations}},
volume = {31},
year = {2003}
}
@article{Basse2019,
author = {Basse, G. W. and Feller, A. and Toulis, P.},
doi = {10.1093/biomet/asy072},
file = {:Users/ekatsevi/papers/Biometrika/Basse, Feller, Toulis - 2019 - Randomization tests of causal effects under interference.pdf:pdf},
journal = {Biometrika},
keywords = {causality,interference,randomization},
mendeley-tags = {causality,interference,randomization},
number = {2},
pages = {487--494},
title = {{Randomization tests of causal effects under interference}},
volume = {106},
year = {2019}
}
@article{Froda2000,
author = {Froda, Sorana and van Eeden, Constance},
doi = {10.2307/3315887},
file = {:Users/ekatsevi/papers/The Canadian Journal of Statistics La Revue Canadienne de Statistique/Froda, van Eeden - 2000 - A Uniform Saddlepoint Expansion for the Null-Distribution of the Wilcoxon-Mann-Whitney Statistic.pdf:pdf},
issn = {03195724},
journal = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
keywords = {and phrases,asymptotic expansions,rates of convergence,wilcoxon-mann- whitney},
number = {1},
pages = {137},
title = {{A Uniform Saddlepoint Expansion for the Null-Distribution of the Wilcoxon-Mann-Whitney Statistic}},
volume = {28},
year = {2000}
}
@article{Ma2013,
abstract = {In genome-wide association studies of binary traits, investigators typically use logistic regression to test common variants for disease association within studies, and combine association results across studies using meta-analysis. For common variants, logistic regression tests are well calibrated, and meta-analysis of study-specific association results is only slightly less powerful than joint analysis of the combined individual-level data. In recent sequencing and dense chip based association studies, investigators increasingly test low-frequency variants for disease association. In this paper, we seek to (1) identify the association test with maximal power among tests with well controlled type I error rate and (2) compare the relative power of joint and meta-analysis tests. We use analytic calculation and simulation to compare the empirical type I error rate and power of four logistic regression based tests: Wald, score, likelihood ratio, and Firth bias-corrected. We demonstrate for low-count variants (roughly minor allele count [MAC] {\textless} 400) that: (1) for joint analysis, the Firth test has the best combination of type I error and power; (2) for meta-analysis of balanced studies (equal numbers of cases and controls), the score test is best, but is less powerful than Firth test based joint analysis; and (3) for meta-analysis of sufficiently unbalanced studies, all four tests can be anti-conservative, particularly the score test. We also establish MAC as the key parameter determining test calibration for joint and meta-analysis. {\textcopyright} 2013 WILEY PERIODICALS, INC.},
author = {Ma, Clement and Blackwell, Tom and Boehnke, Michael and Scott, Laura J.},
doi = {10.1002/gepi.21742},
file = {:Users/ekatsevi/papers/Genetic Epidemiology/Ma et al. - 2013 - Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants.pdf:pdf},
issn = {07410395},
journal = {Genetic Epidemiology},
keywords = {Joint analysis,Low-frequency variants,Meta-analysis,Single nucleotide polymorphisms,Single variant tests},
month = {sep},
number = {6},
pages = {539--550},
pmid = {23788246},
title = {{Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants}},
volume = {37},
year = {2013}
}
@article{Pitman1937a,
author = {Pitman, E. J. G.},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society/Pitman - 1937 - Significance Tests Which May be Applied to Samples From any Populations.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
number = {1},
pages = {119--130},
title = {{Significance Tests Which May be Applied to Samples From any Populations}},
volume = {4},
year = {1937}
}
@article{Zholud2014,
abstract = {Let T be the Student one- or two-sample t-, F-, or Welch statistic. Now release the underlying assumptions of normality, independence and identical distribution and consider a more general case where one only assumes that the vector of data has a continuous joint density. We determine asymptotic expressions for P(T {\textgreater} u) as u → ∞ for this case. The approximations are particularly accurate for small sample sizes and may be used, for example, in the analysis of High-Throughput Screening experiments, where the number of replicates can be as low as two to five and often extreme significance levels are used. We give numerous examples and complement our results by an investigation of the convergence speed - both theoretically, by deriving exact bounds for absolute and relative errors, and by means of a simulation study.},
archivePrefix = {arXiv},
arxivId = {1410.6033},
author = {Zholud, Dmitrii},
doi = {10.3150/13-BEJ552},
eprint = {1410.6033},
file = {::},
issn = {13507265},
journal = {Bernoulli},
keywords = {Dependent random variables,F-test,High-throughput screening,Non-homogeneous data,Non-normal population distribution,Outliers,Small sample size,Student's one- and two-sample t-statistics,Systematic effects,Test power,Welch statistic},
number = {4},
pages = {2102--2130},
title = {{Tail approximations for the student t-, F-, and Welch statistics for non-normal and not necessarily i.i.d. random variables}},
volume = {20},
year = {2014}
}
@article{Jing1994,
author = {Jing, Bing Yi and Feuerverger, Andrey and Robinson, John},
file = {:Users/ekatsevi/papers/Biometrika/Jing, Feuerverger, Robinson - 1994 - On the Bootstrap Saddlepoint Approximations.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {211--215},
title = {{On the Bootstrap Saddlepoint Approximations}},
volume = {81},
year = {1994}
}
@article{Bates2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2002.09644v1},
author = {Bates, Stephen and Sesia, Matteo and Sabatti, Chiara and Candes, Emmanuel},
eprint = {arXiv:2002.09644v1},
file = {:Users/ekatsevi/papers/Proceedings of the National Academy of Sciences/Bates et al. - 2020 - Causal Inference in Genetic Trio Studies.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
keywords = {causal discovery,conditional independence,false discovery rate,family-based association test,fbat,fdr,genome-,gwas,tdt,transmission disequilibrium test,trio,wide association study},
number = {39},
pages = {24117--24126},
title = {{Causal Inference in Genetic Trio Studies}},
volume = {117},
year = {2020}
}
@article{Abd2007,
author = {Abd-Elfattah, Ehab F. and Butler, Ronald W.},
file = {:Users/ekatsevi/papers/Unknown/Abd-Elfattah, Butler - 2007 - The Weighted Log-Rank Class of Permutation Tests P-Values and Confidence Intervals Using Saddlepoint Meth.pdf:pdf},
number = {3},
pages = {543--551},
title = {{The Weighted Log-Rank Class of Permutation Tests : P-Values and Confidence Intervals Using Saddlepoint Methods}},
volume = {94},
year = {2007}
}
@article{Josse2008,
abstract = {The relationship between two sets of variables defined for the same individuals can be evaluated by the RV coefficient. However, it is impossible to assess by the RV value alone whether or not the two sets of variables are significantly correlated, which is why a test is required. Asymptotic tests do exist but fail in many situations, hence the interest in permutation tests. However, the main drawbacks of the permutation tests are that they are time consuming. It is therefore interesting to approximate the permutation distribution with continuous distributions (without doing any permutation). The current approximations (normal approximation, a log-transformation and Pearson type III approximation) are discussed and a new one is described: an Edgeworth expansion. Finally, these different approximations are compared for both simulations and for a sensory example. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Josse, J. and Pag{\`{e}}s, J. and Husson, F.},
doi = {10.1016/j.csda.2008.06.012},
file = {:Users/ekatsevi/papers/Computational Statistics and Data Analysis/Josse, Pag{\`{e}}s, Husson - 2008 - Testing the significance of the RV coefficient.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
number = {1},
pages = {82--91},
title = {{Testing the significance of the RV coefficient}},
volume = {53},
year = {2008}
}
@misc{jensen1995saddlepoint,
author = {Jensen, Jens Ledet},
file = {:Users/ekatsevi/papers/Unknown/Jensen - 1995 - Saddlepoint Approximations.pdf:pdf},
publisher = {Oxford University Press},
title = {{Saddlepoint Approximations}},
year = {1995}
}
@article{Eisinga2013,
abstract = {We discuss saddlepoint approximations to the distribution of the sum of independent non-identically distributed binomial random variables. We examine the accuracy of the saddlepoint methods for a sum of 10 binomials with different sets of parameter values. The numerical results indicate that the saddlepoint approximations provide very accurate estimates for the probability mass function and the right-tail probabilities for the cumulative distribution function of the sum. {\textcopyright} 2012 The Authors. Statistica Neerlandica {\textcopyright} 2012 VVS.},
author = {Eisinga, Rob and {Te Grotenhuis}, Manfred and Pelzer, Ben},
doi = {10.1111/stan.12002},
file = {:Users/ekatsevi/papers/Statistica Neerlandica/Eisinga, Te Grotenhuis, Pelzer - 2013 - Saddlepoint approximations for the sum of independent non-identically distributed binomial rando.pdf:pdf},
issn = {00390402},
journal = {Statistica Neerlandica},
keywords = {Saddlepoint approximation,Sum of non-identical binomial variables},
number = {2},
pages = {190--201},
title = {{Saddlepoint approximations for the sum of independent non-identically distributed binomial random variables}},
volume = {67},
year = {2013}
}
@article{Lieberman1994,
abstract = {In this article, the saddlepoint approximations to the density and tail probability of a ratio of quadratic forms in normal variables are derived. A numerical exposition via the Durbin–Watson test statistic reveals several desirable features. The approximations, which involve only a limited number of computable functions, provide the practitioner with an accessible and a very powerful tool. {\textcopyright} 1994 Taylor {\&} Francis Group, LLC.},
author = {Lieberman, Offer},
doi = {10.1080/01621459.1994.10476825},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Lieberman - 1994 - Saddlepoint approximation for the distribution of a ratio of quadratic forms in normal variables.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Cgf,Fourier transform,Geary's formula,Steepest descent},
number = {427},
pages = {924--928},
title = {{Saddlepoint approximation for the distribution of a ratio of quadratic forms in normal variables}},
volume = {89},
year = {1994}
}
@article{Raz1990,
abstract = {When a linear regression function is estimated by ordinary least squares, the null hypothesis of no relationship between the response and the design variable can be tested by the normal theory F test. This article describes a new test for use in the case in which a smooth regression function is estimated by a nonparametric procedure such as kernel estimation or local regression. The test is constructed as an approximation to an exact permutation test. The test statistic is the ratio of sums of squares that are defined by analogy to the analysis of variance. The null permutation distribution of the test statistic is approximated by matching its exact mean and variance to the moments of a gamma distribution. A simulation study shows that the approximation is excellent for several regression procedures and for normal, heavy-tailed, and skewed error distributions. Simulation results are also used to investigate the power of the test as a function of the smoothing parameter. The test is applied to data on the relationship between lymphocyte concentrations and immunological status in men with human immunodeficiency virus infection. {\textcopyright} 1990 Taylor {\&} Francis Group, LLC.},
author = {Raz, Jonathan},
doi = {10.1080/01621459.1990.10475316},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Raz - 1990 - Testing for no effect when estimating a smooth function by nonparametric regression A randomization approach.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Kernel estimation,Local regression,Permutation test,Smoothing},
number = {409},
pages = {132--138},
title = {{Testing for no effect when estimating a smooth function by nonparametric regression: A randomization approach}},
volume = {85},
year = {1990}
}
@book{Efron1994a,
abstract = {An Introduction to the Bootstrap arms scientists and engineers as well as statisticians with the computational techniques they need to analyze and understand complicated data sets. The bootstrap is a computer-based method of statistical inference that answers statistical questions without formulas and gives a direct appreciation of variance, bias, coverage, and other probabilistic phenomena. This book presents an overview of the bootstrap and related methods for assessing statistical accuracy, concentrating on the ideas rather than their mathematical justification. Not just for beginners, the presentation starts off slowly, but builds in both scope and depth to ideas that are quite sophisticated.},
address = {New York},
author = {Efron, Bradley and Tibshirani, R.J.},
doi = {10.1201/9780429246593},
edition = {1st Editio},
publisher = {Chapman and Hall},
title = {{An Introduction to the Bootstrap}},
year = {1994}
}
@book{Davison1997,
author = {Davison, A. C. and Hinkley, D. V.},
file = {:Users/ekatsevi/papers/Unknown/Davison, Hinkley - 1997 - Bootstrap methods and their application.pdf:pdf},
title = {{Bootstrap methods and their application}},
year = {1997}
}
@article{Romano1990,
abstract = {Fisher's randomization construction of hypothesis tests is a powerful tool to yield tests that are nonparametric in nature in that their level is exactly equal to the nominal level in finite samples over a wide range of distributional assumptions. For example, the usual permutation t test to test equality of means is valid without a normality assumption of the underlying populations. On the other hand, Fisher's randomization construction is not applicable in this example unless the underlying populations differ only in location. In general, the basis for the randomization construction is invariance of the probability distribution of the data under a transformation group. It is the goal of this article to understand the robustness properties of randomization tests by studying their asymptotic validity in situations where the basis for their construction breaks down. Here, asymptotic validity refers to whether the probability of a Type I error tends asymptotically to the nominal level. In particular, it is shown that the randomization construction is generally asymptotically valid for certain one-sample problems, such as for testing a mean or a median, even when the underlying population is not symmetric. In contrast, the randomization construction for two-sample problems may yield invalid tests, though it depends on the precise nature of the problem. For example, the two-sample permutation test based on sample means is generally asymptotically valid only if the samples are of the same size. When comparing medians, however, the two-sample permutation test is generally invalid even if the sample sizes are equal. {\textcopyright} 1990 Taylor {\&} Francis Group, LLC.},
author = {Romano, Joseph P.},
doi = {10.1080/01621459.1990.10474928},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Romano - 1990 - On the behavior of randomization tests without a group invariance assumption.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Consistency,Hypothesis test,Permutation test,Robustness},
number = {411},
pages = {686--692},
title = {{On the behavior of randomization tests without a group invariance assumption}},
volume = {85},
year = {1990}
}
@article{Liu2018b,
abstract = {The distribution of the sum of independent non-identical binomial random variables is frequently encountered in areas such as genomics, healthcare, and operations research. Analytical solutions for the density and distribution are usually cumbersome to find and difficult to compute. Several methods have been developed to approximate the distribution, among which is the saddlepoint approximation. However, implementation of the saddlepoint approximation is non-trivial. In this paper, we implement the saddlepoint approximation in the sinib package and provide two examples to illustrate its usage. One example uses simulated data while the other uses real-world healthcare data. The sinib package addresses the gap between the theory and the implementation of approximating the sum of independent non-identical binomials.},
archivePrefix = {arXiv},
arxivId = {1712.01410},
author = {Liu, Boxiang and Quertermous, Thomas},
doi = {10.32614/rj-2018-011},
eprint = {1712.01410},
file = {:Users/ekatsevi/papers/R Journal/Liu, Quertermous - 2018 - Approximating the sum of independent non-identical binomial random variables.pdf:pdf},
issn = {20734859},
journal = {R Journal},
keywords = {saddlepoint approximation},
mendeley-tags = {saddlepoint approximation},
number = {1},
pages = {472--483},
title = {{Approximating the sum of independent non-identical binomial random variables}},
volume = {10},
year = {2018}
}
@article{LaVecchia2019,
abstract = {Saddlepoint techniques provide numerically accurate, small sample approximations to the distribution of estimators and test statistics. Except for a few simple models, these approximations are not available in the framework of stationary time series. We contribute to fill this gap. Under short or long range serial dependence, for Gaussian and non Gaussian processes, we show how to derive and implement saddlepoint approximations for two relevant classes of frequency domain statistics: ratio statistics and Whittle's estimator. We compare our new approximations to the ones obtained by the standard asymptotic theory and by two widely-applied bootstrap methods. The numerical exercises for Whittle's estimator show that our approximations yield accuracy's improvements, while preserving analytical tractability. A real data example concludes the paper.},
author = {{La Vecchia}, Davide and Ronchetti, Elvezio},
doi = {10.1016/j.jeconom.2018.10.009},
file = {::},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Macroeconomic time series,Periodogram,Relative error,Tilted edgeworth expansion,Whittle's estimator},
number = {2},
pages = {578--592},
publisher = {Elsevier B.V.},
title = {{Saddlepoint approximations for short and long memory time series: A frequency domain approach}},
url = {https://doi.org/10.1016/j.jeconom.2018.10.009},
volume = {213},
year = {2019}
}
@article{Celentano2021,
abstract = {We consider the problem of estimating a low-dimensional parameter in high-dimensional linear regression. Constructing an approximately unbiased estimate of the parameter of interest is a crucial step towards performing statistical inference. Several authors suggest to orthogonalize both the variable of interest and the outcome with respect to the nuisance variables, and then regress the residual outcome with respect to the residual variable. This is possible if the covariance structure of the regressors is perfectly known, or is sufficiently structured that it can be estimated accurately from data (e.g., the precision matrix is sufficiently sparse). Here we consider a regime in which the covariate model can only be estimated inaccurately, and hence existing debiasing approaches are not guaranteed to work. When errors in estimating the covariate model are correlated with errors in estimating the linear model parameter, an incomplete elimination of the bias occurs. We propose the Correlation Adjusted Debiased Lasso (CAD), which nearly eliminates this bias in some cases, including cases in which the estimation errors are neither negligible nor orthogonal. We consider a setting in which some unlabeled samples might be available to the statistician alongside labeled ones (semi-supervised learning), and our guarantees hold under the assumption of jointly Gaussian covariates. The new debiased estimator is guaranteed to cancel the bias in two cases: (1) when the total number of samples (labeled and unlabeled) is larger than the number of parameters, or (2) when the covariance of the nuisance (but not the effect of the nuisance on the variable of interest) is known. Neither of these cases is treated by state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {2107.14172},
author = {Celentano, Michael and Montanari, Andrea},
eprint = {2107.14172},
file = {:Users/ekatsevi/papers/arXiv/Celentano, Montanari - 2021 - CAD Debiasing the Lasso with inaccurate covariate model.pdf:pdf},
journal = {arXiv},
keywords = {AMP,Lasso,debiased lasso,double robustness,high-dimensional regression},
mendeley-tags = {AMP,Lasso,debiased lasso,double robustness,high-dimensional regression},
title = {{CAD: Debiasing the Lasso with inaccurate covariate model}},
url = {http://arxiv.org/abs/2107.14172},
year = {2021}
}
@article{Johnsen2021,
abstract = {We investigate saddlepoint approximations applied to the score test statistic in genome-wide association studies with binary phenotypes. The inaccuracy in the normal approximation of the score test statistic increases with increasing sample imbalance and with decreasing minor allele count. Applying saddlepoint approximations to the score test statistic distribution greatly improve the accuracy, even far out in the tail of the distribution. By using exact results for an intercept model and binary covariate model, as well as simulations for models with nuisance parameters, we emphasize the need for continuity corrections in order to achieve valid {\$}p{\$}-values. The performance of the saddlepoint approximations is evaluated by overall and conditional type I error rate on simulated data. We investigate the methods further by using data from UK Biobank with skin and soft tissue infections as phenotype, using both common and rare variants. The analysis confirms that continuity correction is important particularly for rare variants, and that the normal approximation gives a highly inflated type I error rate for case imbalance.},
archivePrefix = {arXiv},
arxivId = {2110.04025},
author = {Johnsen, P{\aa}l Vegard and Bakke, {\O}yvind and Bj{\o}rnland, Thea and DeWan, Andrew Thomas and Langaas, Mette},
eprint = {2110.04025},
file = {:Users/ekatsevi/papers/arXiv/Johnsen et al. - 2021 - Saddlepoint approximations in binary genome-wide association studies.pdf:pdf},
journal = {arXiv},
number = {2013},
title = {{Saddlepoint approximations in binary genome-wide association studies}},
url = {http://arxiv.org/abs/2110.04025},
year = {2021}
}
@article{Kolassa1995,
abstract = {Hettmansperger (1984) quotes a result showing that the distribution function of the Wilcoxon signed rank statistic is approximated by the usual Edgeworth series using the first four cumulants, to 0(n-1). In light of standard Edgeworth series results for random variables confined to a lattice, this result is counterintuitive. One expects correction terms to be necessary because of the lattice nature of the Wilcoxon statistic. This paper explains this apparent paradox, provides an alternative proof relying on basic Edgeworth series results, and provides a sharper result. Interesting features in this problem highlighting limitations of expansions for random variables on a lattice are discussed. {\textcopyright} 1995.},
author = {Kolassa, John E.},
doi = {10.1016/0167-7152(95)00164-H},
file = {:Users/ekatsevi/papers/Statistics and Probability Letters/Kolassa - 1995 - Edgeworth approximations for rank sum test statistics.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Edgeworth series,Lattice distributions,Wilcoxon statistic},
number = {2},
pages = {169--171},
title = {{Edgeworth approximations for rank sum test statistics}},
volume = {24},
year = {1995}
}
@book{Bhattacharya1976,
abstract = {Although this was first published in 1976, it has gained new significance and renewed interest among statisticians due to the developments of modern statistical techniques such as the bootstrap, the efficacy of which can be ascertained by asymptotic expansions. This also is the only book containing a detailed treatment of various refinements of the multivariate central limit theorem (CLT), including Berry-Essen-type error bounds for probabilities of general classes of functions and sets, and asymptotic expansions for both lattice and non-lattice distributions.},
author = {Bhattacharya, R. N. and Rao, R. Ranga},
doi = {10.2307/1402546},
file = {:Users/ekatsevi/papers/Unknown/Bhattacharya, Rao - 1976 - Normal Approximation and Asymptotic Expansions.pdf:pdf},
isbn = {9780898718973},
issn = {03067734},
title = {{Normal Approximation and Asymptotic Expansions}},
year = {1976}
}
@article{Hemerik2020,
abstract = {The statistical literature is known to be inconsistent in the use of the terms ‘permutation test' and ‘randomisation test'. Several authors successfully argue that these terms should be used to refer to two distinct classes of tests and that there are major conceptual differences between these classes. The present paper explains an important difference in mathematical reasoning between these classes: a permutation test fundamentally requires that the set of permutations has a group structure, in the algebraic sense; the reasoning behind a randomisation test is not based on such a group structure, and it is possible to use an experimental design that does not correspond to a group. In particular, we can use a randomisation scheme where the number of possible treatment patterns is larger than in standard experimental designs. This leads to exact p values of improved resolution, providing increased power for very small significance levels, at the cost of decreased power for larger significance levels. We discuss applications in randomised trials and elsewhere. Further, we explain that Fisher's famous Lady Tasting Tea experiment, which is commonly referred to as the first permutation test, is in fact a randomisation test. This distinction is important to avoid confusion and invalid tests.},
author = {Hemerik, Jesse and Goeman, Jelle J.},
doi = {10.1111/insr.12431},
file = {:Users/ekatsevi/papers/International Statistical Review/Hemerik, Goeman - 2020 - Another Look at the Lady Tasting Tea and Differences Between Permutation Tests and Randomisation Tests.pdf:pdf},
issn = {17515823},
journal = {International Statistical Review},
keywords = {group invariance test,lady tasting tea,permutation test,randomisation test,resampling},
mendeley-tags = {permutation test,resampling},
number = {January},
title = {{Another Look at the Lady Tasting Tea and Differences Between Permutation Tests and Randomisation Tests}},
year = {2020}
}
@article{Che2014,
abstract = {BACKGROUND: Permutation testing is a robust and popular approach for significance testing in genomic research, which has the broad advantage of estimating significance non-parametrically, thereby safe guarding against inflated type I error rates. However, the computational efficiency remains a challenging issue that limits its wide application, particularly in genome-wide association studies (GWAS). Because of this, adaptive permutation strategies can be employed to make permutation approaches feasible. While these approaches have been used in practice, there is little research into the statistical properties of these approaches, and little guidance into the proper application of such a strategy for accurate p-value estimation at the GWAS level.$\backslash$n$\backslash$nMETHODS: In this work, we advocate an adaptive permutation procedure that is statistically valid as well as computationally feasible in GWAS. We perform extensive simulation experiments to evaluate the robustness of the approach to violations of modeling assumptions and compare the power of the adaptive approach versus standard approaches. We also evaluate the parameter choices in implementing the adaptive permutation approach to provide guidance on proper implementation in real studies. Additionally, we provide an example of the application of adaptive permutation testing on real data.$\backslash$n$\backslash$nRESULTS: The results provide sufficient evidence that the adaptive test is robust to violations of modeling assumptions. In addition, even when modeling assumptions are correct, the power achieved by adaptive permutation is identical to the parametric approach over a range of significance thresholds and effect sizes under the alternative. A framework for proper implementation of the adaptive procedure is also generated.$\backslash$n$\backslash$nCONCLUSIONS: While the adaptive permutation approach presented here is not novel, the current study provides evidence of the validity of the approach, and importantly provides guidance on the proper implementation of such a strategy. Additionally, tools are made available to aid investigators in implementing these approaches.},
author = {Che, Ronglin and Jack, John R. and Motsinger-Reif, Alison A. and Brown, Chad C.},
doi = {10.1186/1756-0381-7-9},
file = {:Users/ekatsevi/papers/BioData Mining/Che et al. - 2014 - An adaptive permutation approach for genome-wide association study Evaluation and recommendations for use.pdf:pdf},
issn = {17560381},
journal = {BioData Mining},
keywords = {Monte Carlo,Multiple testing},
mendeley-tags = {Monte Carlo,Multiple testing},
number = {1},
pages = {1--13},
title = {{An adaptive permutation approach for genome-wide association study: Evaluation and recommendations for use}},
volume = {7},
year = {2014}
}
@article{Zhou2009,
author = {Zhou, Chunxiao and Wang, Huixia Judy and Wang, Yongmei Michelle},
file = {:Users/ekatsevi/papers/Advances in Neural Information Processing Systems 22/Zhou, Wang, Wang - 2009 - Efficient Moments-based Permutation Tests.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 22},
title = {{Efficient Moments-based Permutation Tests}},
year = {2009}
}
@article{Niu2024a,
author = {Niu, Ziang and {Ray Choudhury}, Jyotishka and Katsevich, Eugene},
journal = {arxiv},
title = {{Computationally efficient and statistically accurate conditional independence testing with spaCRT}},
year = {2024}
}
@article{Niu2024,
author = {Niu, Ziang and {Ray Choudhury}, Jyotishka and Katsevich, Eugene},
journal = {arXiv},
title = {{The saddlepoint approximation for averages of conditionally independent random variables}},
year = {2024}
}
@article{Chardon2023a,
author = {Chardon, Florence M and Mcdiarmid, Troy A and Page, Nicholas F and Martin, Beth},
file = {:Users/ekatsevi/papers/bioRxiv/Chardon et al. - 2023 - Multiplex, single-cell CRISPRa screening for cell type specific regulatory elements.pdf:pdf},
journal = {bioRxiv},
keywords = {CRISPR,single cell CRISPR,single-cell},
mendeley-tags = {CRISPR,single cell CRISPR,single-cell},
pages = {1--38},
title = {{Multiplex, single-cell CRISPRa screening for cell type specific regulatory elements}},
year = {2023}
}
@article{Davison1996,
author = {Davison, Anthony C.},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society, Series B (Methodological)/Davison - 1998 - Approximate Conditional Inference in Generalized Linear Models.pdf:pdf},
journal = {Journal of the Royal Statistical Society, Series B (Methodological)},
keywords = {added variable,canonical link function,conditional inference,double-saddlepoint approximation,generalized linear model,modified},
number = {3},
pages = {445--461},
title = {{Approximate Conditional Inference in Generalized Linear Models}},
volume = {50},
year = {1998}
}
@book{Efron1994,
author = {Efron, Bradley and Tibshirani, Robert},
publisher = {Chapman and Hall},
title = {{An introduction to the bootstrap}},
year = {1994}
}
@book{Hall2010a,
author = {Hall, Peter},
booktitle = {Springer Series in Statistics},
file = {:Users/ekatsevi/papers/Springer Series in Statistics/Hall - 2010 - The Bootstrap and the Edgeworth expansion.pdf:pdf},
isbn = {0387945083},
pages = {3--5},
title = {{The Bootstrap and the Edgeworth expansion}},
year = {2010}
}
@book{Lista2017,
abstract = {The main goal of an experimental physicist is to measure quantities of interest, possibly with the best precision. In the luckiest cases, measurements lead to the discovery of new physical phenomena that may represent a breakthrough in the knowledge of Nature. Measurements, and, more in general, observations of Nature's behavior, are performed with experiments that record quantitative information about the physical phenomenon under observation.},
author = {Klenke, Achim},
booktitle = {Lecture Notes in Physics},
file = {:Users/ekatsevi/papers/Lecture Notes in Physics/Klenke - 2017 - Probability theory.pdf:pdf},
isbn = {9781447153603},
issn = {00758450},
pages = {1--23},
title = {{Probability theory}},
volume = {941},
year = {2017}
}
@article{Diciccio1991,
author = {DiCiccio, B Y Thomas J and Martin, Michael A},
file = {::},
journal = {Biometrika},
number = {4},
pages = {891--902},
title = {{Approximations of Marginal Tail Probabilities for a Class of Smooth Functions with Applications to Bayesian and Conditional Inference}},
volume = {78},
year = {1991}
}
@article{Nadarajah2015,
abstract = {The exact distribution of the sum of more than two independent beta random variables has not been known. Even in terms of approximations, only the normal approximation is known for the sum. Motivated by Murakami [Statistica Neerlandica, 2014, doi:10.1111/stan.12032], we derive here a saddlepoint approximation for the distribution of sum. An extensive simulation study shows that it always performs better than the normal approximation.},
author = {Nadarajah, Saralees and Jiang, Xiao and Chu, Jeffrey},
doi = {10.1111/stan.12051},
file = {:Users/ekatsevi/papers/Statistica Neerlandica/Nadarajah, Jiang, Chu - 2015 - A saddlepoint approximation to the distribution of the sum of independent non-identically beta random var.pdf:pdf},
issn = {14679574},
journal = {Statistica Neerlandica},
keywords = {Beta distribution,Confluent hypergeometric function,Saddlepoint approximation,saddlepoint approximation},
mendeley-tags = {saddlepoint approximation},
number = {2},
pages = {102--114},
title = {{A saddlepoint approximation to the distribution of the sum of independent non-identically beta random variables}},
volume = {69},
year = {2015}
}
@book{Los,
abstract = {In this chapter we collect basic definitions and results on the normal approximation of distributions of independent random variables (in Kolmogorov distance), and also discuss possible improved rates of approximation when replacing the normal law by corresponding Edgeworth corrections. The first section deals with moment based quantities for single random variables},
author = {Bobkov, Sergey and Chistyakov, Gennadiy and G{\"{o}}tze, Friedrich},
booktitle = {Probability Theory and Stochastic Modelling},
doi = {10.1007/978-3-031-31149-9_4},
file = {:Users/ekatsevi/papers/Probability Theory and Stochastic Modelling/Bobkov, Chistyakov, G{\"{o}}tze - 2023 - Sums of Independent Random Variables.pdf:pdf},
issn = {21993149},
pages = {63--88},
title = {{Sums of Independent Random Variables}},
volume = {104},
year = {2023}
}
@article{Bai2022,
abstract = {This article studies inference for the average treatment effect in randomized controlled trials where treatment status is determined according to a “matched pairs” design. By a “matched pairs” design, we mean that units are sampled iid from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment. This type of design is used routinely throughout the sciences, but fundamental questions about its implications for inference about the average treatment effect remain. The main requirement underlying our analysis is that pairs are formed so that units within pairs are suitably “close” in terms of the baseline covariates, and we develop novel results to ensure that pairs are formed in a way that satisfies this condition. Under this assumption, we show that, for the problem of testing the null hypothesis that the average treatment effect equals a prespecified value in such settings, the commonly used two-sample t-test and “matched pairs” t-test are conservative in the sense that these tests have limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level. We show, however, that a simple adjustment to the standard errors of these tests leads to a test that is asymptotically exact in the sense that its limiting rejection probability under the null hypothesis equals the nominal level. We also study the behavior of randomization tests that arise naturally in these types of settings. When implemented appropriately, we show that this approach also leads to a test that is asymptotically exact in the sense described previously, but additionally has finite-sample rejection probability no greater than the nominal level for certain distributions satisfying the null hypothesis. A simulation study and empirical application confirm the practical relevance of our theoretical results.},
author = {Bai, Yuehao and Romano, Joseph P. and Shaikh, Azeem M.},
doi = {10.1080/01621459.2021.1883437},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Bai, Romano, Shaikh - 2022 - Inference in Experiments With Matched Pairs.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Matched pairs,Matched pairs t-test,Permutation test,Randomized controlled trial,Treatment assignment,Two-sample t-test},
number = {540},
pages = {1726--1737},
publisher = {Taylor {\&} Francis},
title = {{Inference in Experiments With Matched Pairs}},
url = {https://doi.org/10.1080/01621459.2021.1883437},
volume = {117},
year = {2022}
}
@article{Zhou2018,
abstract = {In genome-wide association studies (GWAS) for thousands of phenotypes in large biobanks, most binary traits have substantially fewer cases than controls. Both of the widely used approaches, the linear mixed model and the recently proposed logistic mixed model, perform poorly; they produce large type I error rates when used to analyze unbalanced case-control phenotypes. Here we propose a scalable and accurate generalized mixed model association test that uses the saddlepoint approximation to calibrate the distribution of score test statistics. This method, SAIGE (Scalable and Accurate Implementation of GEneralized mixed model), provides accurate P values even when case-control ratios are extremely unbalanced. SAIGE uses state-of-art optimization strategies to reduce computational costs; hence, it is applicable to GWAS for thousands of phenotypes by large biobanks. Through the analysis of UK Biobank data of 408,961 samples from white British participants with European ancestry for {\textgreater} 1,400 binary phenotypes, we show that SAIGE can efficiently analyze large sample data, controlling for unbalanced case-control ratios and sample relatedness.},
author = {Zhou, Wei and Nielsen, Jonas B. and Fritsche, Lars G. and Dey, Rounak and Gabrielsen, Maiken E. and Wolford, Brooke N. and LeFaive, Jonathon and VandeHaar, Peter and Gagliano, Sarah A. and Gifford, Aliya and Bastarache, Lisa A. and Wei, Wei Qi and Denny, Joshua C. and Lin, Maoxuan and Hveem, Kristian and Kang, Hyun Min and Abecasis, Goncalo R. and Willer, Cristen J. and Lee, Seunggeun},
doi = {10.1038/s41588-018-0184-y},
file = {:Users/ekatsevi/papers/Nature Genetics/Zhou et al. - 2018 - Efficiently controlling for case-control imbalance and sample relatedness in large-scale genetic association studie.pdf:pdf},
issn = {15461718},
journal = {Nature Genetics},
number = {9},
pages = {1335--1341},
pmid = {30104761},
publisher = {Springer US},
title = {{Efficiently controlling for case-control imbalance and sample relatedness in large-scale genetic association studies}},
url = {http://dx.doi.org/10.1038/s41588-018-0184-y},
volume = {50},
year = {2018}
}
@article{Pahl2010,
abstract = {Motivation: In genome-wide association studies (GWAS) examining hundreds of thousands of genetic markers, the potentially high number of false positive findings requires statistical correction for multiple testing. Permutation tests are considered the gold standard for multiple testing correction in GWAS, because they simultaneously provide unbiased type I error control and high power. At the same time, they demand heavy computational effort, especially with large-scale datasets of modern GWAS. In recent years, the computational problem has been circumvented by using approximations to permutation tests, which, however, may be biased. Results: We have tackled the original computational problem of permutation testing in GWAS and herein present a permutation test algorithm one or more orders of magnitude faster than existing implementations, which enables efficient permutation testing on a genome-wide scale. Our algorithm does not rely on any kind of approximation and hence produces unbiased results identical to a standard permutation test. A noteworthy feature of our algorithm is a particularly effective performance when analyzing high-density marker sets. Availability: Freely available on the web at},
author = {Pahl, Roman and Sch{\"{a}}fer, Helmut},
doi = {10.1093/bioinformatics/btq399},
file = {:Users/ekatsevi/papers/Unknown/Pahl, Sch{\"{a}}fer - 2010 - Genome analysis PERMORY an LD-exploiting permutation test algorithm for powerful genome-wide association testing.pdf:pdf},
number = {17},
pages = {2093--2100},
title = {{Genome analysis PERMORY: an LD-exploiting permutation test algorithm for powerful genome-wide association testing}},
url = {http://www.permory.org},
volume = {26},
year = {2010}
}
@article{Lugannani1980,
abstract = {In the present paper a uniform asymptotic series is derived for the probability distribution of the sum of a large number of independent random variables. In contrast to the usual Edgeworth-type series, the uniform series gives good accuracy throughout its entire domain. Our derivation uses the fact that the major components of the distribution are determined by a saddle point and a singularity at the origin. The analogous series for the probability density, due to Daniels, depends only on the saddle point. Two illustrative examples are presented that show excellent agreement with the exact distributions.},
author = {Lugannani, Robert and Rice, Stephen},
doi = {10.2307/1426607},
file = {:Users/ekatsevi/papers/Advances in Applied Probability/Lugannani, Rice - 1980 - Saddle point approximation for the distribution of the sum of independent random variables.pdf:pdf},
issn = {0001-8678},
journal = {Advances in Applied Probability},
number = {2},
pages = {475--490},
title = {{Saddle point approximation for the distribution of the sum of independent random variables}},
volume = {12},
year = {1980}
}
@article{Winkler2016,
abstract = {Permutation tests are increasingly being used as a reliable method for inference in neuroimaging analysis. However, they are computationally intensive. For small, non-imaging datasets, recomputing a model thousands of times is seldom a problem, but for large, complex models this can be prohibitively slow, even with the availability of inexpensive computing power. Here we exploit properties of statistics used with the general linear model (GLM) and their distributions to obtain accelerations irrespective of generic software or hardware improvements. We compare the following approaches: (i) performing a small number of permutations; (ii) estimating the p-value as a parameter of a negative binomial distribution; (iii) fitting a generalised Pareto distribution to the tail of the permutation distribution; (iv) computing p-values based on the expected moments of the permutation distribution, approximated from a gamma distribution; (v) direct fitting of a gamma distribution to the empirical permutation distribution; and (vi) permuting a reduced number of voxels, with completion of the remainder using low rank matrix theory. Using synthetic data we assessed the different methods in terms of their error rates, power, agreement with a reference result, and the risk of taking a different decision regarding the rejection of the null hypotheses (known as the resampling risk). We also conducted a re-analysis of a voxel-based morphometry study as a real-data example. All methods yielded exact error rates. Likewise, power was similar across methods. Resampling risk was higher for methods (i), (iii) and (v). For comparable resampling risks, the method in which no permutations are done (iv) was the absolute fastest. All methods produced visually similar maps for the real data, with stronger effects being detected in the family-wise error rate corrected maps by (iii) and (v), and generally similar to the results seen in the reference set. Overall, for uncorrected p-values, method (iv) was found the best as long as symmetric errors can be assumed. In all other settings, including for familywise error corrected p-values, we recommend the tail approximation (iii). The methods considered are freely available in the tool PALM — Permutation Analysis of Linear Models.},
author = {Winkler, Anderson M. and Ridgway, Gerard R. and Douaud, Gwena{\"{e}}lle and Nichols, Thomas E. and Smith, Stephen M.},
doi = {10.1016/J.NEUROIMAGE.2016.05.068},
file = {:Users/ekatsevi/papers/NeuroImage/Winkler et al. - 2016 - Faster permutation inference in brain imaging.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Gamma distribution,Generalised Pareto distribution,Low rank matrix completion,Negative binomial distribution,Pearson type III distribution,Permutation tests,Tail approximation},
month = {nov},
pages = {502--516},
pmid = {27288322},
publisher = {Academic Press Inc.},
title = {{Faster permutation inference in brain imaging}},
volume = {141},
year = {2016}
}
@article{Fischer2024,
abstract = {In contemporary problems involving genetic or neuroimaging data, thousands of hypotheses need to be tested. Due to their high power, and finite sample guarantees on type-1 error under weak assumptions, Monte-Carlo permutation tests are often considered as gold standard for these settings. However, the enormous computational effort required for (thousands of) permutation tests is a major burden. Recently, Fischer and Ramdas (2024) constructed a permutation test for a single hypothesis in which the permutations are drawn sequentially one-by-one and the testing process can be stopped at any point without inflating the type I error. They showed that the number of permutations can be substantially reduced (under null and alternative) while the power remains similar. We show how their approach can be modified to make it suitable for a broad class of multiple testing procedures. In particular, we discuss its use with the Benjamini-Hochberg procedure and illustrate the application on a large dataset.},
archivePrefix = {arXiv},
arxivId = {2404.15586},
author = {Fischer, Lasse and Ramdas, Aaditya},
eprint = {2404.15586},
file = {:Users/ekatsevi/papers/Unknown/Fischer, Ramdas - 2024 - Multiple testing with anytime-valid Monte-Carlo p-values.pdf:pdf},
pages = {1--22},
title = {{Multiple testing with anytime-valid Monte-Carlo p-values}},
url = {http://arxiv.org/abs/2404.15586},
year = {2024}
}
@article{DiCiccio1990,
abstract = {The fundamental problem addressed in this paper is the problem of constructing confidence limits for a functional of a distribution in nonparametric settings. Specifically, given a random sample of observations from a distribution F, interest focuses on constructing confidence limits for a real valued functional $\Theta$ of F. Several procedures from the bootstrap literature are reviewed and, because no single method has emerged as the best solution for all problems, some new methods are presented as well. These new methods are motivated by an appropriate reduction of the nonparametric problem to a parametric problem with no nuisance parameters via the construction of a least favorable family. The coverage error of an approximate confidence limit is the difference between the exact coverage probability and the nominal level. All the procedures are compared by determining the exact order of coverage error of each method. /// Dans cet article, nous nous int{\'{e}}ressons au probl{\`{e}}me de la construction de limites de confiance d'une fonctionnelle d'une distribution dans un contexte nonparam{\'{e}}trique. Etant donn{\'{e}} un {\'{e}}chantillon al{\'{e}}atoire de la distribution F, nous voulons construire un intervalle de confiance pour une fonctionnelle {\`{a}} valeur r{\'{e}}elle $\Theta$ de F. On pr{\'{e}}sente plusieurs proc{\'{e}}dures de la lit{\'{e}}rature sur l'auto-amor{\c{c}}age, et puisqu'aucune solution ne s'est montr{\'{e}}e id{\'{e}}ale pour tous les probl{\`{e}}mes, on pr{\'{e}}sente {\'{e}}galement quelques nouvelles m{\'{e}}thodes. Ces nouvelles m{\'{e}}thodes sont motiv{\'{e}}es par une r{\'{e}}duction appropri{\'{e}}e du probl{\`{e}}me nonparam{\'{e}}trique {\`{a}} un probl{\`{e}}me param{\'{e}}trique sans param{\`{e}}tre de nuisance {\`{a}} l'aide de la construction d'une famille la moins favorable. L'erreur de couverture d'une intervalle de confiance approximatif est la diff{\'{e}}rence entre la probabilit{\'{e}} de couverture exacte et le niveau pr{\'{e}}sum{\'{e}}. On compare toutes les proc{\'{e}}dures pour d{\'{e}}terminer l'ordre exact de l'erreur de couverture de chaque m{\'{e}}thode.},
author = {DiCiccio, Thomas J. and Romano, Joseph P.},
doi = {10.2307/1403474},
file = {:Users/ekatsevi/papers/International Statistical Review Revue Internationale de Statistique/DiCiccio, Romano - 1990 - Nonparametric Confidence Limits by Resampling Methods and Least Favorable Families.pdf:pdf},
issn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
number = {1},
pages = {59},
title = {{Nonparametric Confidence Limits by Resampling Methods and Least Favorable Families}},
volume = {58},
year = {1990}
}
@article{DiCiccio2017,
abstract = {Given a sample from a bivariate distribution, consider the problem of testing independence. A permutation test based on the sample correlation is known to be an exact level $\alpha$ test. However, when used to test the null hypothesis that the samples are uncorrelated, the permutation test can have rejection probability that is far from the nominal level. Further, the permutation test can have a large Type 3 (directional) error rate, whereby there can be a large probability that the permutation test rejects because the sample correlation is a large positive value, when in fact the true correlation is negative. It will be shown that studentizing the sample correlation leads to a permutation test which is exact under independence and asymptotically controls the probability of Type 1 (or Type 3) errors. These conclusions are based on our results describing the almost sure limiting behavior of the randomization distribution. We will also present asymptotically robust randomization tests for regression coefficients, including a result based on a modified procedure of Freedman and Lane. Simulations and empirical applications are included. Supplementary materials for this article are available online.},
author = {DiCiccio, Cyrus J. and Romano, Joseph P.},
doi = {10.1080/01621459.2016.1202117},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/DiCiccio, Romano - 2017 - Robust Permutation Tests For Correlation And Regression Coefficients.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Least squares,Partial correlation,Randomization tests,Studentization,Testing independence},
month = {jul},
number = {519},
pages = {1211--1220},
publisher = {American Statistical Association},
title = {{Robust Permutation Tests For Correlation And Regression Coefficients}},
volume = {112},
year = {2017}
}
@article{Barry2024,
abstract = {Single-cell CRISPR screens (perturb-seq) link genetic perturbations to phenotypic changes in individual cells. The most fundamental task in perturb-seq analysis is to test for association between a perturbation and a count outcome, such as gene expression. We conduct the first-ever comprehensive benchmarking study of association testing methods for low multiplicity-of-infection (MOI) perturb-seq data, finding that existing methods produce excess false positives. We conduct an extensive empirical investigation of the data, identifying three core analysis challenges: sparsity, confounding, and model misspecification. Finally, we develop an association testing method — SCEPTRE low-MOI — that resolves these analysis challenges and demonstrates improved calibration and power.},
author = {Barry, Timothy and Mason, Kaishu and Roeder, Kathryn and Katsevich, Eugene},
doi = {10.1186/s13059-024-03254-2},
file = {:Users/ekatsevi/papers/Genome Biology/Barry et al. - 2024 - Robust differential expression testing for single-cell CRISPR screens at low multiplicity of infection.pdf:pdf;:Users/ekatsevi/papers/Genome Biology/Barry et al. - 2024 - Robust differential expression testing for single-cell CRISPR screens at low multiplicity of infection(2).pdf:pdf},
issn = {1474760X},
journal = {Genome Biology},
number = {1},
pages = {1--30},
pmid = {38760839},
publisher = {BioMed Central},
title = {{Robust differential expression testing for single-cell CRISPR screens at low multiplicity of infection}},
volume = {25},
year = {2024}
}
@article{Besag1991,
abstract = {The assessment of statistical significance by Monte Carlo simulation may be costly in computer time. This paper looks at a number of ways of calculating exact Monte Carlo p-values by sequential sampling. Such p-values are shown to have properties similar to those obtained by sampling with a fixed sample size. Both standard and generalized Monte Carlo procedures are discussed and, in particular, a sequential method is proposed for dealing with situations in which values can only be conveniently generated using a Markov chain, conditioned to pass through the observed data.},
author = {Besag, Julian and Clifford, Peter},
doi = {10.1093/biomet/78.2.301},
file = {:Users/ekatsevi/papers/Biometrika/Besag, Clifford - 1991 - Sequential Monte Carlo p-values.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Markov chain,Monte Carlo,Monte Carlo testing,Multiple testing,P-value,Sequential estimation,Sequential test,Significance test},
mendeley-tags = {Monte Carlo,Multiple testing},
number = {2},
pages = {301--304},
title = {{Sequential Monte Carlo p-values}},
volume = {78},
year = {1991}
}
@article{Yadlowsky2022,
abstract = {We revisit the classical causal inference problem of estimating the average treatment effect in the presence of fully observed confounding variables using two-stage semiparametric methods. In existing theoretical studies of methods such as G-computation, inverse propensity weighting (IPW), and two common doubly robust estimators-augmented IPW (AIPW) and targeted maximum likelihood estimation (TMLE)-they are either bias-dominated, or have similar asymptotic statistical properties. However, when applied to real datasets, they often appear to have notably different variance. We compare these methods when using a machine learning (ML) model to estimate the nuisance parameters of the semiparametric model, and highlight some of the important differences. When the outcome model estimates have little bias, which is common among some key ML models, G-computation and the TMLE outperforms the other estimators in both bias and variance. We show that the differences can be explained using high-dimensional statistical theory, where the number of confounders d is of the same order as the sample size n. To make this theoretical problem tractable, we posit a generalized linear model for the effect of the con-founders on the treatment assignment and outcomes. Despite making parametric assumptions, this setting is a useful surrogate for some machine learning methods used to adjust for confounding in two-stage semiparametric methods. In particular, the estimation of the first stage adds variance that does not vanish, forcing us to confront terms in the asymptotic expansion that normally are brushed aside as finite sample defects. However, our model emphasizes differences in performance between these estimators beyond first-order asymptotics.},
archivePrefix = {arXiv},
arxivId = {2203.12538v2},
author = {Yadlowsky, Steve},
eprint = {2203.12538v2},
file = {::},
pages = {1--29},
title = {{Explaining Practical Differences Between Treatment Effect Estimators with High Dimensional Asymptotics}},
year = {2022}
}
@article{Murakami2014,
abstract = {Calculating the probability of the corresponding significance point is important for finite sample sizes. However, it is difficult to evaluate this probability when the sample sizes are moderate to large. Under these circumstances, consideration of a more accurate approximation for the distribution function is extremely important. Herein, we performed a saddlepoint approximation in the upper tails for the distribution of the sum of independent non-identically uniform random variables under finite sample sizes. Saddlepoint approximation results were compared with those for a normal approximation. Additionally, the order of errors of the saddlepoint approximation was derived.},
author = {Murakami, Hidetoshi},
doi = {10.1111/stan.12032},
file = {:Users/ekatsevi/papers/Statistica Neerlandica/Murakami - 2014 - A saddlepoint approximation to the distribution of the sum of independent non-identically uniform random variables.pdf:pdf},
issn = {14679574},
journal = {Statistica Neerlandica},
keywords = {Independent and non-identically distributed,Saddlepoint approximation,Sum of uniform random variables,saddlepoint approximation},
mendeley-tags = {saddlepoint approximation},
number = {4},
pages = {267--275},
title = {{A saddlepoint approximation to the distribution of the sum of independent non-identically uniform random variables}},
volume = {68},
year = {2014}
}
@article{Harvill1995,
author = {Harvill, Jane L and Newton, H Joseph},
file = {::},
number = {1},
pages = {226--231},
title = {{Saddlepoint Approximations for the Difference of Order Statistics}},
volume = {82},
year = {1995}
}
@article{Math-,
author = {Math-, Modern and Analysis, Functional and Consider, M},
file = {:Users/ekatsevi/papers/Unknown/Math-, Analysis, Consider - Unknown - The $\epsilon$ 3 argument.pdf:pdf},
number = {1},
pages = {1--5},
title = {{The $\epsilon$ / 3 argument}},
volume = {I}
}
@article{Diciccio1994,
author = {DiCiccio, Thomas J. and Martin, Michael A. and Young, G. Alastair},
file = {:Users/ekatsevi/papers/Statistica Sinica/DiCiccio, Martin, Young - 1994 - Analytical approximations to bootstrap distribution functions using saddlepoint methods.pdf:pdf},
journal = {Statistica Sinica},
number = {1},
pages = {281--295},
title = {{Analytical approximations to bootstrap distribution functions using saddlepoint methods}},
volume = {4},
year = {1994}
}
@article{Kempthorne1955,
author = {Kempthorne, Oscar},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Kempthorne - 1955 - The Randomization Theory of Experimental Inference.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {271},
pages = {946--967},
title = {{The Randomization Theory of Experimental Inference}},
volume = {50},
year = {1955}
}
@book{Eni1967,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy bynhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Eni},
booktitle = {Angewandte Chemie International Edition, 6(11), 951–952.},
file = {:Users/ekatsevi/papers/Angewandte Chemie International Edition, 6(11), 951–952/Eni - 1967 - 済無No Title No Title No Title.pdf:pdf},
isbn = {9783034897907},
number = {Mi},
pages = {5--24},
title = {{済無No Title No Title No Title}},
year = {1967}
}
@article{Kolassa2007,
abstract = {This article considers asymptotic approximations to tail probabilities of a random variable whose distribution depends on a parameter n heuristically representing sample size. Random variables considered have cumulant generating functions with properties similar to that of sums of independent and identically distributed random variables. Probability approximations of Robinson (1982) and Lugannani and Rice (1980) are shown to be equivalent to a relative size O(1/n), under regularity conditions no stronger than the weaker of those necessary to prove either of the two approximations. Applications to permutation testing are discussed.},
author = {Kolassa, John E.},
doi = {10.1080/03610920600974187},
file = {:Users/ekatsevi/papers/Communications in Statistics - Theory and Methods/Kolassa - 2007 - A proof of the asymptotic equivalence of two-tail probability approximations.pdf:pdf},
issn = {03610926},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Asymptotics,Saddlepoint},
number = {2},
pages = {221--228},
title = {{A proof of the asymptotic equivalence of two-tail probability approximations}},
volume = {36},
year = {2007}
}
@book{TSH,
address = {New York},
author = {Lehmann, E. L. and Romano, Joseph P.},
doi = {10.2307/2982206},
edition = {Third},
file = {:Users/ekatsevi/papers/Unknown/Lehmann, Romano - 2005 - Testing Statistical Hypotheses.pdf:pdf},
isbn = {0387988645},
issn = {09641998},
publisher = {Springer},
title = {{Testing Statistical Hypotheses}},
year = {2005}
}
@article{Anderson2001a,
abstract = {Several approximate permutation tests have been proposed for tests of partial regression coefficients in a linear model based on sample partial correlations. This paper begins with an explanation and notation for an exact test. It then compares the distributions of the test statistics under the various permutation methods proposed, and shows that the partial correlations under permutation are asymptotically jointly normal with means 0 and variances 1. The method of Freedman {\&} Lane (1983) is found to have asymptotic correlation 1 with the exact test, and the other methods are found to have smaller correlations with this test. Under local alternatives the critical values of all the approximate permutation tests converge to the same constant, so they all have the same asymptotic power. Simulations demonstrate these theoretical results.},
author = {Anderson, Marti J. and Robinson, John},
doi = {10.1111/1467-842X.00156},
file = {::},
issn = {13691473},
journal = {Australian and New Zealand Journal of Statistics},
keywords = {Asymptotics,Partial correlations,Power,Resampling},
number = {1},
pages = {75--88},
title = {{Permutation tests for linear models}},
volume = {43},
year = {2001}
}
@article{Chung2013,
abstract = {Given independent samples from P andQ, two-sample permutation tests allow one to construct exact level tests when the null hypothesis is P = Q. On the other hand, when comparing or testing particular parameters $\theta$ of P and Q, such as their means or medians, permutation tests need not be level $\alpha$, or even approximately level $\alpha$ in large samples. Under very weak assumptions for comparing estimators, we provide a general test procedure whereby the asymptotic validity of the permutation test holds while retaining the exact rejection probability $\alpha$ in finite samples when the underlying distributions are identical. The ideas are broadly applicable and special attention is given to the k-sample problem of comparing general parameters, whereby a permutation test is constructed which is exact level $\alpha$ under the hypothesis of identical distributions, but has asymptotic rejection probability $\alpha$ under the more general null hypothesis of equality of parameters. A Monte Carlo simulation study is performed as well. A quite general theory is possible based on a coupling construction, as well as a key contiguity argument for the multinomial and multivariate hypergeometric distributions. {\textcopyright} Institute of Mathematical Statistics, 2013.},
author = {Chung, Eunyi and Romano, Joseph P.},
doi = {10.1214/13-AOS1090},
file = {:Users/ekatsevi/papers/Annals of Statistics/Chung, Romano - 2013 - Exact and asymptotically robust permutation tests.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Behrens-Fisher problem,Coupling,Permutation test},
number = {2},
pages = {484--507},
title = {{Exact and asymptotically robust permutation tests}},
volume = {41},
year = {2013}
}
@book{Davidson2003,
abstract = {Economics; Econometrics; Asymptotics},
author = {Davidson, James},
doi = {10.1093/0198774036.001.0001},
file = {:Users/ekatsevi/papers/Stochastic Limit Theory/Davidson - 2003 - Stochastic Limit Theory.pdf:pdf},
isbn = {0198774036},
publisher = {Oxford University Press},
title = {{Stochastic Limit Theory}},
year = {1994}
}
@article{Statistics2009,
author = {Statistics, Mathematical},
file = {::},
journal = {Statistics},
number = {4},
pages = {631--650},
title = {{Saddlepoint Approximations in Statistics Author ( s ): H . E . Daniels Source : The Annals of Mathematical Statistics , Vol . 25 , No . 4 ( Dec ., 1954 ), pp . 631-650 Published by : Institute of Mathematical Statistics Stable URL : http://www.jstor.org/s}},
volume = {25},
year = {2009}
}
@article{Kolassa1990,
author = {Kolassa, John E. and McCullagh, Peter},
file = {:Users/ekatsevi/papers/Annals of Statistics/Kolassa, McCullagh - 1990 - Edgeworth series for lattice distributions.pdf:pdf},
issn = {0091-1798},
journal = {Annals of Statistics},
number = {2},
pages = {981--985},
title = {{Edgeworth series for lattice distributions}},
url = {http://projecteuclid.org/euclid.aop/1176996548},
volume = {18},
year = {1990}
}
@article{Ding2020,
abstract = {We consider a statistical test whose p value can only be approximated using Monte Carlo simulations. We are interested in deciding whether the p value for an observed data set lies above or below a given threshold such as 5{\%}. We want to ensure that the resampling risk, the probability of the (Monte Carlo) decision being different from the true decision, is uniformly bounded. This article introduces a simple open-ended method with this property, the confidence sequence method (CSM). We compare our approach to another algorithm, SIMCTEST, which also guarantees an (asymptotic) uniform bound on the resampling risk, as well as to other Monte Carlo procedures without a uniform bound. CSM is free of tuning parameters and conservative. It has the same theoretical guarantee as SIMCTEST and, in many settings, similar stopping boundaries. As it is much simpler than other methods, CSM is a useful method for practical applications.},
author = {Ding, Dong and Gandy, Axel and Hahn, Georg},
doi = {10.1007/s00180-019-00927-6},
file = {:Users/ekatsevi/papers/Computational Statistics/Ding, Gandy, Hahn - 2020 - A simple method for implementing Monte Carlo tests.pdf:pdf},
journal = {Computational Statistics},
keywords = {Algorithm,Hypothesis testing,Monte Carlo,p value},
pages = {1373--1392},
title = {{A simple method for implementing Monte Carlo tests}},
url = {https://doi.org/10.1007/s00180-019-00927-6},
volume = {35},
year = {2020}
}
@book{VDV1998,
address = {Cambridge},
author = {{Van Der Vaart}, A. W.},
file = {:Users/ekatsevi/papers/Unknown/Vaart - 1998 - Asymptotic Statistics.pdf:pdf},
keywords = {asymptotics},
mendeley-tags = {asymptotics},
publisher = {Cambridge University Press},
title = {{Asymptotic Statistics}},
year = {1998}
}
@article{Vrbik2022,
abstract = {We show how to calculate individual terms of the Edgeworth series to approximate the distribution of the Pearson correlation coefficient with the help of a simple Mathematica program. We also demonstrate how to eliminate the corresponding skewness, thus making the approximation substantially more accurate. This leads, in a rather natural way, to deriving a superior (in terms of its accuracy) version of Fisher's z transformation. The code can be easily modified to deal with any sample statistics defined as a function of several sample means, based on a random independent sample from a multivariate distribution.},
archivePrefix = {arXiv},
arxivId = {2208.05070},
author = {Vrbik, Jan},
eprint = {2208.05070},
file = {:Users/ekatsevi/papers/Unknown/Vrbik - 2022 - Fisher transformation via Edgeworth expansion.pdf:pdf},
journal = {arXiv},
keywords = {edgeworth series,fisher transformation,moment,pearson correlation},
pages = {1--8},
title = {{Fisher transformation via Edgeworth expansion}},
url = {http://arxiv.org/abs/2208.05070},
year = {2022}
}
@article{Feuerverger1999,
abstract = {We show the second‐order relative accuracy, on bounded sets, of the Studentized bootstrap, exponentially tilted bootstrap and nonparametric likelihood tilted bootstrap, for means and smooth functions of means. We also consider the relative errors for larger deviations. Our method exploits certain connections between Edgeworth and saddlepoint approximations to simplify the computations.Les auteurs {\'{e}}tudient la pr{\'{e}}cision relative du deuxieme ordre, sur des ensembles bom{\'{e}}s, de variantes studentis{\'{e}}es, inclin{\'{e}}e exponentiellement et inclin{\'{e}}e par vraisemblance non param{\'{e}}trique de la m{\'{e}}thode d'auto‐amor{\c{c}}age pour la moyenne et les fonctions lisses d'icelle. Us examinent en outre le comportement de l'erreur relative pour de grandes d{\'{e}}viations. Leurs calculs sont facilit{\'{e}}s parcertaines relations existant entre l'approximation de Edgeworth et la m{\'{e}}thode du point de selle.},
author = {Feuerverger, Andrey and Robinson, John and Wong, Augustine},
doi = {10.2307/3315635},
file = {:Users/ekatsevi/papers/Canadian Journal of Statistics/Feuerverger, Robinson, Wong - 1999 - On the relative accuracy of certain bootstrap procedures.pdf:pdf},
issn = {0319-5724},
journal = {Canadian Journal of Statistics},
keywords = {309,62,62620,62g 10,ams 1991 subject class,and phrases,bootstrap approximation,cations,edgeworth expansions,empirical,exponential tilting,likelihood,saddlepoint approximation,studentizing},
number = {2},
pages = {225--236},
title = {{On the relative accuracy of certain bootstrap procedures}},
volume = {27},
year = {1999}
}
@article{Steiß2012,
abstract = {PERMORY is software for accelerated permutation testing of genome-wide association studies (GWAS). We have parallelized PERMORY using the Message-Passing Interface resulting in a nearly linear speedup. Furthermore, we added accelerated analysis of GWAS using quantitative phenotypes, and an accurate estimation of the effective number of independent tests. Availability and implementation: Free download from http://},
author = {Stei{\ss}, Volker and Letschert, Thomas and Sch{\"{a}}fer, Helmut and Pahl, Roman and Bateman, Alex},
doi = {10.1093/bioinformatics/bts086},
file = {:Users/ekatsevi/papers/BIOINFORMATICS APPLICATIONS NOTE/Stei{\ss} et al. - 2012 - PERMORY-MPI a program for high-speed parallel permutation testing in genome-wide association studies.pdf:pdf},
journal = {BIOINFORMATICS APPLICATIONS NOTE},
number = {8},
pages = {1168--1169},
title = {{PERMORY-MPI: a program for high-speed parallel permutation testing in genome-wide association studies}},
url = {http://www.open-mpi.org/},
volume = {28},
year = {2012}
}
@article{Minas2014,
abstract = {In several modern applications, ranging from genomics to neuroimaging, there is a need to compare measurements across different populations, such as those collected from samples of healthy and diseased individuals. The interest is in detecting a group effect, and typically many thousands or even millions of tests need to be performed simultaneously, as exemplified in genomics where single tests are applied for each gene across the genome. Traditional procedures, such as multivariate analysis of variance (MANOVA), are not suitable when dealing with nonvector-valued data structures such as functional or graph-structured observations. In this article, we discuss an existing distance-based MANOVA-like approach, the distance-based F (DBF) test, for detecting such differences. The null sampling distribution of the DBF test statistic relies on the distribution of the measurements and the chosen distance measure, and is generally unavailable in closed form. In practice, Monte Carlo permutation methods are deployed which introduce errors in estimating small p-values and increase familywise type I error rates when not using enough permutations. In this work, we propose an approximate distribution for the DBF test allowing inferences to be drawn without the need for costly permutations. This is achieved by approximating the permutation distribution that would be obtained by enumerating all permutations by the Pearson type III distribution using moment matching. The use of the Pearson type III distribution is motivated via empirical observations with real data. We provide evidence with real and simulated data that the resulting approximate null distribution of the DBF test is flexible enough to work well with a range of distance measures. Through extensive simulations involving different data types and distance measures, we provide evidence that the proposed methodology yields the same statistical power that would otherwise only be achievable if many millions of Monte Carlo permutations were performed.},
author = {Minas, Christopher and Montana, Giovanni},
doi = {10.1002/sam.11227},
issn = {19321872},
journal = {Statistical Analysis and Data Mining},
keywords = {Distance-based inference,Genomics,MANOVA,Neuroimaging,Person type III approximation},
number = {6},
pages = {450--470},
title = {{Distance-based analysis of variance: Approximate inference}},
volume = {7},
year = {2014}
}
@article{Cauvin,
archivePrefix = {arXiv},
arxivId = {arXiv:2208.09274v1},
author = {Cauvin, Pierre-Louis},
eprint = {arXiv:2208.09274v1},
file = {:Users/ekatsevi/papers/arXiv/Cauvin - Unknown - Edgeworth expansion for Bernoulli weighted mean.pdf:pdf},
journal = {arXiv},
pages = {1--12},
title = {{Edgeworth expansion for Bernoulli weighted mean}}
}
@article{Bancroft2013,
author = {Bancroft, Tim and Du, Chuanlong and Nettleton, Dan},
doi = {10.1111/j.l541-0420.2012.01825.x},
file = {:Users/ekatsevi/papers/Unknown/Bancroft, Du, Nettleton - 2013 - Estimation of False Discovery Rate Using Sequential Permutation p-Values.pdf:pdf},
number = {1},
pages = {1--7},
title = {{Estimation of False Discovery Rate Using Sequential Permutation p-Values}},
url = {https://www.jstor.org/stable/41806061?seq=1{\&}cid=pdf-},
volume = {69},
year = {2013}
}
@article{Rizzo2020,
abstract = {* A more theoretical book on the same subject as the book on statistical learning by Hastie/Tibshirani/Friedman},
author = {Rizzo, Maria L.},
doi = {10.1201/9781420010718-12},
file = {::},
journal = {Statistical Computing with R},
keywords = {edgeworth expansion,genome-wide association study,monte carlo,p -value,permutation test},
pages = {231--260},
title = {{Permutation Tests}},
volume = {28},
year = {2020}
}
@article{Raz1990a,
abstract = {When a linear regression function is estimated by ordinary least squares, the null hypothesis of no relationship between the response and the design variable can be tested by the normal theory F test. This article describes a new test for use in the case in which a smooth regression function is estimated by a nonparametric procedure such as kernel estimation or local regression. The test is constructed as an approximation to an exact permutation test. The test statistic is the ratio of sums of squares that are defined by analogy to the analysis of variance. The null permutation distribution of the test statistic is approximated by matching its exact mean and variance to the moments of a gamma distribution. A simulation study shows that the approximation is excellent for several regression procedures and for normal, heavy-tailed, and skewed error distributions. Simulation results are also used to investigate the power of the test as a function of the smoothing parameter. The test is applied to data on the relationship between lymphocyte concentrations and immunological status in men with human immunodeficiency virus infection. {\textcopyright} 1990 Taylor {\&} Francis Group, LLC.},
author = {Raz, Jonathan},
doi = {10.1080/01621459.1990.10475316},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Raz - 1990 - Testing for no effect when estimating a smooth function by nonparametric regression A randomization approach.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Kernel estimation,Local regression,Permutation test,Smoothing},
number = {409},
pages = {132--138},
title = {{Testing for no effect when estimating a smooth function by nonparametric regression: A randomization approach}},
volume = {85},
year = {1990}
}
@article{Eisinga2012,
author = {Eisinga, Rob and Grotenhuis, Manfred Te and Pelzer, Ben},
file = {:Users/ekatsevi/papers/Statistica Neerlandica/Eisinga, Grotenhuis, Pelzer - 2012 - Saddlepoint approximations for the sum of independent non-identically distributed binomial random v.pdf:pdf},
journal = {Statistica Neerlandica},
keywords = {saddlepoint approximation},
mendeley-tags = {saddlepoint approximation},
title = {{Saddlepoint approximations for the sum of independent non-identically distributed binomial random variables}},
year = {2012}
}
@article{Datlinger2017,
abstract = {CRISPR-based genetic screens are accelerating biological discovery, but current methods have inherent limitations. Widely used pooled screens are restricted to simple readouts including cell proliferation and sortable marker proteins. Arrayed screens allow for comprehensive molecular readouts such as transcriptome profiling, but at much lower throughput. Here we combine pooled CRISPR screening with single-cell RNA sequencing into a broadly applicable workflow, directly linking guide RNA expression to transcriptome responses in thousands of individual cells. Our method for CRISPR droplet sequencing (CROP-seq) enables pooled CRISPR screens with single-cell transcriptome resolution, which will facilitate high-throughput functional dissection of complex regulatory mechanisms and heterogeneous cell populations.},
author = {Datlinger, Paul and Rendeiro, Andr{\'{e}} F. and Schmidl, Christian and Krausgruber, Thomas and Traxler, Peter and Klughammer, Johanna and Schuster, Linda C. and Kuchler, Amelie and Alpar, Donat and Bock, Christoph},
doi = {10.1038/nmeth.4177},
file = {:Users/ekatsevi/papers/Nature Methods/Datlinger et al. - 2017 - Pooled CRISPR screening with single-cell transcriptome readout.pdf:pdf},
issn = {15487105},
journal = {Nature Methods},
keywords = {CRISPR},
mendeley-tags = {CRISPR},
number = {3},
pages = {297--301},
pmid = {28099430},
publisher = {Nature Publishing Group},
title = {{Pooled CRISPR screening with single-cell transcriptome readout}},
volume = {14},
year = {2017}
}
@book{Butler2007,
author = {Butler, Ronald},
publisher = {Cambridge University Press},
title = {{Saddlepoint approximations with applications}},
year = {2007}
}
@book{Chen2011,
abstract = {Stein's method for normal approximations is explained, with some examples and applications. In the study of the asymptotic distribution of the sum of dependent random variables, Stein's method may be a very useful tool. We have attempted to write an elementary introduction. For more advanced introductions to Stein's method, see Stein (1986), Barbour (1997) and Chen (1998). {\textcopyright} Springer-Verlag 2000.},
author = {Chen, Louis H.Y. and Goldstein, Larry and Shao, Qi-Man},
booktitle = {Decisions in Economics and Finance},
doi = {10.1007/s102030050003},
file = {::},
isbn = {9783642150067},
issn = {11296569},
number = {1},
publisher = {Springer},
title = {{Normal approximations by Stein's method}},
volume = {23},
year = {2011}
}
@article{Zhou2013,
abstract = {Permutation is an attractive approach to assess association between two vectors x and y, by comparing the observed statistic to the distribution induced by random permutation of one of the vectors. For a number of "standard" statistics, equivalent testing can be performed by using the sample Pearson correlation. Applications include the standard tests applied in the two-sample problem and simple linear regression. We describe a simple approximation to the distribution of the correlation under permutation, providing accurate p-values that can be quickly computed for a variety of data types. The approximation may be especially useful in high-throughput biology applications in which a series of x-vectors is compared to one or more y-vectors.},
author = {Zhou, Yi Hui and Wright, Fred A.},
file = {:Users/ekatsevi/papers/5th International Conference on Bioinformatics and Computational Biology 2013, BICoB 2013/Zhou, Wright - 2013 - Simple and accurate trend tests using a permutation approximation.pdf:pdf},
isbn = {9781622769711},
journal = {5th International Conference on Bioinformatics and Computational Biology 2013, BICoB 2013},
keywords = {Differential expression,Permutation approximation,Saddlepoint},
pages = {277--282},
title = {{Simple and accurate trend tests using a permutation approximation}},
year = {2013}
}
@book{Durrett2019,
author = {Durrett, Rick},
edition = {Fifth Edit},
file = {:Users/ekatsevi/papers/Unknown/Durrett - 2019 - Probability Theory and Examples.pdf:pdf},
publisher = {Cambridge University Press},
title = {{Probability: Theory and Examples}},
year = {2019}
}
@article{Efron1979,
author = {Efron, Bradley},
file = {:Users/ekatsevi/papers/Annals of Statistics/Efron - 1979 - Bootstrap Methods Another Look at the Jackknife.pdf:pdf},
issn = {0091-1798},
journal = {Annals of Statistics},
number = {1},
pages = {1--26},
title = {{Bootstrap Methods: Another Look at the Jackknife}},
url = {http://projecteuclid.org/euclid.aop/1176996548},
volume = {7},
year = {1979}
}
@article{Reid1996,
abstract = {Recent developments in higher-order asymptotic theory for statistical inference have emphasized the fundamental role of the likelihood function in providing accurate approximations to cumulative distribution functions. This paper summarizes the main results, with an emphasis on classes of problems for which relatively easily implemented solutions exist. A survey of the literature indicates the large number of problems solved and solvable by this method. Generalizations and extensions, with suggestions for further development, are considered.},
author = {Reid, N.},
doi = {10.2307/3315622},
file = {:Users/ekatsevi/papers/Canadian Journal of Statistics/Reid - 1996 - Likelihood and higher-order approximations to tail areas A review and annotated bibliography.pdf:pdf},
issn = {03195724},
journal = {Canadian Journal of Statistics},
keywords = {62-02,62e20,ams subject classi cation,approximation,ence,laplace expansion,nonparametric infer-,parametric inference,saddlepoint method,tail area},
number = {2},
pages = {141--166},
title = {{Likelihood and higher-order approximations to tail areas: A review and annotated bibliography}},
volume = {24},
year = {1996}
}
@article{Jing2004,
abstract = {A saddlepoint approximation of the Student's t-statistic was derived by Daniels and Young [Biometrika 78 (1991) 169-179] under the very stringent exponential moment condition that requires that the underlying density function go down at least as fast as a Normal density in the tails. This is a severe restriction on the approximation's applicability. In this paper we show that this strong exponential moment restriction can be completely dispensed with, that is, saddlepoint approximation of the Student's t-statistic remains valid without any moment condition. This confirms the folklore that the Student's t-statistic is robust against outliers. The saddlepoint approximation not only provides a very accurate approximation for the Student's t-statistic, but it also can be applied much more widely in statistical inference. As a result, saddlepoint approximations should always be used whenever possible. Some numerical work will be given to illustrate these points. {\textcopyright} Institute of Mathematical Statistics, 2004.},
author = {Jing, Bing Yi and Shao, Qi Man and Zhou, Wang},
doi = {10.1214/009053604000000742},
file = {:Users/ekatsevi/papers/Annals of Statistics/Jing, Shao, Zhou - 2004 - Saddlepoint approximation for student's t-statistic with no moment conditions.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Absolute error,Asymptotic normality,Edgeworth expansion,Large deviation,Relative error,Saddlepoint approximation,Self-normalized sum,Student's t-statistic},
number = {6},
pages = {2679--2711},
title = {{Saddlepoint approximation for student's t-statistic with no moment conditions}},
volume = {32},
year = {2004}
}
@article{Shah2018,
abstract = {It is a common saying that testing for conditional independence, i.e., testing whether whether two random vectors {\$}X{\$} and {\$}Y{\$} are independent, given {\$}Z{\$}, is a hard statistical problem if {\$}Z{\$} is a continuous random variable (or vector). In this paper, we prove that conditional independence is indeed a particularly difficult hypothesis to test for. Valid statistical tests are required to have a size that is smaller than a predefined significance level, and different tests usually have power against a different class of alternatives. We prove that a valid test for conditional independence does not have power against any alternative. Given the non-existence of a uniformly valid conditional independence test, we argue that tests must be designed so their suitability for a particular problem may be judged easily. To address this need, we propose in the case where {\$}X{\$} and {\$}Y{\$} are univariate to nonlinearly regress {\$}X{\$} on {\$}Z{\$}, and {\$}Y{\$} on {\$}Z{\$} and then compute a test statistic based on the sample covariance between the residuals, which we call the generalised covariance measure (GCM). We prove that validity of this form of test relies almost entirely on the weak requirement that the regression procedures are able to estimate the conditional means {\$}X{\$} given {\$}Z{\$}, and {\$}Y{\$} given {\$}Z{\$}, at a slow rate. We extend the methodology to handle settings where {\$}X{\$} and {\$}Y{\$} may be multivariate or even high-dimensional. While our general procedure can be tailored to the setting at hand by combining it with any regression technique, we develop the theoretical guarantees for kernel ridge regression. A simulation study shows that the test based on GCM is competitive with state of the art conditional independence tests. Code is available as the R package GeneralisedCovarianceMeasure on CRAN.},
author = {Shah, Rajen D. and Peters, Jonas},
file = {:Users/ekatsevi/papers/Annals of Statistics/Shah, Peters - 2020 - The Hardness of Conditional Independence Testing and the Generalised Covariance Measure.pdf:pdf},
issn = {0090-5364},
journal = {Annals of Statistics},
keywords = {causality,conditional randomization test},
mendeley-tags = {causality,conditional randomization test},
number = {3},
pages = {1514--1538},
title = {{The Hardness of Conditional Independence Testing and the Generalised Covariance Measure}},
url = {http://arxiv.org/abs/1804.07203},
volume = {48},
year = {2020}
}
@article{Strawderman1996,
abstract = {Saddlepoint approximations are derived for sums of independent, but not necessarily identically distributed random variables, along with generalizations to estimating equations and multivariate problems. These results are particularly useful for accurately approximating the distribution of regression coefficients. General formulas are given for the distribution of the coefficients arising out of a generalized linear model with both canonical and noncanonical link functions. We illustrate the case of logistic regression with a real data example and show how the Gibbs sampler may be used to obtain confidence sets for each regression parameter based on the saddlepoint approximation. Copyright 1996 Taylor {\&} Francis Group, LLC.},
author = {Strawderman, Robert L. and Casella, George and Wells, Martin T.},
doi = {10.1080/01621459.1996.10476933},
file = {::},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Estimating equation,Generalized linear model,Gibbs sampling,Logistic regression,Maximum likelihood estimation,Monte Carlo integration},
number = {434},
pages = {643--654},
title = {{Practical Small-Sample Asymptotics for Regression Problems}},
volume = {91},
year = {1996}
}
@article{Sandve2011,
abstract = {Motivation: In molecular biology, as in many other scientific fields, the scale of analyses is ever increasing. Often, complex Monte Carlo simulation is required, sometimes within a large-scale multiple testing setting. The resulting computational costs may be prohibitively high. Results: We here present MCFDR, a simple, novel algorithm for false discovery rate (FDR) modulated sequential Monte Carlo (MC) multiple hypothesis testing. The algorithm iterates between adding MC samples across tests and calculating intermediate FDR values for the collection of tests. MC sampling is stopped either by sequential MC or based on a threshold on FDR. An essential property of the algorithm is that it limits the total number of MC samples whatever the number of true null hypotheses. We show on both real and simulated data that the proposed algorithm provides large gains in computational efficiency. Availability: MCFDR is implemented in the Genomic HyperBrowser (http://hyperbrowser.uio.no/mcfdr), a web-based system for genome analysis. All input data and results are available and can be reproduced through a Galaxy Pages document at:},
author = {Sandve, Geir Kjetil and Ferkingstad, Egil and Nyg{\aa}rd, St{\aa}le and Bateman, Alex},
doi = {10.1093/bioinformatics/btr568},
file = {:Users/ekatsevi/papers/Bioinformatics/Sandve et al. - 2011 - Sequential Monte Carlo multiple testing.pdf:pdf},
journal = {Bioinformatics},
keywords = {FDR,Monte Carlo,multiple testing},
mendeley-tags = {FDR,Monte Carlo,multiple testing},
number = {23},
pages = {3235--3241},
title = {{Sequential Monte Carlo multiple testing}},
url = {http://hyperbrowser.uio.no/mcfdr/u/sandve/p/mcfdr.},
volume = {27},
year = {2011}
}
@article{Ge2012,
abstract = {Imaging traits are thought to have more direct links to genetic variation than diagnostic measures based on cognitive or clinical assessments and provide a powerful substrate to examine the influence of genetics on human brains. Although imaging genetics has attracted growing attention and interest, most brain-wide genome-wide association studies focus on voxel-wise single-locus approaches, without taking advantage of the spatial information in images or combining the effect of multiple genetic variants. In this paper we present a fast implementation of voxel- and cluster-wise inferences based on the random field theory to fully use the spatial information in images. The approach is combined with a multi-locus model based on least square kernel machines to associate the joint effect of several single nucleotide polymorphisms (SNP) with imaging traits. A fast permutation procedure is also proposed which significantly reduces the number of permutations needed relative to the standard empirical method and provides accurate small p-value estimates based on parametric tail approximation. We explored the relation between 448,294 single nucleotide polymorphisms and 18,043 genes in 31,662 voxels of the entire brain across 740 elderly subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Structural MRI scans were analyzed using tensor-based morphometry (TBM) to compute 3D maps of regional brain volume differences compared to an average template image based on healthy elderly subjects. We find method to be more sensitive compared with voxel-wise single-locus approaches. A number of genes were identified as having significant associations with volumetric changes. The most associated gene was GRIN2B, which encodes the N-methyl-d-aspartate (NMDA) glutamate receptor NR2B subunit and affects both the parietal and temporal lobes in human brains. Its role in Alzheimer's disease has been widely acknowledged and studied, suggesting the validity of the approach. The various advantages over existing approaches indicate a great potential offered by this novel framework to detect genetic influences on human brains. {\textcopyright} 2012 Elsevier Inc.},
author = {Ge, Tian and Feng, Jianfeng and Hibar, Derrek P. and Thompson, Paul M. and Nichols, Thomas E.},
doi = {10.1016/j.neuroimage.2012.07.012},
file = {:Users/ekatsevi/papers/NeuroImage/Ge et al. - 2012 - Increasing power for voxel-wise genome-wide association studies The random field theory, least square kernel machi(2).pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {ADNI,Alzheimer's disease,Association study,Cluster size inference,GRIN2B,Imaging genetics,Least square kernel machines,MRI,Nonstationarity,Parametric tail approximation,Pareto distribution,Permutation,Random field theory,Tensor-based morphometry,Voxel-wise inference},
number = {2},
pages = {858--873},
pmid = {22800732},
publisher = {Elsevier Inc.},
title = {{Increasing power for voxel-wise genome-wide association studies: The random field theory, least square kernel machines and fast permutation procedures}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2012.07.012},
volume = {63},
year = {2012}
}
@article{Davison2006,
abstract = {Discrete data, particularly count and contingency table data, are typically analysed by using methods that are accurate to first order, such as normal approximations for maximum likelihood estimators. By contrast continuous data can quite generally be analysed by using third-order procedures, with major improvements in accuracy and with intrinsic separation of information concerning parameter components. The paper extends these higher order results to discrete data, yielding a methodology that is widely applicable and accurate to second order. The extension can be described in terms of an approximating exponential model that is expressed in terms of a score variable. The development is outlined and the flexibility of the approach is illustrated by examples. {\textcopyright} 2006 Royal Statistical Society.},
author = {Davison, A. C. and Fraser, D. A.S. and Reid, N.},
doi = {10.1111/j.1467-9868.2006.00548.x},
file = {::},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Binary regression,Categorical data,Conditional inference,Contingency tables,Likelihood,Negative binomial,Non-canonical link function},
number = {3},
pages = {495--508},
title = {{Improved likelihood inference for discrete data}},
volume = {68},
year = {2006}
}
@article{McHale1988,
abstract = {Draft for a book},
author = {McHale, David and McCullagh, P.},
doi = {10.2307/2982788},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society. Series A (Statistics in Society)/McHale, McCullagh - 1988 - Tensor Methods in Statistics.pdf:pdf},
issn = {09641998},
journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
number = {2},
pages = {378},
title = {{Tensor Methods in Statistics.}},
volume = {151},
year = {1988}
}
@article{Fischer2024a,
abstract = {In a Monte-Carlo test, the observed dataset is fixed, and several resampled or permuted versions of the dataset are generated in order to test a null hypothesis that the original dataset is exchangeable with the resampled/permuted ones. Sequential Monte-Carlo tests aim to save computational resources by generating these additional datasets sequentially one by one, and potentially stopping early. While earlier tests yield valid inference at a particular prespecified stopping rule, our work develops a new anytime-valid Monte-Carlo test that can be continuously monitored, yielding a p-value or e-value at any stopping time possibly not specified in advance. Despite the added flexibility, it significantly outperforms the well-known method by Besag and Clifford, stopping earlier under both the null and the alternative without compromising power. The core technical advance is the development of new test martingales (nonnegative martingales with initial value one) for testing exchangeability against a very particular alternative. These test martingales are constructed using new and simple betting strategies that smartly bet on the relative ranks of generated test statistics. The betting strategies are guided by the derivation of a simple log-optimal betting strategy, have closed form expressions for the wealth process, provable guarantees on resampling risk, and display excellent power in practice.},
archivePrefix = {arXiv},
arxivId = {2401.07365},
author = {Fischer, Lasse and Ramdas, Aaditya},
eprint = {2401.07365},
file = {:Users/ekatsevi/papers/arXiv/Fischer, Ramdas - 2024 - Sequential Monte-Carlo testing by betting.pdf:pdf},
journal = {arXiv},
pages = {1--33},
title = {{Sequential Monte-Carlo testing by betting}},
url = {http://arxiv.org/abs/2401.07365},
year = {2024}
}
@article{Jiang2012,
author = {Jiang, H U I and Salzman, Julia},
file = {:Users/ekatsevi/papers/Biometrika/Jiang, Salzman - 2012 - Statistical properties of an early stopping rule for resampling-based multiple testing.pdf:pdf},
journal = {Biometrika},
keywords = {Monte Carlo,Multiple testing},
mendeley-tags = {Monte Carlo,Multiple testing},
number = {4},
pages = {973--980},
title = {{Statistical properties of an early stopping rule for resampling-based multiple testing}},
volume = {99},
year = {2012}
}
@article{Conery2024,
author = {Conery, Mitchell and Pippin, James A and Wagley, Yadav and Trang, Khanh and Matthew, C},
file = {:Users/ekatsevi/papers/bioRxiv/Conery et al. - 2024 - GWAS-informed data integration and non-coding CRISPRi screen illuminate genetic etiology of bone mineral density.pdf:pdf},
journal = {bioRxiv},
title = {{GWAS-informed data integration and non-coding CRISPRi screen illuminate genetic etiology of bone mineral density}},
year = {2024}
}
@article{Skovgaard1987a,
author = {Skovgaard, Ib M.},
file = {:Users/ekatsevi/papers/Journal of Applied Probability/Skovgaard - 1987 - Saddlepoint Expansions for Conditional Distributions.pdf:pdf},
journal = {Journal of Applied Probability},
number = {4},
pages = {875--887},
title = {{Saddlepoint Expansions for Conditional Distributions}},
volume = {24},
year = {1987}
}
@article{Zerbe1979,
abstract = {The randomization analysis of a randomized blocks design is extended to the situation where the observations are growth (or response) curves. A randomization test is developed for testing treatment effects over time (or stimulus) intervals prespecified by the investigator. {\textcopyright} 1979, Taylor {\&} Francis Group, LLC. All rights reserved.},
author = {Zerbe, Gary O.},
doi = {10.1080/03610927908827750},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Zerbe - 1979 - Randomization analysis of randomized blocks design extended to growth and response curves.pdf:pdf},
issn = {1532415X},
journal = {Journal of the American Statistical Association},
keywords = {growth curves,interval specific test,random curves,randomizatiol,test},
number = {365},
pages = {215--221},
title = {{Randomization analysis of randomized blocks design extended to growth and response curves}},
volume = {74},
year = {1979}
}
@article{IanAbramsonUniversityofCalifornia1991,
author = {{Ian Abramson (University of California}, San Diego)},
file = {:Users/ekatsevi/papers/Annals of Statistics/Ian Abramson (University of California - 1991 - Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve,.pdf:pdf},
journal = {Annals of Statistics},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Statistics. {\textregistered} www.jstor.org}},
year = {1991}
}
@book{Fisher1935,
address = {Edinburgh},
author = {Fisher, Ronald},
file = {:Users/ekatsevi/papers/Unknown/Fisher - 1935 - The Design of Experiments.pdf:pdf},
publisher = {Oliver and Boyd},
title = {{The Design of Experiments}},
year = {1935}
}
@article{Bleistein1966,
author = {Bleistein, Norman},
doi = {10.1002/cpa.3160190403},
file = {::},
issn = {10970312},
journal = {Communications on Pure and Applied Mathematics},
number = {4},
pages = {353--370},
title = {{Uniform asymptotic expansions of integrals with stationary point near algebraic singularity}},
volume = {19},
year = {1966}
}
